https://stackoverflow.com/questions/60420515/sum-a-column-of-sparsevectors-in-pyspark

From Spark 2.4+, pyspark.ml.stat added Summarizer class which support the following statistic 
related methods: count, max, min, mean, normL1, normL2, numNonZeros, variance
Except `count` method which returns an Integer, All return a DenseVector with the same size as original vector

use Summarizer.metrics("mean", "count", "normL1", "max") to check more than one stats, this returns a StructType: 

    struct<mean:vector,count:bigint,normL1:vector,max:vector>
 
Example code:

    from pyspark.ml.linalg import SparseVector
    from pyspark.ml.stat import Summarizer 
    
    df = spark.createDataFrame([
        (SparseVector(11,[1,2,3],[1.0,0.2,0.4]),1),
        (SparseVector(11,[1,2],[0.3,0.2]),1),
        (SparseVector(11,[1,3,4],[0.5,0.3,0.2]),1)
    ], ['features_array', 'id']) 
    
    summary = df.select(Summarizer.normL1(df.features_array).alias('value')).first().value
    #DenseVector([0.0, 1.8, 0.4, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
    
To convert DenseVector into SparseVector

    import numpy as np
    
    t = summary.toArray()
    t_nonzero = t!=0
    summary_sparse = SparseVector(t.size, np.where(t_nonzero)[0], t[t_nonzero])
    
Need to groupBy id and then calculate normL1 sum on each id:
---
Step-1: convert SparseVector into array of doubles
    
    from pyspark.sql.functions import array, expr, to_json, from_json
    schema = "array<struct<type:int,size:int,indices:array<int>,values:array<double>>>"
    df1 = df.withColumn("features_array", from_json(to_json(array("features_array")), schema)[0])
    
    df2 = df1.selectExpr("id", """
        transform(
          sequence(0, features_array.size-1), 
          i -> 
          IF(array_contains(features_array.indices, i)
           , features_array.values[array_position(features_array.indices,i)-1]
           , 0
          ) 
        ) AS features_array
      """)
    
    df2.show(truncate=False)
    +---+-------------------------------------------------------+
    |id |features_array                                         |
    +---+-------------------------------------------------------+
    |1  |[0.0, 1.0, 0.2, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]|
    |1  |[0.0, 0.3, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]|
    |1  |[0.0, 0.5, 0.0, 0.3, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]|
    +---+-------------------------------------------------------+
    
Step-2: calculate summary 

  Method-1: using aggregate

    df2.groupby('id').agg(expr("""
        aggregate(
          collect_list(features_array), 
          cast(array() as array<double>), 
          (acc,y) -> 
            zip_with(acc, y, (t1,t2) -> ifnull(t1,0)+ifnull(t2,0))
        ) as summary
      """)
    ).show(truncate=False)
    +---+-------------------------------------------------------+
    |id |summary                                                |
    +---+-------------------------------------------------------+
    |1  |[0.0, 1.8, 0.4, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]|
    +---+-------------------------------------------------------+
    
  Method-2: using pandas_udf:

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    import pandas as pd, numpy as np
    
    @pandas_udf("id:int,features_array:array<double>", PandasUDFType.GROUPED_MAP)
    def col_summary(key, pdf):
       return pd.DataFrame([key + (np.array(pdf.features_array.tolist()).sum(0),)])
    
    df2.groupby('id').apply(col_summary).show(truncate=False)                                                          
    +---+-------------------------------------------------------+                   
    |id |features_array                                         |
    +---+-------------------------------------------------------+
    |1  |[0.0, 1.8, 0.4, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]|
    +---+-------------------------------------------------------+
    
