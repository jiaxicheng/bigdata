https://stackoverflow.com/questions/58712615/what-is-the-best-way-to-find-all-occurrences-of-values-from-one-dataframe-in-ano

Questions: A list of texts and a list of lookup words, need to find all words shown in lookup table and
           saved in an array. both lists contain 100K+ records

Solution: use CountVectorizerModel as 100K lookup words should be manageable for this model (default vocabSize=262144).

    The basic idea is to create the CountVectorizerModel based on a customized list from df2 (lookup table). 
    split the text in the first df into an array column (or use pyspark.ml.feature.RegexTokenizer) and then 
    transform this column into a column of SparseVectors:


    from pyspark.ml.feature import CountVectorizerModel
    from pyspark.sql.functions import split, udf, col, lower

    lst = [ r.fruit_lookup for r in df_lookup.collect() ] 

    model = CountVectorizerModel.from_vocabulary(lst, inputCol='words_arr', outputCol='fruits_vec')

    df1 = model.transform(df.withColumn('words_arr', split(lower(col('value')), r'[\s\p{Punct}]+')))
    df1.show(truncate=False)                                                                                           
    +------------------------------------+------------------------------------------+-------------------+
    |value                               |words_arr                                 |fruits_vec         |
    +------------------------------------+------------------------------------------+-------------------+
    |I like apples                       |[I, like, apples]                         |(4,[0],[1.0])      |
    |oranges are good                    |[oranges, are, good]                      |(4,[1],[1.0])      |
    |eating bananas is healthy           |[eating, bananas, is, healthy]            |(4,[2],[1.0])      |
    |tomatoes are red, bananas are yellow|[tomatoes, are, red, bananas, are, yellow]|(4,[2,3],[1.0,1.0])|
    +------------------------------------+------------------------------------------+-------------------+
 
Method-1: use vocabulary if it can be loaded properly
---

    to_match = udf(lambda v: [ vocabulary[i] for i in v.indices ], 'array<string>')
    to_indices = udf(lambda v: list(map(int, v.indices)), 'array<int>')

    df2 = df1.withColumn('fruit_idx', to_indices('fruits_vec')) \
         .withColumn('extracted_fruits', to_match('fruits_vec')).drop('words_arr', 'fruits_vec')
    df2.show(truncate=False)                                     
    +------------------------------------+---------+-------------------+
    |value                               |fruit_idx|extracted_fruits   |
    +------------------------------------+---------+-------------------+
    |I like apples                       |[0]      |[apples]           |
    |oranges are good                    |[1]      |[oranges]          |
    |eating bananas is healthy           |[2]      |[bananas]          |
    |tomatoes are red, bananas are yellow|[2, 3]   |[bananas, tomatoes]|
    +------------------------------------+---------+-------------------+


Method-2: use SparceVector.indices and join
---

    from pyspark.sql.functions import explode_outer, collect_set

    to_indices = udf(lambda v: list(map(int, v.indices)), 'array<int>')

    df3 = df1.withColumn('fid', explode_outer(to_indices('fruits_vec'))).drop('words_arr', 'fruits_vec')

    df3.join(df_lookup, df31.fid == df_lookup.id, how='left') \
       .groupby('value') \
       .agg(collect_set('fruit_lookup').alias('extracted_fruits')) \
       .show()
    +--------------------+-------------------+                                      
    |               value|   extracted_fruits|
    +--------------------+-------------------+
    |    oranges are good|          [oranges]|
    |       I like apples|           [apples]|
    |tomatoes are red,...|[tomatoes, bananas]|
    |eating bananas is...|          [bananas]|
    +--------------------+-------------------+

Extended for preprocessing on lookup values with multiple words:
---
Added the following df1 pre-process step and create an array column including all 
N-gram combinations. For each string with L words, N=2 will add (L-1) more items in array, 
if N=3, (L-1)+(L-2) more items.

    from pyspark.sql.functions import split, udf, regexp_replace, lower

    # max number of words in a single entry of the lookup table df2
    N = 2

    # Pre-process the `text` field up to N-grams, 
    # example: ngram_str('oranges are good', 3) 
    #      --> ['oranges', 'are', 'good', 'oranges are', 'are good', 'oranges are good']
    def ngram_str(s_t_r, N):
      arr = s_t_r.split()           
      L = len(arr)           
      for i in range(2,N+1):           
        if L - i < 0: break           
        arr += [ ' '.join(arr[j:j+i]) for j in range(L-i+1) ]           
      return arr           

    udf_ngram_str = udf(lambda x: ngram_str(x, N), 'array<string>')

    df1_processed = df1.withColumn('words_arr', udf_ngram_str(lower(regexp_replace('text', r'[\s\p{Punct}]+', ' '))))


    lst = [ r.fruit_lookup for r in df2.collect() ]

    model = CountVectorizerModel.from_vocabulary(lst, inputCol='words_arr', outputCol='fruits_vec')

    df3 = model.transform(df1_processed)
    df3.show(20,40)
    +----------------------------------------+----------------------------------------+-------------------+
    |                                    text|                               words_arr|         fruits_vec|
    +----------------------------------------+----------------------------------------+-------------------+
    |                           I like apples|  [i, like, apples, i like, like apples]|      (5,[0],[1.0])|
    |                        oranges are good|[oranges, are, good, oranges are, are...|      (5,[1],[1.0])|
    |               eating bananas is healthy|[eating, bananas, is, healthy, eating...|      (5,[2],[1.0])|
    |    tomatoes are red, bananas are yellow|[tomatoes, are, red, bananas, are, ye...|(5,[2,3],[1.0,1.0])|
    |                                    test|                                  [test]|          (5,[],[])|
    |I have dragon fruit and apples in my bag|[i, have, dragon, fruit, and, apples,...|(5,[0,4],[1.0,1.0])|
    +----------------------------------------+----------------------------------------+-------------------+

Alternative Method: left join with rlike as the condition:

    from pyspark.sql.functions import expr, collect_set
    
    df1.alias('d1').join(
          df2.alias('d2')
        , expr('d1.text rlike concat("\\\\b", d2.fruit_lookup, "\\\\b")')
        , 'left'
    ).groupby('text') \
     .agg(collect_set('fruit_lookup').alias('extracted_fruits')) \
     .show()
    +----------------------------------------+----------------------+
    |text                                    |extracted_fruits      |
    +----------------------------------------+----------------------+
    |oranges are good                        |[oranges]             |
    |I like apples                           |[apples]              |
    |I have dragon fruit and apples in my bag|[dragon fruit, apples]|
    |tomatoes are red, bananas are yellow    |[tomatoes, bananas]   |
    |eating bananas is healthy               |[bananas]             |
    |test                                    |[]                    |
    +----------------------------------------+----------------------+

Where: "\\\\b is word boundary so that the lookup values do not mess up with their contexts.
