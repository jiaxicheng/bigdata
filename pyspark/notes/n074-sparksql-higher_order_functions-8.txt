Continue..

Spark SQL higher-order functions:

(25) drop duplicated from an ArrayType column using aggregate:
  REF: https://stackoverflow.com/questions/60959421
 
    fees_data = [[1584579671000, 12.11], [1584579672000, 12.11], [1584579673000, 12.11]];
    fees = [ {"updated_at":u,"fee":f} for u,f in fees_data ]

    status_data = [[1584579671000, "Closed-A"], [1584579672000, "Closed-A"], [1584579673000, "Closed-B"], [1584579674000, "Closed"]];
    status = [ {"updated_at":u,"status":f} for u,f in status_data ]

    schema = "fee:array<struct<updated_at:long,fee:float>>,status:array<struct<updated_at:long,status:string>>"

    df = spark.createDataFrame([(fees, status)], schema=schema)

    df.show(truncate=False, vertical=True)                                                                             
    -RECORD 0------------------------------------------------------------------------------------------------------------
     fee    | [[1584579671000, 12.11], [1584579672000, 12.11], [1584579673000, 12.11]]                                   
     status | [[1584579671000, Closed-A], [1584579672000, Closed-A], [1584579673000, Closed-B], [1584579674000, Closed]]

    root
     |-- fee: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- updated_at: long (nullable = true)
     |    |    |-- fee: float (nullable = true)
     |-- status: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- updated_at: long (nullable = true)
     |    |    |-- status: string (nullable = true)

    df.selectExpr(
      """
        aggregate(
          status,
          /* start: set the initial value of `acc` as an array of structs */
          array((bigint(NULL) as updated_at, string(NULL) as status)),
          /* merge: check and compare y with the last item of `acc` by element_at(acc, -1) */
          (acc, y) ->
            CASE
              /* the first item, status is NULL, set acc = array(y) */
              WHEN element_at(acc, -1).updated_at is NULL THEN array(y)
              /* if status from the last item of acc != y.status, then push y to acc */
              WHEN element_at(acc, -1).status != y.status THEN concat(acc, array(y))
              /* else skip y, return acc as_is */
              ELSE acc
            END
        ) as status
      """).show(truncate=False)
    +-------------------------------------------------------------------------------+
    |status                                                                         |
    +-------------------------------------------------------------------------------+
    |[[1584579671000, Closed-A], [1584579673000, Closed-B], [1584579674000, Closed]]|
    +-------------------------------------------------------------------------------+
    


(26) use transform and zip_with to split strings based on position patterns
  REF: https://stackoverflow.com/questions/61396133/create-separate-array-element-based-on-condition
  Note: have PHONE, FAX, CELL etc, split them into corresponding items, say:
  ---
    from:
      -2812597115~112211~1123645-CELL~PHONE~FAX-17-TESTB' 
    to: 
      -2812597115-CELL-17-TESTB
      -112211-PHONE-17-TESTB
      -1123645-FAX-17-TESTB

    Code:

    df = spark.createDataFrame([ 
       (['-5594162570~222222-PHONE~FAX-17-TEST', '-2812597115~112211~1123645-CELL~PHONE~FAX-17-TESTB'],),
       (['-5594162570-PHONE-17-TEST'],)
    ], ['typed_phone_numbers']) 

    """
    : first: split typed_phone_numbers into array of arrays using split by pattern '-'
    : second: for each inner array T, split T[1] and T[2] by '~' and use zip_with to concatenate
    :         each corresponding items with '', T[3] and T[4]
    : third: flatten the resulting array of arrays
    """
    df.selectExpr("transform(typed_phone_numbers, x -> split(x,'-')) as typed_phone_numbers") \
        .selectExpr("""
            flatten(
              transform(
                typed_phone_numbers, 
                T -> zip_with(split(T[1],'~'),split(T[2],'~'), (x,y) -> concat_ws('-', '', x, y, T[3], T[4]))
              )
            ) as typed_phone_numbers
         """).show(truncate=False)
    +--------------------------------------------------------------------------------------------------------------------------+
    |typed_phone_numbers                                                                                                       |
    +--------------------------------------------------------------------------------------------------------------------------+
    |[-5594162570-PHONE-17-TEST, -222222-FAX-17-TEST, -2812597115-CELL-17-TESTB, -112211-PHONE-17-TESTB, -1123645-FAX-17-TESTB]|
    |[-5594162570-PHONE-17-TEST]                                                                                               |
    +--------------------------------------------------------------------------------------------------------------------------+



(27) Using aggregate/filter+arrays_zip to filter an array items based on the value at the same indices from another Array column:
  REF: https://stackoverflow.com/questions/61609421/filter-a-list-in-pyspark-dataframe

  Below target is to filter txts which have score >= 0.5

    df=spark.createDataFrame(
      [
        (1, ['foo1','foo2','foo3'],[0.1,0.5,0.6]), # create your data here, be consistent in the types.
        (2, ['bar1','bar2','bar3'],[0.5,0.7,0.7]),
        (3, ['baz1','baz2','baz3'],[0.1,0.2,0.3]),
      ], ['id', 'txt','score'] # add your columns label here
    ) 

    df.selectExpr("*"
      , "filter(arrays_zip(txt,score), x -> x.score >= 0.5).txt as txt_new_1"
      , """aggregate(
             sequence(0,size(txt)-1),
             array(),
             (acc,i) -> IF(score[i] >= 0.5, concat(acc,array(txt[i])), acc)
           ) as txt_new_2
       """).show()
    +---+------------------+---------------+------------------+------------------+
    | id|               txt|          score|         txt_new_1|         txt_new_2|
    +---+------------------+---------------+------------------+------------------+
    |  1|[foo1, foo2, foo3]|[0.1, 0.5, 0.6]|      [foo2, foo3]|      [foo2, foo3]|
    |  2|[bar1, bar2, bar3]|[0.5, 0.7, 0.7]|[bar1, bar2, bar3]|[bar1, bar2, bar3]|
    |  3|[baz1, baz2, baz3]|[0.1, 0.2, 0.3]|                []|                []|
    +---+------------------+---------------+------------------+------------------+




