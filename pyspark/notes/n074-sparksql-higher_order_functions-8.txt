Continue..

Spark SQL higher-order functions:

(25) drop duplicated from an ArrayType column using aggregate:
  REF: https://stackoverflow.com/questions/60959421
 
    fees_data = [[1584579671000, 12.11], [1584579672000, 12.11], [1584579673000, 12.11]];
    fees = [ {"updated_at":u,"fee":f} for u,f in fees_data ]

    status_data = [[1584579671000, "Closed-A"], [1584579672000, "Closed-A"], [1584579673000, "Closed-B"], [1584579674000, "Closed"]];
    status = [ {"updated_at":u,"status":f} for u,f in status_data ]

    schema = "fee:array<struct<updated_at:long,fee:float>>,status:array<struct<updated_at:long,status:string>>"

    df = spark.createDataFrame([(fees, status)], schema=schema)

    df.show(truncate=False, vertical=True)                                                                             
    -RECORD 0------------------------------------------------------------------------------------------------------------
     fee    | [[1584579671000, 12.11], [1584579672000, 12.11], [1584579673000, 12.11]]                                   
     status | [[1584579671000, Closed-A], [1584579672000, Closed-A], [1584579673000, Closed-B], [1584579674000, Closed]]

    root
     |-- fee: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- updated_at: long (nullable = true)
     |    |    |-- fee: float (nullable = true)
     |-- status: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- updated_at: long (nullable = true)
     |    |    |-- status: string (nullable = true)

    df.selectExpr(
      """
        aggregate(
          status,
          /* start: set the initial value of `acc` as an array of structs */
          array((bigint(NULL) as updated_at, string(NULL) as status)),
          /* merge: check and compare y with the last item of `acc` by element_at(acc, -1) */
          (acc, y) ->
            CASE
              /* the first item, status is NULL, set acc = array(y) */
              WHEN element_at(acc, -1).updated_at is NULL THEN array(y)
              /* if status from the last item of acc != y.status, then push y to acc */
              WHEN element_at(acc, -1).status != y.status THEN concat(acc, array(y))
              /* else skip y, return acc as_is */
              ELSE acc
            END
        ) as status
      """).show(truncate=False)
    +-------------------------------------------------------------------------------+
    |status                                                                         |
    +-------------------------------------------------------------------------------+
    |[[1584579671000, Closed-A], [1584579673000, Closed-B], [1584579674000, Closed]]|
    +-------------------------------------------------------------------------------+
    


(26) use transform and zip_with to split strings based on position patterns
  REF: https://stackoverflow.com/questions/61396133/create-separate-array-element-based-on-condition
  Note: have PHONE, FAX, CELL etc, split them into corresponding items, say:
  ---
    from:
      -2812597115~112211~1123645-CELL~PHONE~FAX-17-TESTB' 
    to: 
      -2812597115-CELL-17-TESTB
      -112211-PHONE-17-TESTB
      -1123645-FAX-17-TESTB

    Code:

    df = spark.createDataFrame([ 
       (['-5594162570~222222-PHONE~FAX-17-TEST', '-2812597115~112211~1123645-CELL~PHONE~FAX-17-TESTB'],),
       (['-5594162570-PHONE-17-TEST'],)
    ], ['typed_phone_numbers']) 

    """
    : first: split typed_phone_numbers into array of arrays using split by pattern '-'
    : second: for each inner array T, split T[1] and T[2] by '~' and use zip_with to concatenate
    :         each corresponding items with '', T[3] and T[4]
    : third: flatten the resulting array of arrays
    """
    df.selectExpr("transform(typed_phone_numbers, x -> split(x,'-')) as typed_phone_numbers") \
        .selectExpr("""
            flatten(
              transform(
                typed_phone_numbers, 
                T -> zip_with(split(T[1],'~'),split(T[2],'~'), (x,y) -> concat_ws('-', '', x, y, T[3], T[4]))
              )
            ) as typed_phone_numbers
         """).show(truncate=False)
    +--------------------------------------------------------------------------------------------------------------------------+
    |typed_phone_numbers                                                                                                       |
    +--------------------------------------------------------------------------------------------------------------------------+
    |[-5594162570-PHONE-17-TEST, -222222-FAX-17-TEST, -2812597115-CELL-17-TESTB, -112211-PHONE-17-TESTB, -1123645-FAX-17-TESTB]|
    |[-5594162570-PHONE-17-TEST]                                                                                               |
    +--------------------------------------------------------------------------------------------------------------------------+



(27) Using aggregate/filter+arrays_zip to filter an array items based on the value at the same indices from another Array column:
  REF: https://stackoverflow.com/questions/61609421/filter-a-list-in-pyspark-dataframe

  Below target is to filter txts which have score >= 0.5

    df=spark.createDataFrame(
      [
        (1, ['foo1','foo2','foo3'],[0.1,0.5,0.6]), # create your data here, be consistent in the types.
        (2, ['bar1','bar2','bar3'],[0.5,0.7,0.7]),
        (3, ['baz1','baz2','baz3'],[0.1,0.2,0.3]),
      ], ['id', 'txt','score'] # add your columns label here
    ) 

    df.selectExpr("*"
      , "filter(arrays_zip(txt,score), x -> x.score >= 0.5).txt as txt_new_1"
      , """aggregate(
             sequence(0,size(txt)-1),
             array(),
             (acc,i) -> IF(score[i] >= 0.5, concat(acc,array(txt[i])), acc)
           ) as txt_new_2
       """).show()
    +---+------------------+---------------+------------------+------------------+
    | id|               txt|          score|         txt_new_1|         txt_new_2|
    +---+------------------+---------------+------------------+------------------+
    |  1|[foo1, foo2, foo3]|[0.1, 0.5, 0.6]|      [foo2, foo3]|      [foo2, foo3]|
    |  2|[bar1, bar2, bar3]|[0.5, 0.7, 0.7]|[bar1, bar2, bar3]|[bar1, bar2, bar3]|
    |  3|[baz1, baz2, baz3]|[0.1, 0.2, 0.3]|                []|                []|
    +---+------------------+---------------+------------------+------------------+



(28) use transform to do full-outer-join of two array of stucts columns
  REF: https://stackoverflow.com/questions/61662553/how-can-i-full-outer-join-two-arrays-of-structs-within-a-single-row
    
  Target: take a FULL-OUTER Join of two array of structs columns based on some conditions
  handle it with two separate transforms, one with the left-out-join and another handle all action_values excluded
  from actions and then use CONCAT to merge two array of structs:
    
    df = spark.read.json('/home/xicheng/test/json-31.txt',multiLine=True) 
    
    df.printSchema()                                                                                                   
    root
     |-- action_values: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- action_device: string (nullable = true)
     |    |    |-- action_type: string (nullable = true)
     |    |    |-- value: string (nullable = true)
     |-- actions: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- action_device: string (nullable = true)
     |    |    |-- action_type: string (nullable = true)
     |    |    |-- value: string (nullable = true)
    
    df.createOrReplaceTempView("df_table")
    
    spark.sql("""
    
      SELECT
    
      concat(
        /* actions left join action_values with potentially one-to-multiple matched values */
        flatten(
          transform(actions, x -> 
            transform(
              filter(action_values, y -> y.action_device = x.action_device AND y.action_type = x.action_type), 
              z -> named_struct(
                'action_device', x.action_device,
                'action_type', x.action_type,
                'count', x.value,
                'value', z.value
              )
            )
          )
        ),
        /* action_values missing from actions */
        transform(
          filter(action_values, x -> !exists(actions, y -> x.action_device = y.action_device AND x.action_type = y.action_type)),
          z -> named_struct(
            'action_device', z.action_device,
            'action_type', z.action_type,
            'count', NULL,
            'value', z.value
          )
        )
      ) as result
    
      FROM df_table
    
    """).show(truncate=False)
    


(29) use map_from_entries to merge array of structs into MapType column. 

  REF: https://stackoverflow.com/questions/61780598
  Target: Use map_from_entries to combine an array of structs into MapType column: 
  Notice: the key must not be NULL, thus we added a filter() **

    from pyspark.sql.functions import monotonically_increasing_id, first

    row1 = [['01', '100.0'], ['01', '400.0'], [None, '0.0'], ['06', '0.0'], ['01', '300'], [None, '0.0'], ['06', '200.0']]
    row2 = [[None, '200.0'], ['06', '300.0'], ['01', '500'], ['06', '100.0'], ['01', '200'], ['07', '50.0']]

    df = spark.createDataFrame([(e,) for e in [row1, row2]],['col1'])

  Task-1: More general way, keys are unknown, so we will create a MapType column and then use explode+groupby+pivot
  to pivot the data:

    df1 = df.withColumn('id', monotonically_increasing_id()) \
      .selectExpr("*", """

        explode(
          aggregate(
            col1,
            /* zero_value: find all non-NULL keys from all the first items of inner arrays */
            map_from_entries(
              filter(
                array_distinct(
                  /* convert item to named_struct, using x[0] as key, NULL as value */
                  transform(col1, x -> (x[0] as k, string(NULL) as v))
                )
                , y -> y.k is not NULL
              )
            ),
            (acc, y) ->
              map_from_entries(
                /* iterate through map_keys (using k) and set up values by IF(k=y[0], greatest(y[1],acc[k]), acc[k]) */
                transform(
                  map_keys(acc), 
                  k -> (k as k, IF(k=y[0], greatest(y[1],acc[k]), acc[k]) as v)
                )
              )
          )
        )

    """).groupby('id', 'col1') \
      .pivot('key') \
      .agg(first('value'))

    df1.toDF(*[ c if c in ['id', 'col1'] else 'max_'+c  for c in df1.columns ]).show()
    +---+--------------------+------+------+------+                                 
    | id|                col1|max_01|max_06|max_07|
    +---+--------------------+------+------+------+
    |  1|[[, 200.0], [06, ...|   500| 300.0|  50.0|
    |  0|[[01, 100.0], [01...| 400.0| 200.0|  null|
    +---+--------------------+------+------+------+


  Task-2: if the list are known, then it's much simpler, just use struct in aggregate function: see below:

    df.selectExpr("col1", """ 
      
      aggregate(
        col1, 
        (string(NULL) as max_01, string(NULL) as max_06, string(NULL) as max_07), 
        (acc, y) -> 
          CASE
            WHEN y[0] = '01' THEN (greatest(y[1], acc['max_01']) as max_01, acc['max_06'] as max_06, acc['max_07'] as max_07)
            WHEN y[0] = '06' THEN (acc['max_01'] as max_01, greatest(y[1], acc['max_06']) as max_06, acc['max_07'] as max_07)
            WHEN y[0] = '07' THEN (acc['max_01'] as max_01, acc['max_06'] as max_06, greatest(y[1], acc['max_07']) as max_07)
            ELSE acc
          END 
      ) as col2
          
    """).select("col1", "col2.*").show() 
    +--------------------+------+------+------+
    |                col1|max_01|max_06|max_07|
    +--------------------+------+------+------+
    |[[01, 100.0], [01...| 400.0| 200.0|  null|
    |[[, 200.0], [06, ...|   500| 300.0|  50.0|
    +--------------------+------+------+------+
    



