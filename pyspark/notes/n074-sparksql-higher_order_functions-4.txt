Continue..

Spark SQL higher-order functions:

(12) aggregate and type-casting:
  REF: https://docs.databricks.com/delta/data-transformation/higher-order-lambda-functions.html
  REF: https://stackoverflow.com/questions/59212758/pandas-to-pyspark-cumprod-function

  Note: it's important to type-cast when setting up the initial buffer of the aggregate function, 
        for example `double(1.0)`, `(double(1) as acc, int(0) as cnt)`, otherwise, Spark treat them
        as IntegerType/StringType by default.

    from pyspark.sql.functions import collect_list, col
    from pyspark.sql import Window

    df = spark.createDataFrame([(1, 1,0.9), (1, 2,0.13), (1, 3,0.5), (1, 4,1.0), (1, 5,0.6)], ['col1', 'col2','col3'])

    w1 = Window.partitionBy('col1').orderBy('col2')

    df.withColumn('col4_arr', collect_list(1.0 - col('col3')).over(w1)) \
      .selectExpr(
          'col1'
        , 'col2'
        , 'col4_arr'
        , 'aggregate(col4_arr, double(1.0), (x,y) -> x*y, z -> round(z,4)) as col4'
      ).show(truncate=False)
    +----+----+------------------------------------------+------+                   
    |col1|col2|col4_arr                                  |col4  |
    +----+----+------------------------------------------+------+
    |1   |1   |[0.09999999999999998]                     |0.1   |
    |1   |2   |[0.09999999999999998, 0.87]               |0.087 |
    |1   |3   |[0.09999999999999998, 0.87, 0.5]          |0.0435|
    |1   |4   |[0.09999999999999998, 0.87, 0.5, 0.0]     |0.0   |
    |1   |5   |[0.09999999999999998, 0.87, 0.5, 0.0, 0.4]|0.0   |
    +----+----+------------------------------------------+------+

  Another Way (illustration purpose only):

      df.withColumn('col4_arr', collect_list(1.0 - col('col3')).over(w1)) \
        .withColumn('col4', F.expr("""

            aggregate(
                transform(col4_arr, e -> e*100)
              , (double(1) as acc, int(0) as cnt)
              , (x,y) -> (x.acc*y, x.cnt+1)
              , z -> z.acc / pow(100, z.cnt)
            )
         """)).show(truncate=False)

    Example-12.1: 
 
    df = spark.createDataFrame([
        ([[5.0],[25.0,25.0],[40.0]],"c"), 
        ([[5.0],[25.0, 80.0]],"d"),
        ([[5.0],[25.0, 75.0]],"e")
       ],["B", "C"])

    df.withColumn("Avg", expr("""
        aggregate(
            flatten(B)
          , (double(0) as total, int(0) as cnt) 
          , (x,y) -> (x.total+y, x.cnt+1)
          , z -> round(z.total/z.cnt,2)
        )
     """)).show()
    +-----------------------------+---+-----+
    |B                            |C  |Avg  |
    +-----------------------------+---+-----+
    |[[5.0], [25.0, 25.0], [40.0]]|c  |23.75|
    |[[5.0], [25.0, 80.0]]        |d  |36.67|
    |[[5.0], [25.0, 75.0]]        |e  |35.0 |
    +-----------------------------+---+-----+



(13) array_contains for one-hot-encoding
  REF: https://stackoverflow.com/questions/59222061

    df = spark.createDataFrame([
             (1, 'London|Paris|Tokyo'),
             (2, 'Tokyo|Barcelona|Mumbai|London'),
             (3, 'Vienna|Paris|Seattle')
         ], ['ID','CITY'])

  Method-1: use splt + array_contains:

    df.withColumn('cities', split('CITY', '\|')) \ 
      .select('ID', *[ array_contains('cities', c).astype('int').alias(c) for c in city_of_interest ]) \ 
      .show()

  Method-2: use regexp_replace + find_in_set (reference only)

    df.withColumn('cities', regexp_replace('CITY', '\|', ',')) \
      .selectExpr('ID', *[ 'int(find_in_set("{0}",cities)>0) as `{0}`'.format(c) for c in city_of_interest ]) \
      .show()
    +---+-----+-------+-----+
    | ID|Paris|Seattle|Tokyo|
    +---+-----+-------+-----+
    |  1|    1|      0|    1|
    |  2|    0|      0|    1|
    |  3|    1|      1|    0|
    +---+-----+-------+-----+

  Method using Pandas:

    df[city_of_interest] = df.CITY.str.get_dummies()[city_of_interest]
    df = df.drop('CITY', axis=1)



(14) array_distinct + transform + size: do aggregation
   REF: https://stackoverflow.com/questions/59281773/spark-how-to-get-distinct-values-with-their-count

    from pyspark.sql.functions import collect_list

    df = spark.read.csv('/home/xicheng/test/groupby-1.txt', header=True)

    df.groupby('name').agg(collect_list('food').alias('foods'), collect_list('drink').alias('drinks')) \
      .selectExpr(
          'name' 
        , 'transform(array_distinct(drinks), d -> array(d, size(filter(drinks, x -> x = d)))) as drinks' 
        , 'transform(array_distinct(foods), d -> array(d, size(filter(foods, x -> x = d)))) as foods' 
      ).show(truncate=False) 
    +----+-----------------------+-------------------------+                        
    |name|drinks                 |foods                    |
    +----+-----------------------+-------------------------+
    |Dave|[[soda, 1]]            |[[salad, 1]]             |
    |John|[[water, 2], [soda, 1]]|[[salad, 1], [burger, 2]]|
    +----+-----------------------+-------------------------+

  Notes:
    (1) the following return a struct: array<struct<d:string,col2:int>> 

            transform(drinks, d -> (d, size(filter(drinks, x -> x = d)))) as drinks

        the following is better with named struct:

            transform(array_distinct(drinks), d -> (d as drink, size(filter(drinks, x -> x = d)) as cnt)) as drinks

        the following return array of strings:
            
            transform(drinks, d -> array(d, size(filter(drinks, x -> x = d)))) as drinks

    (2) Scala code for reference:

      import org.apache.spark.sql.functions.collect_list

      val df = Seq(("John", "salad"),("Dave", "salad"),("John", "burger"),("John", "burger")).toDF("name","food")

      val df_new = (df.groupBy("name").agg(collect_list("food").alias("foods")) 
        .selectExpr(
            "name"
          , "transform(array_distinct(foods), d ->array(d, size(filter(foods, x -> x = d)))) as foods"
       ))

       df_new.show(2,0)
       +----+-------------------------+                                                
       |name|foods                    |
       +----+-------------------------+
       |Dave|[[salad, 1]]             |
       |John|[[salad, 1], [burger, 2]]|
       +----+-------------------------+

(14.2) Another similar example finding most frequent element in an ArrayType column: 
  REF: https://stackoverflow.com/questions/63904438/pyspark-column-value-is-a-list-of-string
  Method: 
    1. use array_distinct to find distinct elements in array `values`
    2. use transform to convert (1) to array of structs with two fields: cnt and val
       where cnt is calculated by size + filter and val is the element value in (1)
    3. take the array_max of (2) and its `val` field

  Code:

    df = spark.createDataFrame([
       (1,['good','good','good','bad','bad','good','good']),
       (2,['bad','badd','good','bad',Null,'good','bad'])
    ], ['id', 'values'])

    df.selectExpr("*", """
        array_max(
          transform(
            array_distinct(values), x-> (size(filter(values, y -> y=x)) as cnt, x as val)
          )
        ).val as most_frequent
    """).show(2,60)
    +---+----------------------------------------+-------------+
    | id|                                  values|most_frequent|
    +---+----------------------------------------+-------------+
    |  1|[good, good, good, bad, bad, good, good]|         good|
    |  2|      [bad, badd, good, bad,, good, bad]|          bad|
    +---+----------------------------------------+-------------+
    


(15) transform + filter to find the first non-null values in a embedded struct/array

    REF: https://stackoverflow.com/questions/59316791/json-iteration-in-spark
    Find the first non-null value in a array fields using transform + filter SQL using LATERAL VIEW + inline function

    Spark version 2.4+:
    
    val df = spark.read.option("multiLine","true").json("/home/xicheng/test/json-6.txt")
    
    # Method-1: using API functions:
    
      val df1 = df.selectExpr("inline(CarBrands)")
    
      df1.selectExpr(
            "model"
          , "transform(filter(dealerspot, x -> x.dealername[0] is not null), y -> y.dealername[0])[0] AS dealername"
          , "transform(filter(dealerspot, x -> x.dealerlat[0] is not null), y -> y.dealerlat[0])[0] AS dealerlat"
      ).show
      +-----+----------+---------+
      |model|dealername|dealerlat|
      +-----+----------+---------+
      | audi|   "first"|  "45.00"|
      |  bmw|  "sports"|  "99.00"|
      |  toy|  "nelson"|  "35.00"|
      +-----+----------+---------+
      
    
    Mehotd-2: using Spark SQL:
    
      df.createOrReplaceTempView("df_table")
    
      spark.sql("""
    
          SELECT m.model
          ,      trim(BOTH '"' FROM 
                   transform(
                       filter(m.dealerspot, x -> x.dealername[0] is not null)
                     , y -> y.dealername[0]
                   )[0]
                 ) AS dealername
          ,      trim(BOTH '"' FROM
                   transform(
                       filter(m.dealerspot, x -> x.dealerlat[0] is not null)
                     , y -> y.dealerlat[0]
                   )[0]
                 ) AS dealerlat
          FROM df_table LATERAL VIEW OUTER inline(CarBrands) m
    
      """).show
      +-----+----------+---------+
      |model|dealername|dealerlat|
      +-----+----------+---------+
      | audi|     first|    45.00|
      |  bmw|    sports|    99.00|
      |  toy|    nelson|    35.00|
      +-----+----------+---------+

    Method-3: using get_json_object + to_json

      import org.apache.spark.sql.functions.{get_json_object,to_json,trim,explode}

      val df1 = (df.withColumn("data", explode($"CarBrands")) 
          .select("data.*") 
          .withColumn("dealerspot", to_json($"dealerspot")))

      df1.select(
            $"model"
          , trim(get_json_object($"dealerspot", "$[*].dealername[0]"), "\"\\") as "dealername"
          , trim(get_json_object($"dealerspot", "$[*].dealerlat[0]"), "\"\\") as "dealerlat"
      ).show

