Continue..

Spark SQL higher-order functions:

(12) aggregate and type-casting:
  REF: https://docs.databricks.com/delta/data-transformation/higher-order-lambda-functions.html
  REF: https://stackoverflow.com/questions/59212758/pandas-to-pyspark-cumprod-function

  Note: it's important to type-cast when setting up the initial buffer of the aggregate function, 
        for example `double(1.0)`, `(double(1) as acc, int(0) as cnt)`, otherwise, Spark treat them
        as IntegerType/StringType by default.

    from pyspark.sql.functions import collect_list, col
    from pyspark.sql import Window

    df = spark.createDataFrame([(1, 1,0.9), (1, 2,0.13), (1, 3,0.5), (1, 4,1.0), (1, 5,0.6)], ['col1', 'col2','col3'])

    w1 = Window.partitionBy('col1').orderBy('col2')

    df.withColumn('col4_arr', collect_list(1.0 - col('col3')).over(w1)) \
      .selectExpr(
          'col1'
        , 'col2'
        , 'col4_arr'
        , 'aggregate(col4_arr, double(1.0), (x,y) -> x*y, z -> round(z,4)) as col4'
      ).show(truncate=False)
    +----+----+------------------------------------------+------+                   
    |col1|col2|col4_arr                                  |col4  |
    +----+----+------------------------------------------+------+
    |1   |1   |[0.09999999999999998]                     |0.1   |
    |1   |2   |[0.09999999999999998, 0.87]               |0.087 |
    |1   |3   |[0.09999999999999998, 0.87, 0.5]          |0.0435|
    |1   |4   |[0.09999999999999998, 0.87, 0.5, 0.0]     |0.0   |
    |1   |5   |[0.09999999999999998, 0.87, 0.5, 0.0, 0.4]|0.0   |
    +----+----+------------------------------------------+------+

  Another Way (illustration purpose only):

      df.withColumn('col4_arr', collect_list(1.0 - col('col3')).over(w1)) \
        .withColumn('col4', F.expr("""

            aggregate(
                transform(col4_arr, e -> e*100)
              , (double(1) as acc, int(0) as cnt)
              , (x,y) -> (x.acc*y, x.cnt+1)
              , z -> z.acc / pow(100, z.cnt)
            )
         """)).show(truncate=False)

