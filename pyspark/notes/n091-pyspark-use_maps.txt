Using mapping in pyspark:

discover different ways using maps in pyspark code.
---
Some notes:
 (1) explode('map_col') will add two columns with name `key` and `value`, 
     SQL syntax: `explode('map_col') as (key_name, value_name)`

 (2) Sort by categories list:  
    REF: https://stackoverflow.com/questions/62015149
     
       categories=['insert', 'seed', 'update', 'snapshot', 'delete']
       map1 = create_map([ m for i,e in enumerate(categories) for m in [e, lit(i)] ])
       df.orderBy(map1[col('b')].desc()).show()

 (3) compare two MapType columns, if no duplicate keys exist, then convert to sorted array 
     of structs and then do comparison:

       sort_array(transform(map_keys(map1), x -> (x, map1[x]))) = sort_array(transform(map_keys(map2), x -> (x, map2[x])))
 


Example-1: Use Python dict to fillna different value for each column:
    Note: the same column will have the same fill_value
    REF: https://stackoverflow.com/questions/58282826

    # create mapping based on data in D2
    mapping = { row.col_name.replace(' ',''):row.value for row in D2.collect() }

    # using reduce function and fillna with subset=
    df_new =  reduce(lambda d,c: d.fillna(mapping[c], subset=[c]), D1.columns, D1)

    #or using list_comprehension
    from pyspark.sql.functions import isnan, when, col
    df_new = D1.select([ when(isnan(c), mapping[c]).otherwise(col(c)).alias(c) for c in D1.columns ])

    
    Example-1.2: A similar post: change value with freqItems() 
        REF: https://stackoverflow.com/questions/58286674

        # use df.freqItems() to find most frequent items in a column and map colu_name to this value
        d1 = df.freqItems(df.columns)
        mapping = d1.select([ F.col(c)[0].alias(c.split('_')[0]) for c in d1.columns ]).first().asDict()
        df_new = df.select([ when(col(c) == 'nn', mapping[c]).otherwise(col(c)).alias(c) for c in df.columns ])

        Notice: the array items from df.freqItems() are unsorted, can not use item-0 for the most-frequent items.


Example-2: Use mapping created from F.create_map() or even F.array()
    Note: different values should be filled with different fill_values
    
    df = spark.createDataFrame([(0,), (3,), (7,)],['event_type'])

    Example-2.1: use F.create_map and a dict:

        from itertools import chain

        d = {0:'Created', 3:'c3', 7:'Deleted'}

        mapping = F.create_map([F.lit(i) for i in chain.from_iterable(d.items())])
        #Column<b'map(0, Created, 3, c3, 7, Deleted)'>

        df.withColumn('values', mapping[F.col('event_type')]).show()
        +----------+-------+
        |event_type| values|
        +----------+-------+
        |         0|Created|
        |         3|     c3|
        |         7|Deleted|
        +----------+-------+

    Example-2.2: use F.array and a List:
    
        lst = [ 'Created', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'Deleted']

        mapping_a = F.array([F.lit(i) for i in lst])
        # Column<b'array(Created, c1, c2, c3, c4, c5, c6, Deleted)'>

        df.withColumn('values', mapping_a[F.col('event_type')]).show()


Example-3: use Spark SQL builtin function `str_to_map` to create Map:
     REF: https://stackoverflow.com/questions/59675879/pyspark-split-spark
    
    Below convert a StringType column `dsp_price_style` into a MapType column and then retrieve the desired keys:
    
    from pyspark.sql.functions import expr
    
    df = spark.createDataFrame([
        (0,"1000:10,1001:100,1002:5,1003:7"),
        (1,"1002:5,1000:100,1001:15,1003:6")
      ], ['req_id', 'dsp_price_style'])
    
    df.withColumn('dsp_price_map', expr("str_to_map(dsp_price_style,',',':')")) \
        .selectExpr(
          'req_id', 
          *sum([ ['{0} as {0}_key'.format(k), 'dsp_price_map["{0}"] as {0}_val'.format(k) ] for k in keys ], [])
    ).show() 
    +------+--------+--------+--------+--------+
    |req_id|1000_key|1000_val|1001_key|1001_val|
    +------+--------+--------+--------+--------+
    |     0|    1000|      10|    1001|     100|
    |     1|    1000|     100|    1001|      15|
    +------+--------+--------+--------+--------+
    

Example-4: use MapType in the aggregate function of groupby
  REF:https://stackoverflow.com/questions/60061916/using-pyspark-window-functions-with-conditions-to-add-rows

  Use groupby and collect_set + map_from_entries to create map1 containing variable -> value mapping
  and then calculate D_value use map1, add this as a new entry 'varD' into map1 and then explode this into Rows.

    from pyspark.sql.functions import expr
    
    df.groupby('id') \
      .agg(expr("map_from_entries(collect_set(struct(variable,value))) as map1")) \
      .withColumn('D_value', expr('IF(map1["varA"] < 16 AND map1["varC"] != -9,2,1)')) \
      .selectExpr("id", "explode_outer(map_concat(map1, map('varD', D_value))) AS (variable,value)") \
      .show(truncate=False)
    +---+--------+-----+
    |id |variable|value|
    +---+--------+-----+
    |1  |varB    |1    |
    |1  |varC    |-9   |
    |1  |varA    |30   |
    |1  |varD    |1    |
    +---+--------+-----+
    
  WHERE:
  (1) groupby id and create a MapType column `map1` using `map_from_entries(collect_set(struct(variable,value)))`
  (2) calculate D_value using this MapType column
  (3) Add a new entry map('varD', D_value) to this MapType column
  (4) explode the resulting Map.


Example-5: use SQL expression CASE/WHEN statement to handle the mapping from one to another:
  
  We have a mapping and want to convert matched items in an ArrayType column

    mappings = { 
      'PastNews': 'ContextualInformation', 
      'ContinuingNews': 'News', 
      'KnownAlready': 'OriginalEvent', 
      'SignificantEventChange': 'NewSubEvent', 
    }          

    sql_epxr = "transform(Categories, x -> CASE x {} ELSE x END)".format(
        " ".join("WHEN '{}' THEN '{}'".format(k,v) for k,v in mappings.items())
    )
    # this yields the following SQL expression:
    # transform(Categories, x -> 
    #    CASE x 
    #      WHEN 'PastNews' THEN 'ContextualInformation' 
    #      WHEN 'ContinuingNews' THEN 'News' 
    #      WHEN 'KnownAlready' THEN 'OriginalEvent' 
    #      WHEN 'SignificantEventChange' THEN 'NewSubEvent' 
    #      ELSE x 
    #    END
    # )

    df.withColumn('Categories_new', F.expr(sql_epxr)).show(truncate=False)                                             
    +------------------+---------------------------------------------+-----------------------------------------+
    |PostID            |Categories                                   |Categories_new                           |
    +------------------+---------------------------------------------+-----------------------------------------+
    |266269932671606786|[EmergingThreats, Factoid, KnownAlready]     |[EmergingThreats, Factoid, OriginalEvent]|
    |266804609954234369|[Donations, ServiceAvailable, ContinuingNews]|[Donations, ServiceAvailable, News]      |
    +------------------+---------------------------------------------+-----------------------------------------+

  Notes: this applies also to regular StringType columns. So instead of creating a MapType column using F.create_map()
         function, we can use SQL CASE/WHEN or IF() statements to do the mapping:

    from pyspark.sql import functions as F

    df = spark.createDataFrame([('F',44),('M',35),(None,34),('',23)],['Gender','count'])

    sql_expr = "CASE Gender {} ELSE 'missing' END".format(
        " ".join("WHEN '{}' THEN '{}'".format(k,v) for k,v in map1.items())
    )

    df.withColumn('Gender_new', F.expr(sql_expr)).show()                                                               
    +------+-----+----------+
    |Gender|count|Gender_new|
    +------+-----+----------+
    |     F|   44|    Female|
    |     M|   35|      Male|
    |  null|   34|   missing|
    |      |   23|   missing|
    +------+-----+----------+



