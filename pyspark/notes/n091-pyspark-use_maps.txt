Using mapping in pyspark:

discover different ways using maps in pyspark code.
---
Some notes:
 (1) explode('map_col') will add two columns with name `key` and `value`
     similar to inline(struct_col) in SparkSQL



Example-1: Use Python dict to fillna different value for each column:
    Note: the same column will have the same fill_value
    REF: https://stackoverflow.com/questions/58282826

    # create mapping based on data in D2
    mapping = { row.col_name.replace(' ',''):row.value for row in D2.collect() }

    # using reduce function and fillna with subset=
    df_new =  reduce(lambda d,c: d.fillna(mapping[c], subset=[c]), D1.columns, D1)

    #or using list_comprehension
    from pyspark.sql.functions import isnan, when, col
    df_new = D1.select([ when(isnan(c), mapping[c]).otherwise(col(c)).alias(c) for c in D1.columns ])

    
    Example-1.2: A similar post: change value with freqItems() 
        REF: https://stackoverflow.com/questions/58286674

        # use df.freqItems() to find most frequent items in a column and map colu_name to this value
        d1 = df.freqItems(df.columns)
        mapping = d1.select([ F.col(c)[0].alias(c.split('_')[0]) for c in d1.columns ]).first().asDict()
        df_new = df.select([ when(col(c) == 'nn', mapping[c]).otherwise(col(c)).alias(c) for c in df.columns ])

        Notice: the array items from df.freqItems() are unsorted, can not use item-0 for the most-frequent items.


Example-2: Use mapping created from F.create_map() or even F.array()
    Note: different values should be filled with different fill_values
    
    df = spark.createDataFrame([(0,), (3,), (7,)],['event_type'])

    Example-2.1: use F.create_map and a dict:

        from itertools import chain

        d = {0:'Created', 3:'c3', 7:'Deleted'}

        mapping = F.create_map([F.lit(i) for i in chain.from_iterable(d.items())])
        #Column<b'map(0, Created, 3, c3, 7, Deleted)'>

        df.withColumn('values', mapping[F.col('event_type')]).show()
        +----------+-------+
        |event_type| values|
        +----------+-------+
        |         0|Created|
        |         3|     c3|
        |         7|Deleted|
        +----------+-------+

    Example-2.2: use F.array and a List:
    
        lst = [ 'Created', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'Deleted']

        mapping_a = F.array([F.lit(i) for i in lst])
        # Column<b'array(Created, c1, c2, c3, c4, c5, c6, Deleted)'>

        df.withColumn('values', mapping_a[F.col('event_type')]).show()


Example-3: use Spark SQL builtin function `str_to_map` to create Map:
     REF: https://stackoverflow.com/questions/59675879/pyspark-split-spark
    
    Below convert a StringType column `dsp_price_style` into a MapType column and then retrieve the desired keys:
    
    from pyspark.sql.functions import expr
    
    df = spark.createDataFrame([
        (0,"1000:10,1001:100,1002:5,1003:7"),
        (1,"1002:5,1000:100,1001:15,1003:6")
      ], ['req_id', 'dsp_price_style'])
    
    df.withColumn('dsp_price_map', expr("str_to_map(dsp_price_style,',',':')")) \
        .selectExpr(
          'req_id', 
          *sum([ ['{0} as {0}_key'.format(k), 'dsp_price_map["{0}"] as {0}_val'.format(k) ] for k in keys ], [])
    ).show() 
    +------+--------+--------+--------+--------+
    |req_id|1000_key|1000_val|1001_key|1001_val|
    +------+--------+--------+--------+--------+
    |     0|    1000|      10|    1001|     100|
    |     1|    1000|     100|    1001|      15|
    +------+--------+--------+--------+--------+
    

Example-4: use MapType in the aggregate function of groupby
  REF:https://stackoverflow.com/questions/60061916/using-pyspark-window-functions-with-conditions-to-add-rows

  Use groupby and collect_set + map_from_entries to create map1 containing variable -> value mapping
  and then calculate D_value use map1, add this as a new entry 'varD' into map1 and then explode this into Rows.

    from pyspark.sql.functions import expr
    
    df.groupby('id') \
      .agg(expr("map_from_entries(collect_set(struct(variable,value))) as map1")) \
      .withColumn('D_value', expr('IF(map1["varA"] < 16 AND map1["varC"] != -9,2,1)')) \
      .selectExpr("id", "explode_outer(map_concat(map1, map('varD', D_value))) AS (variable,value)") \
      .show(truncate=False)
    +---+--------+-----+
    |id |variable|value|
    +---+--------+-----+
    |1  |varB    |1    |
    |1  |varC    |-9   |
    |1  |varA    |30   |
    |1  |varD    |1    |
    +---+--------+-----+
    
  WHERE:
  (1) groupby id and create a MapType column `map1` using `map_from_entries(collect_set(struct(variable,value)))`
  (2) calculate D_value using this MapType column
  (3) Add a new entry map('varD', D_value) to this MapType column
  (4) explode the resulting Map.


