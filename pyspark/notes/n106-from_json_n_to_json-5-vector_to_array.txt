Using from_json and to_json to convert an Spark ML Vector column into an ArrayType column without using UDF

---
Code example see below:
    
    from pyspark.sql.functions import from_json, to_json, array
    from pyspark.ml.linalg import Vectors 
    
    df = spark.createDataFrame([
        (Vectors.dense([0.01,0.98,0.0]),), 
        (Vectors.dense([0.12,0.82,0.06]),),
        (Vectors.sparse(3, [0,2], [0.88,0.12]),),
        (Vectors.sparse(3, [2], [1.02]),)
    ], ["vec"]) 
    
    # schema for ArrayType(VectorUDT())
    schema = 'array<struct<type:int,size:int,indices:array<int>,values:array<double>>>'
    
    df1 = df.withColumn('data', from_json(to_json(array('vec')), schema)[0])
    df1.show(truncate=False)                                                                                           
    +---------------------+--------------------------------+
    |vec                  |data                            |
    +---------------------+--------------------------------+
    |[0.01,0.98,0.0]      |[1,,, [0.01, 0.98, 0.0]]        | <-- dense
    |[0.12,0.82,0.06]     |[1,,, [0.12, 0.82, 0.06]]       | <-- dense 
    |(3,[0,2],[0.88,0.12])|[0, 3, [0.0, 2.0], [0.88, 0.12]]| <-- sparse
    |(3,[2],[1.02])       |[0, 3, [2.0], [1.02]]           | <-- sparse
    +---------------------+--------------------------------+
    
    df1.printSchema()                                                                                                  
    root
     |-- vec: vector (nullable = true)
     |-- data: struct (nullable = true)
     |    |-- type: integer (nullable = true)
     |    |-- size: integer (nullable = true)
     |    |-- indices: array (nullable = true)
     |    |    |-- element: float (containsNull = true)
     |    |-- values: array (nullable = true)
     |    |    |-- element: float (containsNull = true)
    
    df_new = df1.selectExpr("vec", """
       /* data.type == 1, dense vector, take data.values */
       IF(data.type = 1
         , data.values
         /* otherwise, take sequence from 0 to data.size-1 and convert it into
           array with i from data.indices and corresponding value in data.values
           using array_position(1-based)
         */
         , transform(sequence(0,data.size-1), i -> IFNULL(data.values[int(array_position(data.indices, i))-1],0)) 
       ) as arr
    """)
    df_new.show(truncate=False)                                                                                        
    +---------------------+------------------+
    |vec                  |arr               |
    +---------------------+------------------+
    |[0.01,0.98,0.0]      |[0.01, 0.98, 0.0] |
    |[0.12,0.82,0.06]     |[0.12, 0.82, 0.06]|
    |(3,[0,2],[0.88,0.12])|[0.88, 0.0, 0.12] |
    |(3,[2],[1.02])       |[0.0, 0.0, 1.02]  |
    +---------------------+------------------+

    df_new.printSchema()                                                                                               
    root
     |-- vec: vector (nullable = true)
     |-- arr: array (nullable = true)
     |    |-- element: double (containsNull = true)


**WHERE:**

(1) use `to_json` to convert array('vec') into StringType column, note, to_json can not be directly apply to
`pyspark.ml.linalg.VectorUDT`, argument must be one of the Complex datatype: ArrayType, StructType or MapType.

(2) The VectorUDT takes a StructType with 4 fields: 
  + type: 0 for Sparse Vector and 1 for Dense Vector
  + size: for SparseVector only
  + indices: for SparseVector only, array of indices for items with non-zero values
  + values: all values for Dense Vector, non-zero values for Sparse Vector

(3) Use from_json and the following schema to retrieve the 4 fields mentioned above:

    array<struct<type:int,size:int,indices:array<int>,values:array<double>>>

(4) For Dense Vector where type == 1, take the values array directly

(5) For Sparse Vector, create a sequence from `0` to `size-1`, iterate through this list by `i`
    use `array_position(indices, i)` to find the index of `i` in `indices` array (return 0 if not exist)
    and retrieve the corresponding value at the same index in `values` array.
    (notice array_position is 1-based index, while Array reference is 0-based). 

        IFNULL(values[array_position(indices,i) - 1], 0)

    for i not in indices, `values[array_position(indices,i) - 1]` becomes values[-1] which is always NULL


**Notes:**

(1) if the column contains only Dense Vector, then it can be simplified using from_json

    df = spark.createDataFrame([
        (Vectors.dense([0.01,0.98,0.0]),), 
        (Vectors.dense([0.12,0.82,0.06]),)
    ], ["vec"]) 

    df_new = df.selectExpr("vec", "from_json(string(vec), 'array<double>') as arr")
    #DataFrame[vec: vector, arr: array<double>]

    df_new.show()
    +----------------+------------------+
    |vec             |arr               |
    +----------------+------------------+
    |[0.01,0.98,0.0] |[0.01, 0.98, 0.0] |
    |[0.12,0.82,0.06]|[0.12, 0.82, 0.06]|
    +----------------+------------------+

(2) With Spark 3.0+, use pyspark.ml.functions.vector_to_array    <-- Need more info

   see ref: SPARK-30762  https://issues.apache.org/jira/browse/SPARK-30762
            https://github.com/apache/spark/blob/master/python/pyspark/ml/functions.py
            https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/functions.scala
   other link: https://stackoverflow.com/questions/38384347/how-to-split-vector-into-columns-using-pyspark

   Notice this function is not available with 3.0-preview-2 and before

##############
Another Example: https://stackoverflow.com/questions/60431464/how-to-get-item-from-vector-struct-in-pyspark

  Below example retrieve values only from the SparseVectors.

    from pyspark.ml.linalg import SparseVector
    from pyspark.sql.functions import array, to_json, from_json
    
    df = spark.createDataFrame([
      (list("abc"),SparseVector(4527,[0,1,31],[0.6363067860791387,1.0888040725098247,4.371858972705023])),
      (['a'], SparseVector(4527,[8],[2.729945780576634])
    ],['words', 'features'])
    
    # only values field is needed: 
    schema = "array<struct<values:array<double>>>"
    
    df.withColumn('features', from_json(to_json(array('features')), schema)[0].values).show(truncate=False)
    +---------+-----------------------------------------------------------+
    |words    |features                                                   |
    +---------+-----------------------------------------------------------+
    |[a, b, c]|[0.6363067860791387, 1.0888040725098247, 4.371858972705023]|
    |[a]      |[2.729945780576634]                                        |
    +---------+-----------------------------------------------------------+

