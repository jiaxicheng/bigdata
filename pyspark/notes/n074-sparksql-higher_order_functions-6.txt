Continue..

Spark SQL higher-order functions:

(19) using aggregate to calculate stateful values from the previous rows
  REF: https://stackoverflow.com/questions/60109004/can-we-dynamically-retrieve-previous-rows-value-of-a-updating-column-in-pyspark
  Using the same method shown in https://github.com/jiaxicheng/bigdata/blob/master/pyspark/notes/n053-window_with_gaps_using_aggregate-2-array.txt

    from pyspark.sql.functions import array_sort, collect_list, struct

    df = spark.createDataFrame([(1,5),(3,6),(7,10),(13,17), (15,20)],['START', 'END'])

    df.agg(array_sort(collect_list(struct('START', 'END'))).alias('data'))\
      .selectExpr("""
        inline_outer(
          aggregate(
            data,
            array((bigint(NULL) as start, bigint(NULL) as end)),
            (acc, y) ->
              CASE
                WHEN element_at(acc, -1).start is NULL THEN
                  array((y.START as start, y.END as end))
                WHEN element_at(acc, -1).end > y.start THEN
                  concat(acc,
                    array((element_at(acc, -1).end as start, element_at(acc, -1).end + y.END - y.START as end))
                  )
                ELSE
                  concat(acc, array((y.START as start, y.END as end)))
              END
          )
        )
        
      """).show()
    +-----+---+
    |start|end|
    +-----+---+
    |    1|  5|
    |    5|  8|
    |    8| 11|
    |   13| 17|
    |   17| 22|
    +-----+---+


    
(20) array_distinct + transform + filter + size: CountVectorizer row-wise
  REF: https://stackoverflow.com/questions/60119554/string-indexer-countvectorizer-pyspark-on-single-row
  ---
    (1) find vacubulary using array_distinct (merge two arrays with concat)
    (2) use transform to iterate through this vacubulary and use filter+size to count each item

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([(["a", "b" ,"b", "c"], ["a","b", "x", "y"])],['column1', 'column2'])

    df.withColumn('vocabulary', expr('array_distinct(concat(column1,column2))')) \
      .withColumn('a1', expr("transform(vocabulary, x -> size(filter(column1, y -> y = x)))")) \
      .withColumn('a2', expr("transform(vocabulary, x -> size(filter(column2, y -> y = x)))")) \
      .show()
    +------------+------------+---------------+---------------+---------------+
    |     column1|     column2|     vocabulary|             a1|             a2|
    +------------+------------+---------------+---------------+---------------+
    |[a, b, b, c]|[a, b, x, y]|[a, b, c, x, y]|[1, 2, 1, 0, 0]|[1, 1, 0, 1, 1]|
    +------------+------------+---------------+---------------+---------------+



(21) using aggregate with MapType columns
  REF: https://stackoverflow.com/questions/60405497/calculate-rolling-sum-of-array-in-pyspark-and-save-as-dict
  ---
  (1) use array_distinct to find all distinct keys in array column
  (2) use transform to convert array item into a MapType with a single key
  (3) use aggregate and map_concat to conver array of maps into a map:
  
      aggregate(arr_of_maps, map(), (acc, y) -> map_concat(acc, y))

  Python Code:

    from pyspark.sql import Window
    from pyspark.sql.functions import collect_list, flatten

    df = spark.createDataFrame([(2,[1,2,3]), (2,[1,2,4]), (3,[1,2]), (4,[1,3]), (5,[1,3])],['timestamp', 'vars'])

    w1 = Window.partitionBy().orderBy('timestamp')

    df.withColumn("all_vals", flatten(collect_list("vars").over(w1))) \
      .selectExpr("timestamp", """
        aggregate(
          transform(
            array_distinct(all_vals), x -> map(x, size(filter(all_vals, y -> y = x)))
          ), 
          map(), 
          (acc, y) -> map_concat(acc, y)
        ) as new_vars
    """).show(truncate=False) 
    +---------+--------------------------------+
    |timestamp|new_vars                        |
    +---------+--------------------------------+
    |1        |[1 -> 1, 2 -> 1, 3 -> 1]        |
    |2        |[1 -> 2, 2 -> 2, 3 -> 1, 4 -> 1]|
    |3        |[1 -> 3, 2 -> 3, 3 -> 1, 4 -> 1]|
    |4        |[1 -> 4, 2 -> 3, 3 -> 2, 4 -> 1]|
    |5        |[1 -> 5, 2 -> 3, 3 -> 3, 4 -> 1]|
    +---------+--------------------------------+

  Method-2: using map_from_arrays:

    from pyspark.sql.functions import collect_list, flatten, array_distinct, expr

    df.withColumn("all_vals", flatten(collect_list("vars").over(w1))) \
      .withColumn("all_keys", array_distinct("all_vals")) \
      .withColumn("all_values", expr("transform(all_keys, x -> size(filter(all_vals, y -> y = x)))")) \
      .selectExpr("timestamp", "map_from_arrays(all_keys, all_values) as new_vals") \
      .show(truncate=False)
    +---------+--------------------------------+
    |timestamp|new_vals                        |
    +---------+--------------------------------+
    |1        |[1 -> 1, 2 -> 1, 3 -> 1]        |
    |2        |[1 -> 2, 2 -> 2, 3 -> 1, 4 -> 1]|
    |3        |[1 -> 3, 2 -> 3, 3 -> 1, 4 -> 1]|
    |4        |[1 -> 4, 2 -> 3, 3 -> 2, 4 -> 1]|
    |5        |[1 -> 5, 2 -> 3, 3 -> 3, 4 -> 1]|
    +---------+--------------------------------+

  Method-3: With Spark 3.0+, use aggregate + transform_values:

    from pyspark.sql.functions import collect_list, flatten

    df.withColumn("all_vals", flatten(collect_list("vars").over(w1))) \
      .selectExpr("timestamp", """
        aggregate(
          all_vals, 
          /* EMPTY map as zero values */
          map(), 
          /* merge: if y not in acc, add map(y,1), else adjust acc[y] += 1 using transform_values */
          (acc, y) -> 
            IF(acc[y] is NULL
             , map_concat(acc, map(y,1))
             , transform_values(acc, (k,v) -> int(IF(k=y,v+1,v)))
            )
        ) as new_vals 
    """).show(truncate=False)
    +---------+--------------------------------+
    |timestamp|new_vals                        |
    +---------+--------------------------------+
    |1        |[1 -> 1, 2 -> 1, 3 -> 1]        |
    |2        |[1 -> 2, 2 -> 2, 3 -> 1, 4 -> 1]|
    |3        |[1 -> 3, 2 -> 3, 3 -> 1, 4 -> 1]|
    |4        |[1 -> 4, 2 -> 3, 3 -> 2, 4 -> 1]|
    |5        |[1 -> 5, 2 -> 3, 3 -> 3, 4 -> 1]|
    +---------+--------------------------------+




