Continue..

Spark SQL higher-order functions:


(11) Solving the window with Gap issues using aggregate function of Spark 2.4+
---
Steps:
  (1) groupby and create an sorted ArrayType column with collect_list() and sort_array()
  (2) Using aggregate function, protocal see below:

      case class ArrayAggregate(
         argument: Expression,
         zero: Expression,
         merge: Expression,
         finish: Expression

  (3) argument is the ArrayType column from array_sort(collect_list)
      we take a slice of this array from the 2nd element to the last.

    Notice: using slice(dta,2,size(dta)-1) does NOT handle NULL well, for example when dta = array()
      with EMPTY array, solution:
       (3.1) using dta and then cast() the zero_value is probably a better solution, see example-11-6
       (3.2) using size(dta) ignoring `-1`:
     
          slice(dta,2,size(dta))

  (4) zero and merge experssion must have the same dataType and Nullability (containsNull in metadata)
     Notes: Nullability is only ignored on the first level of dataType, if you have
        an array of structs (like we had in most examples here), you will need to make
        sure zero part and merge part have the same Nullability. 
     The simplest solution is to set all initial values to NULL in zero expression
     or add type cast(for example in this example)

  (5) merge is the part to implement the logic with SQL statement, usually WHEN/CASE/END or IF statement
  (6) For array of structs, use inline to explode and split the fields.


Example-11-1: count number of records every 4 hours, the 4 hour window must have no overlap
           but might have overlap based on the actual data

  REF: https://stackoverflow.com/questions/58221350/pyspark-how-to-aggregate-over-4-hours-windows-in-groups

  Below example use an array<struct<start:timestamp,cnt:int>>

    from pyspark.sql.functions import expr, sort_array, collect_list
    
    df = spark.read.csv('/home/xicheng/test/window-9.txt', header=True)
    
    df1 = (df.withColumn('pdate', expr("to_timestamp(concat(Date_of_purchase, ' ', time_of_purchase),'MM/dd/yy h:mm a')"))
        .groupby('email')
        .agg(sort_array(collect_list('pdate')).alias('data')))
    
    df1.selectExpr('email', """ 
       inline( 
         aggregate(
           slice(data,2,size(data)-1),
           cast(array((data[0] as start, 1 as cnt)) as array<struct<start:timestamp,cnt:int>>),
           (acc, y) -> 
             IF(element_at(acc, -1).start + INTERVAL 4 HOURS >= y,
               concat(slice(acc,1,size(acc)-1),
                 array((element_at(acc, -1).start as start, element_at(acc, -1).cnt+1 as cnt))
               ),
               concat(acc, array((y as start, 1 as cnt)))
             )
         )
       )
     """).show() 
    +-------------+-------------------+---+
    |        email|              start|cnt|
    +-------------+-------------------+---+
    |def@gmail.com|2018-11-10 12:17:00|  2|
    |def@gmail.com|2018-11-10 20:16:00|  3|
    |abc@gmail.com|2018-11-10 12:10:00|  3|
    |abc@gmail.com|2018-11-11 06:16:00|  2|
    +-------------+-------------------+---+


Example-11-2: Using aggregate function and concat an array to do the aggregation/reduce:
  REF: https://stackoverflow.com/questions/57186107/pyspark-iterate-over-groups-in-a-dataframe
  Method:
   (1) Use groupby + sort_array + collect_list + struct to create a sorted array column `data`
   (2) Use aggregate function to iterate through this array column from the 2nd element to the last
       using `slice(data,2,size(data)-1)`
   (3) initialize start acc with array(data[0])
   (4) if y.date is between element_at(acc, -1).LB AND date_add(element_at(acc, -1).LB, 20), use the existing LB
       so we append (Updated_date as Updated_date, y.date as date, element_at(acc, -1).LB as LB) to `acc`
       otherwise append y to `acc`
   (5) Use inline function to explode the array of structs
   (6) calculate UB which is date_add(LB, 20)

  Code:

    from pyspark.sql.functions import array_sort, struct, collect_list

    df = spark.read.csv('/home/xicheng/test/window-8.txt', header=True, inferSchema=True)
    
    df1 = (df.selectExpr('*', 'date_sub(date,10) as LB') 
        .groupby('id')
        .agg(array_sort(collect_list(struct('Updated_date', 'date', 'LB'))).alias('data')))  
    
    df_new = df1.selectExpr("id", """ 
    
       inline(   
         aggregate(
           slice(data,2,size(data)-1), 
           /* initialize is important, specify data types of all fields */
           array(data[0]),
           (acc, y) -> 
             concat(acc,
               /* if y.date is in the existing [LB, UB], then use existing boundary
                * else use y.LB as the new LB
                */
               IF(y.date BETWEEN element_at(acc, -1).LB AND date_add(element_at(acc, -1).LB, 20),
                 array((y.Updated_date as Updated_date, y.date as date, element_at(acc, -1).LB as LB)),
                 array(y)
               )
             )
         )
       )
    """).selectExpr('*', 'date_add(LB, 20) as UB')
    
    df_new.show(truncate=False)
    +---+-------------------+-------------------+----------+----------+
    |id |Updated_date       |date               |LB        |UB        |
    +---+-------------------+-------------------+----------+----------+
    |b  |2018-11-15 10:36:59|2019-01-25 00:00:00|2019-01-15|2019-02-04|
    |b  |2018-11-16 10:58:01|2019-02-10 00:00:00|2019-01-31|2019-02-20|
    |b  |2018-11-17 10:42:12|2019-02-04 00:00:00|2019-01-31|2019-02-20|
    |b  |2018-11-24 10:24:56|2019-02-10 00:00:00|2019-01-31|2019-02-20|
    |b  |2018-12-01 10:28:46|2019-02-02 00:00:00|2019-01-31|2019-02-20|
    |a  |2018-10-30 10:25:45|2019-02-14 00:00:00|2019-02-04|2019-02-24|
    |a  |2018-11-28 10:51:34|2019-02-14 00:00:00|2019-02-04|2019-02-24|
    |a  |2018-11-29 10:46:07|2019-01-11 00:00:00|2019-01-01|2019-01-21|
    |a  |2018-11-30 10:42:56|2019-01-14 00:00:00|2019-01-01|2019-01-21|
    |a  |2018-12-01 10:28:46|2019-01-16 00:00:00|2019-01-01|2019-01-21|
    |a  |2018-12-02 10:22:06|2019-01-22 00:00:00|2019-01-12|2019-02-01|
    +---+-------------------+-------------------+----------+----------+



Example-11-3: set a sub-group label based on the interval of N=6, there is no overlap between label window
    but could have gaps between adjacent windows.

  REF: https://stackoverflow.com/questions/57102219/finding-non-overlapping-windows-in-a-pyspark-dataframe

    from pyspark.sql.functions import sort_array, collect_list, col

    d = [ (1,0), (1,1), (1,4), (1,7), (1,14), (1,18)
        , (2,5), (2,20), (2,21)
        , (3,0), (3,1), (3,1.5), (3,2), (3,3.5), (3,4), (3,6)
        , (3,6.5), (3,7), (3,11), (3,14), (3,18), (3,20), (3,24)
        , (4,0), (4,1), (4,2), (4,6), (4,7)
    ]

    mydf = spark.createDataFrame([(int(x[0]), float(x[1])) for x in d], ['id', 't'])

    df1 = mydf.groupby('id').agg(sort_array(collect_list(col('t').astype('int'))).alias('data'))

    df1.selectExpr("id", """

        inline_outer(
          aggregate(
            /* expr: slice of array `data` from the 2nd element to the last */
            slice(data,2,size(data)-1),
            /* start: use data[0] to initialize, cast is import to resolve 
             * datatype mismatch issue from containsNull=False 
             */
            cast(array((data[0] as t, 0 as t0, 0 as g)) as array<struct<t:int,t0:int,g:int>>),
            /* merge: the main logic to setup g by comaring y to the latest saved t0 */
            (acc, y) -> concat(acc,
                IF(y - element_at(acc, -1).t0 < 6
                , array(named_struct('t', y, 't0', element_at(acc, -1).t0, 'g', element_at(acc, -1).g))
                , array(named_struct('t', y, 't0', y, 'g', element_at(acc, -1).g+1))
                )
              )
          )
        )

    """).show(50)
    +---+---+---+---+
    | id|  t| t0|  g|
    +---+---+---+---+
    |  1|  0|  0|  0|
    |  1|  1|  0|  0|
    |  1|  4|  0|  0|
    |  1|  7|  7|  1|
    |  1| 14| 14|  2|
    |  1| 18| 14|  2|
    |  3|  0|  0|  0|
    |  3|  1|  0|  0|
    |  3|  1|  0|  0|
    |  3|  2|  0|  0|
    |  3|  3|  0|  0|
    |  3|  4|  0|  0|
    |  3|  6|  6|  1|
    |  3|  6|  6|  1|
    |  3|  7|  6|  1|
    |  3| 11|  6|  1|
    |  3| 14| 14|  2|
    |  3| 18| 14|  2|
    |  3| 20| 20|  3|
    |  3| 24| 20|  3|
    |  2|  5|  5|  0|
    |  2| 20| 20|  1|
    |  2| 21| 20|  1|
    |  4|  0|  0|  0|
    |  4|  1|  0|  0|
    |  4|  2|  0|  0|
    |  4|  6|  6|  1|
    |  4|  7|  6|  1|
    +---+---+---+---+

  Notes:
   (1) values of t should not have duplicates, or otherwise it might have some issues when they are shown on the boundary
   (2) Troubleshooting: type-casting to the zero_value solved this error!
       
       ERROR: cannot resolve 'aggregate(`data`, ...)' due to data type mismatch: argument 3 requires array<struct<t:int,g:int>> type, however, 'lambdafunction(CASE WHEN (.....)'  is of array<struct<t:int,g:int>> type.
      ---
      the issue was from: -> ignoreNullability is not propogated recursively. thus the zero values
      should be set to NULL so left.valueContainsNull is true 
      ---
      Source code: sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala
    


Example-11-4: A typical session-gap issues:
  REF: https://stackoverflow.com/questions/64374547
  Target: set up session_timestamp:
   (1) A Session only starts with an event == `E` 
   (2) If a new event happens in one hour after the session starts, it belongs to that session.
   (3) If an event happened more than one hour of the event who started the session, it belongs to another session

    from pyspark.sql import functions as F

    #df = spark.read.csv('/home/xicheng/test/window-40.txt', header=True, inferSchema=True)
    df = spark.createDataFrame([
           ('E', '1', '2020-09-09 13:15:00'), ('E', '1', '2020-09-09 13:30:00'), ('E', '1', '2020-09-09 14:20:00'), 
           ('T', '1', '2020-09-09 14:35:00'), ('T', '2', '2020-09-09 13:20:00'), ('E', '2', '2020-09-09 13:25:00'), 
           ('E', '2', '2020-09-09 14:45:00'), ('T', '2', '2020-09-09 14:50:00'), ('T', '2', '2020-09-09 16:10:00'), 
           ('E', '2', '2020-09-09 16:30:00') 
       ], ['event', 'consumer', 'timestamp']) 

    df1 = df.groupby("consumer").agg(F.sort_array(F.collect_list(F.struct("timestamp","event"))).alias("dta"))

    df_new = df1.selectExpr("consumer", """ 
       inline( 
         aggregate( 
           slice(dta,2,size(dta)-1), 
           /* zero_value using dta[0] and setup session_timestamp = if(dta[0].event = 'E', dta[0].timestamp, NULL) */
           array((dta[0] as dta, if(dta[0].event = 'E', dta[0].timestamp, NULL) as session_timestamp)), 
           (acc, x) -> 
             concat(acc, array((x as dta, 
               IF(x.timestamp < element_at(acc,-1).session_timestamp + interval 1 hour 
                , element_at(acc,-1).session_timestamp 
                , if(x.event = 'E', x.timestamp, NULL) 
               ) as session_timestamp))
             ) 
          )  
      )  
    """).select("consumer", "dta.*", "session_timestamp")
    df_new.show()
    +--------+-------------------+-----+-------------------+                        
    |consumer|          timestamp|event|  session_timestamp|
    +--------+-------------------+-----+-------------------+
    |       1|2020-09-09 13:15:00|    E|2020-09-09 13:15:00|
    |       1|2020-09-09 13:30:00|    E|2020-09-09 13:15:00|
    |       1|2020-09-09 14:20:00|    E|2020-09-09 14:20:00|
    |       1|2020-09-09 14:35:00|    T|2020-09-09 14:20:00|
    |       2|2020-09-09 13:20:00|    T|               null|
    |       2|2020-09-09 13:25:00|    E|2020-09-09 13:25:00|
    |       2|2020-09-09 14:45:00|    E|2020-09-09 14:45:00|
    |       2|2020-09-09 14:50:00|    T|2020-09-09 14:45:00|
    |       2|2020-09-09 16:10:00|    T|               null|
    |       2|2020-09-09 16:30:00|    E|2020-09-09 16:30:00|
    +--------+-------------------+-----+-------------------+



example-11-5: calculate EMA from the previously updated EMA using formula: 
  REF: https://stackoverflow.com/questions/64248259
  Logic: For every customer if row == 1 then SMA as EMA else ( C * LAG(EMA) + A * B ) as EMA

    ab = spark.createDataFrame(
    [(1,"1/1/2020", 41.0,0.5,   0.5 ,1,     '10.22'),
     (1,"10/3/2020",24.0,0.3,   0.7 ,2,     ''     ),
     (1,"21/5/2020",32.0,0.4,   0.6 ,3,     ''     ),
     (2,"3/1/2020", 51.0,0.22,  0.78,1,     '34.78'),
     (2,"10/5/2020",14.56,0.333,0.66,2,     ''     ),
     (2,"30/9/2020",17.0,0.66,  0.34,3,     ''     )],["CID","date","A","B","C","Row","SMA"] )
    
    
    df1 = ab.groupby('CID').agg(F.sort_array(F.collect_list(F.struct('Row','date','A','B','C','SMA'))).alias('dta'))
    
    df1.selectExpr(
      "CID",
      """
        inline(
          aggregate(
            slice(dta,2,size(dta)-1),
            array((dta[0] as data, dta[0].SMA as EMA)),
            (acc, x) -> concat(acc, array((x as data, x.C*element_at(acc,-1).EMA + x.A*X.B as EMA)))
          )
        )
      """
    ).select("CID", "data.*", "EMA").show(truncate=False)
    +---+---+---------+-----+-----+----+-----+------------------+                   
    |CID|Row|date     |A    |B    |C   |SMA  |EMA               |
    +---+---+---------+-----+-----+----+-----+------------------+
    |1  |1  |1/1/2020 |41.0 |0.5  |0.5 |10.22|10.22             |
    |1  |2  |10/3/2020|24.0 |0.3  |0.7 |     |14.354            |
    |1  |3  |21/5/2020|32.0 |0.4  |0.6 |     |21.412399999999998|
    |2  |1  |3/1/2020 |51.0 |0.22 |0.78|34.78|34.78             |
    |2  |2  |10/5/2020|14.56|0.333|0.66|     |27.80328          |
    |2  |3  |30/9/2020|17.0 |0.66 |0.34|     |20.6731152        |
    +---+---+---------+-----+-----+----+-----+------------------+



Example-11-6: aggregate to calculate based on a fixed range and check the existance using array_contains
  REF: https://stackoverflow.com/questions/64462226
  Requirements for each line X:
    (1) find all rows where row.user == X.user and row.ts < X.ts
    (2) from these rows extract most recent metric_value for each protocol 
        (if corresponding record is older than X.ts - 4, than throw it out)
    (3) calculate avg of these metric_values, and add the value to a new column
  Method: 
    * use WindowSpec to define (1) and if-part of (2)
    * use F.reserve so that the latest processed protocols to the front of the array
    * create an array of structs with two fields: protocols (save processed protocols) 
      and total (sum of metric_value for all processed protocols)
    * use array_contains() to check if protocol processed
  Code:

    from pyspark.sql import Window, functions as F

    df = spark.createDataFrame([
        (0, 'user1', 'tcp', 197), (1, 'user1', 'udp', 155), (2, 'user1', 'tcp', 347), 
        (3, 'user1', 'tcp', 117), (4, 'user1', 'tcp', 230), (5, 'user2', 'udp', 225), 
        (6, 'user1', 'udp', 297), (7, 'user1', 'tcp', 790), (8, 'user1', 'udp', 216), 
        (9, 'user1', 'udp', 200) 
    ], ['ts', 'user', 'protocol', 'metric_value'])

    cols = ['ts', 'protocol', 'metric_value']

    w1 = Window.partitionBy('user').orderBy('ts').rangeBetween(-4,-1)

    df.withColumn("dta", F.reverse(F.collect_list(F.struct(*cols)).over(w1))) \
      .selectExpr("*", """
         aggregate(
           dta,
           cast((array() as protocols, 0 as total) as struct<protocols:array<string>,total:bigint>),
           (acc, x) -> 
             IF(array_contains(acc.protocols, x.protocol),
                acc,
                (concat(acc.protocols, array(x.protocol)) as protocols, acc.total+x.metric_value as total)
             ),
           acc -> round(acc.total/size(acc.protocols),2)
         ) as avg
    """).show()
    +---+-----+--------+------------+--------------------+-----+                    
    | ts| user|protocol|metric_value|                 dta|  avg|
    +---+-----+--------+------------+--------------------+-----+
    |  0|user1|     tcp|         197|                  []| null|
    |  1|user1|     udp|         155|     [[0, tcp, 197]]|197.0|
    |  2|user1|     tcp|         347|[[1, udp, 155], [...|176.0|
    |  3|user1|     tcp|         117|[[2, tcp, 347], [...|251.0|
    |  4|user1|     tcp|         230|[[3, tcp, 117], [...|136.0|
    |  6|user1|     udp|         297|[[4, tcp, 230], [...|230.0|
    |  7|user1|     tcp|         790|[[6, udp, 297], [...|263.5|
    |  8|user1|     udp|         216|[[7, tcp, 790], [...|543.5|
    |  9|user1|     udp|         200|[[8, udp, 216], [...|503.0|
    |  5|user2|     udp|         225|                  []| null|
    +---+-----+--------+------------+--------------------+-----+



More similar Examples (island&Gap):
(1) example-16(id=5): https://stackoverflow.com/questions/59548034
(2) example-19(id=6): https://stackoverflow.com/questions/60109004
(3) example-25(id=8): https://stackoverflow.com/questions/60959421
(4) example-35(id=9): https://stackoverflow.com/questions/63739214
(5) example-38(id=10): https://stackoverflow.com/questions/64144891
(6) example-40(id=10): https://stackoverflow.com/a/64270968/9510729

