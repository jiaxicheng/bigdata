Continue..

Spark SQL higher-order functions:

---
(43) aggregate function set calculated value in a struct field of accumulator
  REF: https://stackoverflow.com/q/65515349/9510729
  Issue: any null elements in the `col` array will result in the null value for the whole calculation
    using the below aggregate function, to overcome this we need to use coalesce while reset size of the
    array elements to exclude null:

  Not working:

    power_mean = lambda col: func.expr(f"""
        aggregate(`{col}`, 0D, (acc,x) -> acc+power(x, 2*size(`{col}`)), acc -> power(acc/size(`{col}`),0.5/size(`{col}`)))
    """)

  Working solution:
    
    power_mean = lambda col: func.expr(f"""
        aggregate(
          /* expr: array column to iterate through */
          `{col}`,
          /* start: set zero value and accumulator as an struct<psum:double,n:int> */
          (0D as psum, size(filter(`{col}`, x -> x is not null)) as n),
          /* merge: iterate through expr and calculate `sum([(x)**p for x in values])` */
          (acc,x) -> (acc.psum+power(coalesce(x,0),2*acc.n) as psum, acc.n as n),
          /* finish: post processing */
          acc -> power(acc.psum/acc.n, 0.5/acc.n)
        )
    """)
    
    df = spark.createDataFrame([([20,5,None,10],)],['value'])
    df.select("value", power_mean("value").alias('totalScore')).show(truncate=False)
    +------------+------------------+
    |value       |totalScore        |
    +------------+------------------+
    |[20, 5,, 10]|16.697421658984894|
    +------------+------------------+
    


(44) discard all elements after one matches from the stop-words
  Method: using aggregate function, set a flag and skip the elements when it becomes true

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
      ("Variable speeds allow you to refine every texture with culinary precision",),
      ("Perfect for family meals and entertaining and kitchen cabinets",),
      ("The more research you do, the more questions you may have",),
    ], ['value'])

    stop_words = ["and", "you"]

    df.selectExpr("flatten(sentences(lower(value))) as words") \
        .withColumn('stop_words', F.array(*map(F.lit, stop_words))) \
        .selectExpr("words", """
            aggregate(
              words, 
              cast((array(), false) as struct<words:array<string>,flag:boolean>),
              (acc,x) -> 
                if(acc.flag, 
                  acc, 
                  if(array_contains(stop_words, x),
                    (acc.words as words, true as flag),
                    (concat(acc.words, array(x)) as words, false as flag)
                  )
                )
            ).words as cleaned_words
         """).show(truncate=False)
    +-------------------------------------------------------------------------------------+-----------------------------+
    |words                                                                                |cleaned_words                |
    +-------------------------------------------------------------------------------------+-----------------------------+
    |[variable, speeds, allow, you, to, refine, every, texture, with, culinary, precision]|[variable, speeds, allow]    |
    |[perfect, for, family, meals, and, entertaining, and, kitchen, cabinets]             |[perfect, for, family, meals]|
    |[the, more, research, you, do, the, more, questions, you, may, have]                 |[the, more, research]        |
    +-------------------------------------------------------------------------------------+-----------------------------+

  Note:
   (1) array() is an EMPTY array, so there is no need to remove the first element on the finish argument (skipped it)
   (2) Another way using regexp_replace:

       # stop words by default split by white-spaces, escape metacharacters
       ptn = r'(^|\s+)(?:{})(\s+|$).*'.format('|'.join(stop_words))

       df.withColumn('value', F.regexp_replace(F.lower(F.col('value')), ptn, '')) \
           .selectExpr('flatten(sentences(value)) as words') \
           .show(truncate=False)

    or use a more reliable pattern which escape all metacharacters:

        meta_chars = re.compile(r'([]$^+.*\(\)/{}\|?=><[])')
        ptn = r'(^|\s+)(?:{})(\s+|$).*'.format('|'.join(meta_chars.sub(r'\\\1', x) for x in stop_words))



(45) use aggregate function to calculate running sum of an ArrayType column
  REF: https://stackoverflow.com/questions/58041497

    from pyspark.sql.functions import array_remove, split, regexp_replace, expr

    df = spark.createDataFrame([(e,) for e in ["aaabbbb0000ccaa", 'ttttesst']], ['x'])

    # iterate the full array, need typecast to zero value of value and also use ifnull() on calculation
    expe1 = "aggregate(l1, cast(array() as array<int>), (acc,x) -> concat(acc, array(x+ifnull(element_at(acc,-1),0))))"
    # iterate from the 2nd element of the Array, no need typecast and ifnull()
    expr2 = "aggregate(slice(l1,2,size(l1)), array(l1[0]), (acc,x) -> concat(acc, array(x+element_at(acc,-1))))"

    df1 = df.withColumn('x1', expr(r"""split(rtrim('\0',regexp_replace(x, '((.)\\2*)', '$1\0')),'\0')""")) \
        .withColumn('l1', expr("""transform(x1, e -> length(e))""")) \
        .withColumn('l2', expr(expr1))
    df1.show(3,0)
    +---------------+-------------------------+---------------+------------------+
    |x              |x1                       |l1             |l2                |
    +---------------+-------------------------+---------------+------------------+
    |aaabbbb0000ccaa|[aaa, bbbb, 0000, cc, aa]|[3, 4, 4, 2, 2]|[3, 7, 11, 13, 15]|
    |ttttesst       |[tttt, e, ss, t]         |[4, 1, 2, 1]   |[4, 5, 7, 8]      |
    +---------------+-------------------------+---------------+------------------+

