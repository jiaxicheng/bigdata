Continue..

Spark SQL higher-order functions:

---
(43) aggregate function set calculated value in a struct field of accumulator
  REF: https://stackoverflow.com/q/65515349/9510729
  Issue: any null elements in the `col` array will result in the null value for the whole calculation
    using the below aggregate function, to overcome this we need to use coalesce while reset size of the
    array elements to exclude null:

  Not working:

    power_mean = lambda col: func.expr(f"""
        aggregate(`{col}`, 0D, (acc,x) -> acc+power(x, 2*size(`{col}`)), acc -> power(acc/size(`{col}`),0.5/size(`{col}`)))
    """)

  Working solution:
    
    power_mean = lambda col: func.expr(f"""
        aggregate(
          /* expr: array column to iterate through */
          `{col}`,
          /* start: set zero value and accumulator as an struct<psum:double,n:int> */
          (0D as psum, size(filter(`{col}`, x -> x is not null)) as n),
          /* merge: iterate through expr and calculate `sum([(x)**p for x in values])` */
          (acc,x) -> (acc.psum+power(coalesce(x,0),2*acc.n) as psum, acc.n as n),
          /* finish: post processing */
          acc -> power(acc.psum/acc.n, 0.5/acc.n)
        )
    """)
    
    df = spark.createDataFrame([([20,5,None,10],)],['value'])
    df.select("value", power_mean("value").alias('totalScore')).show(truncate=False)
    +------------+------------------+
    |value       |totalScore        |
    +------------+------------------+
    |[20, 5,, 10]|16.697421658984894|
    +------------+------------------+
    


(44) discard all elements after one matches from the stop-words
  Method: using aggregate function, set a flag and skip the elements when it becomes true

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
      ("Variable speeds allow you to refine every texture with culinary precision",),
      ("Perfect for family meals and entertaining and kitchen cabinets",),
      ("The more research you do, the more questions you may have",),
    ], ['value'])

    stop_words = ["and", "you"]

    df.selectExpr("flatten(sentences(lower(value))) as words") \
        .withColumn('stop_words', F.array(*map(F.lit, stop_words))) \
        .selectExpr("words", """
            aggregate(
              words, 
              cast((array(), false) as struct<words:array<string>,flag:boolean>),
              (acc,x) -> 
                if(acc.flag, 
                  acc, 
                  if(array_contains(stop_words, x),
                    (acc.words as words, true as flag),
                    (concat(acc.words, array(x)) as words, false as flag)
                  )
                )
            ).words as cleaned_words
         """).show(truncate=False)
    +-------------------------------------------------------------------------------------+-----------------------------+
    |words                                                                                |cleaned_words                |
    +-------------------------------------------------------------------------------------+-----------------------------+
    |[variable, speeds, allow, you, to, refine, every, texture, with, culinary, precision]|[variable, speeds, allow]    |
    |[perfect, for, family, meals, and, entertaining, and, kitchen, cabinets]             |[perfect, for, family, meals]|
    |[the, more, research, you, do, the, more, questions, you, may, have]                 |[the, more, research]        |
    +-------------------------------------------------------------------------------------+-----------------------------+

  Note:
   (1) array() is an EMPTY array, so there is no need to remove the first element on the finish argument (skipped it)
   (2) Another way using regexp_replace:

       # stop words by default split by white-spaces, escape metacharacters
       ptn = '(^|\s+)(?:{})(\s+|$).*'.format('|'.join(stop_words))

       df.withColumn('value', F.regexp_replace(F.lower(F.col('value')), ptn, '')) \
           .selectExpr('flatten(sentences(value)) as words') \
           .show(truncate=False)

