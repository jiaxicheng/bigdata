Retrieving data type information of a column or the whole dataframe from df.schema:

to adjust, convert or save the data-type definitions of the fields or the whole data frame:
---
 + change datatype of a structfield or an arrayitem  inside a complex data type
 + discard some unwanted fields in a structtype
 + retrieve fieldname of any StructType field


df.schema:
---
  + prperties:
    + names: same as df.columns
    + fields: returns a list: [StructField(id,IntegerType,true), StructField(date_purchased,DateType,true)]
  + class_method:
    + fromJson(): 
      specify the DataType using a JSON value(must be a dict). for example:

          col_dtype = StructType.fromJson(json_value)

  + instance_method:
    + add(): Add a new StructField() to the exist StructType()
    + fieldNames():  same as df.columns
    + simpleString():  
      + return a string: 'struct<id:int,email:string,date_purchased:date>'
    + json():
      + put the DataType() into a JSON string
    + jsonValue():
      + put the DataType() into a JSON Value which is a Python object
    + fromInternal(), toInternal(), needConversion()
      + three are connected, use-case is limited, only DateType, TimeStampType might be useful
        most of simple DataType has needConversion() == False
    + typeName(): it almost always returns 'struct'

Notes:
 (1) the return of df.schema.jsonValue() is an Python object which can be very useful to 
     programmingly retrieve all column information in the dataframe
     including dtypes, field_name of StructType() columns
 (2) fromJson() class method can be used to reassign the dtype of a compound datatype:
     StructType, ArrayType, MapType, StructField (outermost DataType)
 (3) _parse_datetype_string() for class:`DataType.simpleString`
     + since spark 2.3, this also support a schema in a DDL-formatted string
       "a DOUBLE, b STRING", "a:array<short>", "map<string,string>"
     + case-insensitive
 (4) an example using fromInternal(), toInternal(), needConversion():
     REF: http://brianstempin.com/notebooks/Timestamp+demonstration.html

        from datetime import datetime

        DateType().toInternal(datetime(2019,9,28))   --> 18167
        DateType().fromInternal(18166)               --> datetime.date(2019, 9, 27)

        DoubleType().needConversion()     <-- False
        TimestampType().needConversion()  <-- True

 (5) Spark Complex Data Types used with schema, examples in DDL:
     
        ArrayType :    myarr    array<string>
        MapType   :    mymap    map<string,int>
        StructType:    mystruct struct<f1:string,f2:int>
   
        CREATE TABLE my_tbl (myarr array<string>, mymap map<string,int>, mystruct struct<f1:string,f2:int>);

 (6) For machine-learning projects, the name of features are saved in the training_df.schema as 'metadata'. you can
     find this info by:
        
        training_df.select('features').schema.jsonValue()['fields'][0]['metadata']['ml_attr']['attrs']['numeric']

 
#####
data structs used in df.schema.jsonValue():
---
&struct_field StructField:
  name: f_name
  type: DataType()
  nullable: True
  metadata: {}

StructType:
  name: s_name
  type: {
    type: 'struct',
    fields: [*struct_field]
  }
  nullable: True
  metadata: {}

ArrayType:
  name: a_name
  type: 
    type: 'array'
    elementType: 
      type: DataType()
      fields: [*struct_field]
    containsNull: True
  nullable: True
  metadata: {}

MapType:
  name: m_name
  type: 
    type: 'map'
    keyType: DataType()
    valueType: DataType()
    valueContainsNull: False
  nullable: False
  metadata: {}

******
Some more metadata information: using spark._jsparkSession

    tid = spark._jsparkSession.sessionState().sqlParser().parseTableIdentifier('my_tbl')
    metadata = spark._jsparkSession.sessionState().catalog.getTableMetadata(tid) 
    https://stackoverflow.com/questions/58537239/how-to-retrieve-metadata-of-the-table-in-pyspark


Example-1: using df.schema.jsonValue() to map column names with corresponding dataType which can be
           used in cast()/astype(). using fromJson() classmethod for StructType, ArrayType etc. compound data types

    field_dtypes = { f['name']:f['type'] for f in df.schema.jsonValue()['fields'] }       


Example-2: Using df.schema.jsonValue() to retrieve all fields in a StructType() and flatten the fields:
https://stackoverflow.com/questions/58219205/how-to-move-all-json-structs-up-one-level-convert-all-json-structs-to-strings

    struct_fields_mapping = { 
            f['name']:[i['name'] for i in f['type']['fields']] 
                for f in df.schema.jsonValue()['fields'] 
                if type(f['type']) is dict and f['type']['type'] == 'struct' 
    }

    >>> print(fields_mapping)
    {'key 1': ['s'],
     'key 2': ['n', 's'],
     'key 3': ['n'],
     'key 4': ['n', 's'],
     'key 5': ['s']}


Example-3: saving dataframe into CSV and keep the schema for data reloading

    from pyspark.sql.functions import to_json, from_json
    from pyspark.sql.types import ArrayType, StructType

    df.printSchema()
    root
     |-- array_b: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- a: string (nullable = true)
     |    |    |-- b: double (nullable = true)
     |-- struct_c: struct (nullable = true)
     |    |-- a: double (nullable = true)
     |    |-- b: double (nullable = true)

    # df.schema.jsonValue() saves the dataframe schema metadata in Python dict,
    # the following dict_field_dtype will save each column and its dtype meta data
    # in a dictionary
    field_dtypes = { f['name']:f['type'] for f in df.schema.jsonValue()['fields'] }

    # now convert array_b and struct_c into JSON and then save it into CSV file
    df.withColumn('array_b', to_json('array_b')) \
      .withColumn('struct_c', to_json('struct_c')) \
      .coalesce(1).write.mode('overwrite') \
      .csv('/path/to/csv', header=True)

    # read the data back and recover their DataType
    spark.read.csv('/path/to/csv', header=True) \
         .withColumn('array_b', from_json('array_b', ArrayType.fromJson(field_dtypes['array_b']))) \
         .withColumn('struct_c', from_json('struct_c', StructType.fromJson(field_dtypes['struct_c']))) \
         .printSchema()
    root
     |-- array_b: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- a: string (nullable = true)
     |    |    |-- b: double (nullable = true)
     |-- struct_c: struct (nullable = true)
     |    |-- a: double (nullable = true)
     |    |-- b: double (nullable = true)



Example-4: change DataType within a nested data column: below adjust struct_c.b from DoubleType() to StringType()

    field_dtypes['struct_c']['fields'][1]['type'] = 'string'

    df.withColumn('struct_c', from_json(to_json('struct_c'), StructType.fromJson(field_dtypes['struct_c']))) \
      .printSchema()
    root
     |-- array_b: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- a: string (nullable = true)
     |    |    |-- b: double (nullable = true)
     |-- struct_c: struct (nullable = true)
     |    |-- a: double (nullable = true)
     |    |-- b: string (nullable = true)


Example-5: delete some fields in a column of complex data types. 

    in the following example, delete array_b.a from the column array_b

    similar post in SO: https://stackoverflow.com/questions/58243292

    Method-1: using jsonValue()
    f_array_b = field_dtypes['array_b']
    f_array_b['elementType']['fields'] = [ f for f in f_array_b['elementType']['fields'] if f['name'] not in ('a') ]
    new_schema = ArrayType.fromJson(f_array_b)

    Method-2: A simpler way is just to modify df.schema.simpleString()
    >>> df.select('array_b').schema.simpleString()
    'struct<array_b:array<struct<a:string,b:double>>>'
    >>> new_schema = 'array<struct<b:double>>'
 
    After you have new_schema, adjust the data using to_json + from_json
    df.withColumn('array_b', from_json(to_json('array_b'), new_schema))


Example-6: use the existing df.schema to assign another df's schema with some extra new fields

    Note: the StructType used for df.schema is a mutable data type, any late change to an assigned value 
          from df.schema will change the df.schema itself. so be careful
    REF: https://stackoverflow.com/questions/58204068

    # the following will change the original df.schema, thus is incorrect
    schema = df.schema
    schema.add('cs', 'double')
         
    # correct method is as following: 
    # create a new schema from existing f.schema.jsonValue(), then add new fields.
    schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')


Example-7: use df.dtypes and DDL-formated schema

    # added new_cols
    new_cols = [ ("col1", "float"), ("col2", "int") ]
    
    # set up DDL-formated schema with 2 new columns to the existing df
    schema = ','.join(f'{c} {d}' for c,d in df.dtypes + new_cols)

    #create a empty dataframe: 
    df1 = spark.createDataFrame([],schema=schema)

    
Example-8: use type-casting to adjust field name and/or field datatype

  Note: change `int_value` field under event_params column from `bigint` to `string`. 
  the same method applied to: (1) change field_name, (2) change field data type, see example-4.
  Not applied to: remove/add a field

    df                                                                                                                 
    #DataFrame[col2: bigint, event_params: array<struct<key:string,value:struct<int_value:bigint,string_value:string>>>]

    # retrieve dtype of `event_params` and do string replacement
    dtype = dict(df.dtypes)["event_params"].replace('int_value:bigint','int_value:string')

    df.withColumn('event_params', df["event_params"].cast(dtype))


