
Pandas UDF is a PySpark DataFrame API function added since Spark 2.3. Unlike the original 
UDF function, the pandas_udf is a vectorized version of UDF which can use some of the Pandas functions.

The drawbacks of the original UDF:
(1) data serialization/deserialization between Python interpretor and JVM
(2) calculation are performed on row-basis which is low efficient

Note: Pandas UDF overcame the 2nd drawback but the data SerDe between Python and JVM
still holds. 

Select functions(if available) in the following order:
(1) Spark SQL builtin functions or Dataframe API functions
(2) Scala-based UDF functions
(3) pyspark.sql.functions.pandas_udf
(4) pyspark.sql.functions.udf

When using udf might be better than using Spark SQL or API functions:
+ if pyspark.sql.functions.explode() involves mass increment on data volume
+ if using udf can avoid shuffling of big volumn of data, i.e. df.join

When to use Pandas UDF:
(1) when the functionality is not available through Spark SQL or DataFrame API functions
(2) only when the function can be vectorized. In another word, if the operation can not be
    vectorized, for example, a stateful list which value relies on the updated values of
    its neighbouring rows. In such case, using the original udf, either backed by
    Scala or Python.
 
Some configuration directives:
(1) the default size of Arrow-record batch:
  spark.sql.execution.arrow.maxRecordsPerBatch=10000

* update PyArrow:

    pip3 install -U pyarrow


Pandas for Spark 3.0+: 
---
Note: 
(1) type hint for pandas_udf, type-hint is not required with applyInPandas and mapInPandas:
  + pd.Series to primary data types + ArrayType 
  + pd.DataFrame to StructType 
  + from typing import Iterator, Callable, Tuple
    + Tuple is used when Multiple Series as input arguments, see example-4,13
    + Callable is used with `lambda` functions: 

         myfunc: Callable[[pd.Series, pd.DataFrame], pd.Series] = lambda s,d: func(s,d)
  + valid types used for pandas_udf are, either pd.Series or pd.DataFrame or the Iterator
    or combination of them using Tuple etc. check below code
  
      python/pyspark/sql/pandas/typehints.py

(2) pandas_udf function:
  + Series to Series:
    + take one or more pd.Series and return a pd.Series 
    + with non-column argument, use lambda function, see example-8 function `get_iso2_1()`
    
        def __get_iso2_1(addr:Series, ptn:Broadcast) -> Series:    
          return Series([...])

        get_iso2_1 = pandas_udf(lambda x:__get_iso2_1(x, df_ptn), "array<array<string>>")

  + Iterator of Series to Iterator of Series:   
    + useful when UDF execution requirs some expensive initialization. see Example-11,14
    + when non-column argument, use following from example-8 function `get_iso2_4()`

        from typing import Iterator

        def __get_iso2_4(addr:Series, ptn:Broadcast) -> Series:
          return Series([ ... ])

        @pandas_udf("array<array<string>>")
        def get_iso2_4(iterator:Iterator[Series]) -> Iterator[Series]:
          state = very_expensive_init()
          # add initialization here if needed
          for addr in iterator:
            yield __get_iso2_4(addr, df_ptn, state)

  
  + Iterator of Multiple Series to Iterator of Series: see example-4,13
    + pd.DataFrame also applies

    from typing import Iterator, Tuple
    @pandas_udf("long")
    def multiply(it: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:
      for s,d in it:
        yield s*d.v

  + Series to Scalar: see example-3
    + WindowSpec now supports more than just unbounded Window frame.
    + Input argument can not be StructType(df.DataFrame)

(3) Pandas function APIs:
  + Grouped map: pyspark.sql.GroupedData
    + split-apply+combine
    + applyInPandas(func, schema)
      + func is a Python function to define the computations of each group
      + function can take 1 or 2 arguments: func(pdf) or func(key, pdf)
    + all data of a group must be loaded into memory before the function is applied
      could be an issue for skewed data
  + Map: pyspark.sql.DataFrame
    + df.mapInPandas(iterator, schema)
      can do something similar to applyInPandas, except without `groupby`
  + Cogrouped map: pyspark.sql.PandasCogroupedOps
    + Used when two dataframes are required in pandas functions, for example `merge_asof`
    + df1.groupby(*keys).cogroup(df2.groupby(*keys)).applyInPandas(func, schema)
      where keys are list of cols to groupby
    + func takes either two dataframes as argument or three argument with the first one tuple (grouping keys)
      return another pandas.DataFrame
(4) as of Spark 3.0.0, MapType, nested StructType and array of TimestampType are not supported


Three PandasUDFType (as of Spark 2.4.5): **deprecated**
+ SCALAR:
  + function argument(s): is a `pandas.core.series.Series` 
    + for ArrayType column, each item is a `numpy.ndarray`
  + function returning a also `pandas.core.series.Series`
  + where_to_use:
    + pyspark.sql.DataFrame.withColumn()
    + pyspark.sql.DataFrame.select()

+ GROUPED_MAP: 
  + function argument(key, pdf), 
    + key is optional, if exist contains a tuple of all columns in the groupby() function.
    + pdf is the Pandas dataframe containing rows in the current group
  + function returning a dataframe, returnType should be df.schema or any customized StructType()
    Note: the function can return all fields after groupby even though not in groupby and aggregation list
  + where_to_use: 
    + pyspark.sql.GroupedData.apply()
        df.groupby('c1', 'c2').apply(my_pudf_func)
  + examples:
    + Series.interpolate()

+ GROUP_AGG
  + function argument(s1, s2): one or more Series
  + returning a Scalar
  + where to use: 
    + pyspark.sql.GroupedData.agg()
        df.groupby('c1', 'c2').agg(my_pudf_func)
    + pyspark.sql.Window
        df.withColumn('c2', my_udf_func('c1').over(w1))
      Note: for Window function, only support unbounded WindowSpec
  + examples:
    + pyspark.sql.Window:
      Series.is_unique:  is_monotonic_increasing, is_monotonic_decreasing
          udf_p1 = F.pandas_udf(lambda x: x.is_unique, 'boolean', F.PandasUDFType.GROUPED_AGG)
  Notes:
   (1) as of spark 2.4.4, MapType, StructType are not supported as output types.
      
Limitations:
 (1) as of spark 2.4.5, all Spark SQL data types are supported by Arrow-based conversion 
     except MapType, Array of Timestamp (i.e., slide window created by F.window function will not work
     with pandas_udf), Array of Date and nested Struct. This applies to both input argument type and return type.
     


Some Examples:

Example-1: pandas.Series.interpolate() + GROUPED_MAP

This Pandas function returns Series or DataFrame, so we can only use GROUPED_MAP:

    import pandas as pd

    df = spark.createDataFrame([
          ('A', 't0', None) 
        , ('A', 't1', 1.5) 
        , ('A', 't2', 1.7) 
        , ('B', 't3', 0.5) 
        , ('B', 't4', None) 
        , ('B', 't5', 1.1) 
        , ('C', 't6', 4.3) 
        , ('C', 't7', 3.4) 
        , ('C', 't8', None) 
        , ('C', 't9', 2.7) 
      ], schema='Key:string,time:string,value:double')

    df.show()                                                                                                           
    +---+----+-----+
    |Key|time|value|
    +---+----+-----+
    |  A|  t0| null|
    |  A|  t1|  1.5|
    |  A|  t2|  1.7|
    |  B|  t3|  0.5|
    |  B|  t4| null|
    |  B|  t5|  1.1|
    |  C|  t6|  4.3|
    |  C|  t7|  3.4|
    |  C|  t8| null|
    |  C|  t9|  2.7|
    +---+----+-----+

    # in this function, you can handle any Pandas operations
    def f_interp(pdf:pd.DataFrame) -> pd.DataFrame: 
        return pdf.assign(value=lambda x: x.sort_values('time').value.interpolate(method='nearest'))

    df.groupby('Key').applyInPandas(f_interp, df.schema).show()
    +---+----+-----+                                                                
    |Key|time|value|
    +---+----+-----+
    |  B|  t3|  0.5|
    |  B|  t4|  0.5|
    |  B|  t5|  1.1|
    |  C|  t6|  4.3|
    |  C|  t7|  3.4|
    |  C|  t8|  3.4|
    |  C|  t9|  2.7|
    |  A|  t0| null|
    |  A|  t1|  1.5|
    |  A|  t2|  1.7|
    +---+----+-----+


Example-2: calculate the cosine similarity + GROUPED_MAP
REF: https://stackoverflow.com/questions/58204068/pyspark-cosinesimilarity-over-sataframe

    import numpy as np
    import pandas as pd
    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType

    df = spark.createDataFrame([
          (1, 2, 0.9, 0.1, 0.1)
        , (1, 3, 0.3, 0.4, 0.9)
        , (1, 4, 0.2, 0.9, 0.15)
        , (2, 1, 0.8, 0.8, 1.0)
      ]  , ['Customer1', 'Customer2', 'v_cust1', 'v_cust2', 'cosine_sim'])

    #Target-1: do the groupby aggregate
    def cosine_similarity_groupby(key:tuple, pdf:pd.DataFrame) -> pd.DataFrame: 
        cs = np.dot(pdf.v_cust1, pdf.v_cust2) / (np.linalg.norm(pdf.v_cust1) * np.linalg.norm(pdf.v_cust2)) 
        return pd.DataFrame([key + (cs,)]) 

    df.groupby('Customer1').applyInPandas(cosine_similarity_groupby,'Customer1:long,cs:double').show()
    +---------+------------------+                                                  
    |Customer1|                cs|
    +---------+------------------+
    |        1|0.4063381906012777|
    |        2|               1.0|
    +---------+------------------+

    #Target-2: do the transform()

    schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')

    def cosine_similarity_transform(pdf:pd.DataFrame) -> pd.DataFrame:
      return pdf.assign(cs=lambda x: np.dot(x.v_cust1, x.v_cust2) / (np.linalg.norm(x.v_cust1) * np.linalg.norm(x.v_cust2)))

    df.groupby('Customer1').applyInPandas(cosine_similarity_transform, schema).show()
    +---------+---------+-------+-------+----------+------------------+             
    |Customer1|Customer2|v_cust1|v_cust2|cosine_sim|                cs|
    +---------+---------+-------+-------+----------+------------------+
    |        1|        2|    0.9|    0.1|       0.1|0.4063381906012777|
    |        1|        3|    0.3|    0.4|       0.9|0.4063381906012777|
    |        1|        4|    0.2|    0.9|      0.15|0.4063381906012777|
    |        2|        1|    0.8|    0.8|       1.0|               1.0|
    +---------+---------+-------+-------+----------+------------------+


Notes: 
 (*) after spark 3.0+, PandasUDFType is deprecated.
 (1) When return type is `PandasUDFType.GROUPED_MAP`
     + the function can take two arguments
       + key: which is a tuple save all fields in groupby() function
       + pdf: which is the subset of the dataframe containing only grouped Rows
     + return value can be a DataFrame with one row(aggregate) or all rows in the current grouped(transform)
       when return one Row dataframe: use the `+` to merge two tuples.

         pd.DataFrame([key + (pdf.v.mean(),)])

 (2) When doing transform with returntype='PandasUDFType.GROUPED_MAP', we can use the existing df.schema,
     but must initialize a new instance instead so that the existing df.schema won't be changed (or use deep-copy)

          # the following two lines is not OK, the existing df.schema will be changed
          schema = df.schema
          schema.add('cs', 'double')

          # the following line is fine, old schema is kept as-is
          schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')

Troubleshooting:
  (1) Error: Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
      Ans: the return value of udf function, if it's a np.dtype, it should be forced into Python types, for example:
           return float(np.log(x))
           return list(np.array(...))

  (2) Error: PySpark 2.4.5: IllegalArgumentException when using PandasUDF
    known issue: pandas_udf()/toPandas() does not work with pyarrow 0.15.0+ (see https://issues.apache.org/jira/browse/SPARK-29367), this is fixed as of version 2.4.5 and 3.0+. also refer to the following link:
    REF: https://stackoverflow.com/questions/61202005/pyspark-2-4-5-illegalargumentexception-when-using-pandasudf

  (3) Error: pyarrow.lib.ArrowInvalid: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
    Ans: due to the actual return type does not match the one specified in schema,
    REF: https://www.mail-archive.com/issues@arrow.apache.org/msg30138.html

  (4) ERROR: pandas_udf java.lang.IllegalArgumentException
        py4j.protocol.Py4JJavaError: An error occurred while calling o62.collectAsArrowToPython
  Reason: pandas_udf with older version Spark (2.3.0) not working with latest pyarrow release(0.15.0)
  Resolve: vim $SPARK_HOME/conf/spark-env.sh and then append the following line:

        ARROW_PRE_0_15_IPC_FORMAT=1

  REF: https://issues.apache.org/jira/browse/SPARK-29367


