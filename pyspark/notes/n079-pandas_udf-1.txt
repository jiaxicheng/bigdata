
Pandas UDF is a PySpark DataFrame API function added since Spark 2.3. Unlike the original 
UDF function, the pandas_udf is a vectorized version of UDF which can use some of the Pandas functions.

The drawbacks of the original UDF:
(1) data serialization/deserialization between Python interpretor and JVM
(2) calculation are performed on row-basis which is low efficient

Note: Pandas UDF overcame the 2nd drawback but the data SerDe between Python and JVM
still holds. 

Select functions(if available) in the following order:
(1) Spark SQL builtin functions or Dataframe API functions
(2) Scala-based UDF functions
(3) pyspark.sql.functions.pandas_udf
(4) pyspark.sql.functions.udf

When using udf might be better than using Spark SQL or API functions:
+ if pyspark.sql.functions.explode() involves mass increment on data volume
+ if using udf can avoid shuffling of big volumn of data, i.e. df.join

When to use Pandas UDF:
(1) when the functionality is not available through Spark SQL or DataFrame API functions
(2) only when the function can be vectorized. In another word, if the operation can not be
    vectorized, for example, a stateful list which value relies on the updated values of
    its neighbouring rows. In such case, using the original udf, either backed by
    Scala or Python.
 
Some configuration directives:
(1) the default size of Arrow-record batch:
  spark.sql.execution.arrow.maxRecordsPerBatch=10000


Pandas for Spark 3.0+: 
---
Note: 
(1) type hint for pandas_udf, type-hint is not required with applyInPandas and mapInPandas:
  + pd.Series to primary data types + ArrayType 
  + pd.DataFrame to StructType 
  + from typing import Iterator, Callable, Tuple
    + Tuple is used when Multiple Series as input arguments, see example-4,13
    + Callable is used with `lambda` functions: 

         myfunc: Callable[[pd.Series, pd.DataFrame], pd.Series] = lambda s,d: func(s,d)

(2) pandas_udf function:
  + Series to Series:
    + take one or more pd.Series and return a pd.Series 
    + when non-Column arguments are required for the function, using Iterator will have problem

  + Iterator of Series to Iterator of Series:   
    + useful when UDF execution requirs some expensive initialization.
  
    from typing import Iterator
    @pandas_udf("long")
    def test(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:
      state = very_expensive_init()
      for x in iterator: 
        yield calculate_with_state(x, state)

  + Iterator of Multiple Series to Iterator of Series: see example-4,13

    from typing import Iterator, Tuple
    @pandas_udf("long")
    def multiply(it: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:
      for s,d in it:
        yield s*d.v

  + Series to Scalar: see example-3
    + WindowSpec now supports more than just unbounded Window frame.
(3) Pandas function APIs:
  + Grouped map: pyspark.sql.GroupedData
    + split-apply+combine
    + applyInPandas(func, schema)
      + func is a Python function to define the computations of each group
      + function can take 1 or 2 arguments: func(pdf) or func(key, pdf)
    + all data of a group must be loaded into memory before the function is applied
      could be an issue for skewed data
  + Map: pyspark.sql.DataFrame
    + df.mapInPandas(iterator, schema)
      can do something similar to applyInPandas, except without `groupby`
  + Cogrouped map: pyspark.sql.PandasCogroupedOps
    + Used when two dataframes are required in pandas functions, for example `merge_asof`
    + df1.groupby(*keys).cogroup(df2.groupby(*keys)).applyInPandas(func, schema)
      where keys are list of cols to groupby
    + func takes either two dataframes as argument or three argument with the first one tuple (grouping keys)
      return another pandas.DataFrame
(4) as of Spark 3.0.0, MapType, nested StructType and array of TimestampType are not supported


Three PandasUDFType (as of Spark 2.4.5): **deprecated**
+ SCALAR:
  + function argument(s): is a `pandas.core.series.Series` 
    + for ArrayType column, each item is a `numpy.ndarray`
  + function returning a also `pandas.core.series.Series`
  + where_to_use:
    + pyspark.sql.DataFrame.withColumn()
    + pyspark.sql.DataFrame.select()

+ GROUPED_MAP: 
  + function argument(key, pdf), 
    + key is optional, if exist contains a tuple of all columns in the groupby() function.
    + pdf is the Pandas dataframe containing rows in the current group
  + function returning a dataframe, returnType should be df.schema or any customized StructType()
    Note: the function can return all fields after groupby even though not in groupby and aggregation list
  + where_to_use: 
    + pyspark.sql.GroupedData.apply()
        df.groupby('c1', 'c2').apply(my_pudf_func)
  + examples:
    + Series.interpolate()

+ GROUP_AGG
  + function argument(s1, s2): one or more Series
  + returning a Scalar
  + where to use: 
    + pyspark.sql.GroupedData.agg()
        df.groupby('c1', 'c2').agg(my_pudf_func)
    + pyspark.sql.Window
        df.withColumn('c2', my_udf_func('c1').over(w1))
      Note: for Window function, only support unbounded WindowSpec
  + examples:
    + pyspark.sql.Window:
      Series.is_unique:  is_monotonic_increasing, is_monotonic_decreasing
          udf_p1 = F.pandas_udf(lambda x: x.is_unique, 'boolean', F.PandasUDFType.GROUPED_AGG)
  Notes:
   (1) as of spark 2.4.4, MapType, StructType are not supported as output types.
      
Limitations:
 (1) as of spark 2.4.5, all Spark SQL data types are supported by Arrow-based conversion 
     except MapType, Array of Timestamp (i.e., slide window created by F.window function will not work
     with pandas_udf), Array of Date and nested Struct. This applies to both input argument type and return type.
     


Some Examples:

Example-1: pandas.Series.interpolate() + GROUPED_MAP

This Pandas function returns Series or DataFrame, so we can only use GROUPED_MAP:

    import pandas as pd

    df = spark.createDataFrame([
          ('A', 't0', None) 
        , ('A', 't1', 1.5) 
        , ('A', 't2', 1.7) 
        , ('B', 't3', 0.5) 
        , ('B', 't4', None) 
        , ('B', 't5', 1.1) 
        , ('C', 't6', 4.3) 
        , ('C', 't7', 3.4) 
        , ('C', 't8', None) 
        , ('C', 't9', 2.7) 
      ], schema='Key:string,time:string,value:double')

    df.show()                                                                                                           
    +---+----+-----+
    |Key|time|value|
    +---+----+-----+
    |  A|  t0| null|
    |  A|  t1|  1.5|
    |  A|  t2|  1.7|
    |  B|  t3|  0.5|
    |  B|  t4| null|
    |  B|  t5|  1.1|
    |  C|  t6|  4.3|
    |  C|  t7|  3.4|
    |  C|  t8| null|
    |  C|  t9|  2.7|
    +---+----+-----+

    # in this function, you can handle any Pandas operations
    def f_interp(pdf:pd.DataFrame) -> pd.DataFrame: 
        return pdf.assign(value=lambda x: x.sort_values('time').value.interpolate(method='nearest'))

    df.groupby('Key').applyInPandas(f_interp, df.schema).show()
    +---+----+-----+                                                                
    |Key|time|value|
    +---+----+-----+
    |  B|  t3|  0.5|
    |  B|  t4|  0.5|
    |  B|  t5|  1.1|
    |  C|  t6|  4.3|
    |  C|  t7|  3.4|
    |  C|  t8|  3.4|
    |  C|  t9|  2.7|
    |  A|  t0| null|
    |  A|  t1|  1.5|
    |  A|  t2|  1.7|
    +---+----+-----+


Example-2: calculate the cosine similarity + GROUPED_MAP
REF: https://stackoverflow.com/questions/58204068/pyspark-cosinesimilarity-over-sataframe

    import numpy as np
    import pandas as pd
    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType

    df = spark.createDataFrame([
          (1, 2, 0.9, 0.1, 0.1)
        , (1, 3, 0.3, 0.4, 0.9)
        , (1, 4, 0.2, 0.9, 0.15)
        , (2, 1, 0.8, 0.8, 1.0)
      ]  , ['Customer1', 'Customer2', 'v_cust1', 'v_cust2', 'cosine_sim'])

    #Target-1: do the groupby aggregate
    def cosine_similarity_groupby(key:tuple, pdf:pd.DataFrame) -> pd.DataFrame: 
        cs = np.dot(pdf.v_cust1, pdf.v_cust2) / (np.linalg.norm(pdf.v_cust1) * np.linalg.norm(pdf.v_cust2)) 
        return pd.DataFrame([key + (cs,)]) 

    df.groupby('Customer1').applyInPandas(cosine_similarity_groupby,'Customer1:long,cs:double').show()
    +---------+------------------+                                                  
    |Customer1|                cs|
    +---------+------------------+
    |        1|0.4063381906012777|
    |        2|               1.0|
    +---------+------------------+

    #Target-2: do the transform()

    schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')

    def cosine_similarity_transform(pdf:pd.DataFrame) -> pd.DataFrame:
      return pdf.assign(cs=lambda x: np.dot(x.v_cust1, x.v_cust2) / (np.linalg.norm(x.v_cust1) * np.linalg.norm(x.v_cust2)))

    df.groupby('Customer1').applyInPandas(cosine_similarity_transform, schema).show()
    +---------+---------+-------+-------+----------+------------------+             
    |Customer1|Customer2|v_cust1|v_cust2|cosine_sim|                cs|
    +---------+---------+-------+-------+----------+------------------+
    |        1|        2|    0.9|    0.1|       0.1|0.4063381906012777|
    |        1|        3|    0.3|    0.4|       0.9|0.4063381906012777|
    |        1|        4|    0.2|    0.9|      0.15|0.4063381906012777|
    |        2|        1|    0.8|    0.8|       1.0|               1.0|
    +---------+---------+-------+-------+----------+------------------+


Notes: 
 (*) after spark 3.0+, PandasUDFType is deprecated.
 (1) When return type is `PandasUDFType.GROUPED_MAP`
     + the function can take two arguments
       + key: which is a tuple save all fields in groupby() function
       + pdf: which is the subset of the dataframe containing only grouped Rows
     + return value can be a DataFrame with one row(aggregate) or all rows in the current grouped(transform)
       when return one Row dataframe: use the `+` to merge two tuples.

         pd.DataFrame([key + (pdf.v.mean(),)])

 (2) When doing transform with returntype='PandasUDFType.GROUPED_MAP', we can use the existing df.schema,
     but must initialize a new instance instead so that the existing df.schema won't be changed (or use deep-copy)

          # the following two lines is not OK, the existing df.schema will be changed
          schema = df.schema
          schema.add('cs', 'double')

          # the following line is fine, old schema is kept as-is
          schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')

Troubleshooting:
  (1) Error: Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
      Ans: the return value of udf function, if it's a np.dtype, it should be forced into Python types, for example:
           return float(np.log(x))
           return list(np.array(...))

  (2) Error: PySpark 2.4.5: IllegalArgumentException when using PandasUDF
    known issue: pandas_udf()/toPandas() does not work with pyarrow 0.15.0+ (see https://issues.apache.org/jira/browse/SPARK-29367), this is fixed as of version 2.4.5 and 3.0+. also refer to the following link:
    REF: https://stackoverflow.com/questions/61202005/pyspark-2-4-5-illegalargumentexception-when-using-pandasudf

  (3) Error: pyarrow.lib.ArrowInvalid: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
    Ans: due to the actual return type does not match the one specified in schema,
    REF: https://www.mail-archive.com/issues@arrow.apache.org/msg30138.html
