About PySpark Partitions:


Types of partitions:
---
  + Hash partitioning (default)
    most of time, Hash partitioning is good enough. with numPartition only supplied, rows will be
    evenly distributed. However, using specific cols/keys as partitioner, there could be significant
    skewness in data distributions
  + Range partitioning
    Range partitioning need Ordering[K], and require additional data scan. This is available after Spark 2.4
  + Round-robin 
  Notes:
    if the key distribution is significantly skewed
    (1) the same key has large number or rows, no Partitioner can help
    (2) the same hash slot gets too many rows, then adjust numPartitions, or add a new parition column
        set the same value to the same key, so to keep them in the same bucket


Things to consideration when partitioning:
---
  + data skewness:
    know your data and how it's distributed. use an appropriate keys to repartition you
    data to evenly sprad the load. 
  + reading large unsplitable files, use repartition to post-split data
  + #partitions should be proportional to #executors (the size of partition is limited by 
    the available RAM of each executor)
  + repartitioning is expensive. in a typical scenario, most of the data has to be serialized,
    shuffled and deserialized and few operations can benefit from pre-partition data:
    + groupby, window functions do not gain anything from pre-partitions.
    REF: https://stackoverflow.com/questions/30995699/how-to-define-partitioning-of-dataframe
    **Note:** after groupby, the number of partitions will be set to the system default, not the 
      N from df.repartition(N).groupby()


pyspark.RDD:
---
  + Get partition infor:
    + getNumPartitions()
    + glom()
  + reset partitions:
    + partitionBy(numPartitions, partitionFunc=<function portable_hash>)
    + coalesce(numPartitions, shuffle=False)
    + repartition(numPartitions)
    + repartitionAndSortWithinPartitions(
          numPartitions=None
        , partitionFunc=<function portable_hash>
        , ascending=True
        , keyfunc=<function RDD.<lambda>>
      )
  + sorting
    + sortBy(keyfunc, ascending=True, numPartitions=None)
    + sortByKey(ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda>>)
  + map functions and preservesPartitioning:
    + map(f, preservesPartitioning=False)
    + flatMap(f, preservesPartitioning=False)
    + mapPartitions(f, preservesPartitioning=False)
    + mapPartitionsWithIndex(f, preservesPartitioning=False)
  + reduce functions and numPartitions, partitionFunc
    + aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)
    + combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash>)
    + foldByKey(zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash>)
    + groupByKey(numPartitions=None, partitionFunc=<function portable_hash>)
    + reduceByKey(func, numPartitions=None, partitionFunc=<function portable_hash>)
  + groupby + join:
    + distinct(numPartitions=None)
    + cogroup(other, numPartitions=None)
    + groupBy(f, numPartitions=None, partitionFunc=<function portable_hash>)
    + fullOuterJoin(other, numPartitions=None)
    + join(other, numPartitions=None)
    + leftOuterJoin(other, numPartitions=None)
    + rightOuterJoin(other, numPartitions=None)
    + subtract(other, numPartitions=None)
    + subtractByKey(other, numPartitions=None)

  Notes:
   (1) the function arguments: 
       + partitionFunc=<function portable_hash>: are designed for pair-RDD and function argument is 
            the key of the tuple (key, value). example:

                  rdd.keyBy(lambda x: x[1]).partitionBy(N, lambda x: hash(x))

       + keyfunc=<function RDD.<lambda>>: 
         + applied to repartitionAndSortWithinPartitions, sortBy, sortByKey
         + soryByKey() only applied to pair-RDD, sortBy() to tuple of any elements

   (2) repartition will trigger a shuffle to redistribute data. to reduce #Partitions, 
       consider using coalesce
   (3) in mapPartitionsWithIndex(func): func takes two arguments 
       func(idx, it) <-- idx is similar to spark_partition_id
   (4) in map-related functions, preservesPartitioning is only useful for pair-RDDs when the 
       function changes the key of RDD elements and thus potentially modify the partitions.
       keep the default 'false' unless you are dealing with pair-RDD and the functions
       changed the first item of the tuple.
   (5) in the following line,  groupBy using a tuple (x[3], x[7]): 
   
           rdd.groupBy(lambda x: (x[3], x[7]), 100, partitionFunc=lambda x: hash(x[1]))

       the parameter in `partitionFunc` must be a function of groupBy'd item which is a tuple of two items. 
       Thus you can use hash(x[1]), hash(x), but hash(x[3]) will yield IndexError: tuple index out of range
   (6) check number of records in each partition:

           spark.range(0,100).withColumn('id', expr("IF(id<90,1,id)")).repartition(5,'id').rdd.glom().map(len).collect()
           [1, 3, 3, 1, 92]

           # find Top-10 count of rows in partitions
           df.repartition(N,'LNAME','Address').rdd.glom().map(len).takeOrdered(10, key=lambda x: -x)


pyspark.sql.DataFrame:
---
  + coalesce(numPartitions)
  + repartition(numPartitions, *cols)
    + hash partitioned
  + repartitionByRange(numPartitions, *cols)
    + range partitioned
  + sortWithinPartitions(*cols, **kwargs)
  + foreachPartition(f)
  + toLocalIterator()

  Notes:
   (1) repartition() and repartitionByRange() do not guarentee that different partition cols 
       distributed into different partitions. partition cols can be categorical for both methods.
       the range-boundaries are computed with a randomized algorithm called reservoir sampling
       REF: https://www.waitingforcode.com/apache-spark-sql/range-partitioning-apache-spark-sql/read
   (2) repartition()/repartitionByRange() is very useful when working with mapPartitions() or
       mapPartitionsWithIndex() or foreachPartition(f)
   (3) using the following method to set up the partitions at run-time:

         spark.conf.set('spark.sql.shuffle.partitions', 500)

   (4) If you want to use mapPartitions() to grab the same partitions in one chuck, and then sent to
       driver as toLocalIterator(), you will need to wrap up the data with list as [list_of_dicts] 
       so that the whole chunk is treated as a single item.
       Example: https://stackoverflow.com/questions/58757225

         chunks = (spark_df.repartition(N).rdd
             .mapPartitions(lambda iterator: [[x.asDict() for x in iterator]])
             .toLocalIterator()
         )




pyspark.sql.SparkSession:
---
  + range(start, end=None, step=1, numPartitions=None)


pyspark.sql.DataFrameReader(spark)
---
  + jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)


pyspark.sql.DataFrameWriter(df)
---
  + orc(path, mode=None, partitionBy=None, compression=None)
  + partitionBy(*cols)


pyspark.sql.functions:
---
  + pyspark.sql.functions.spark_partition_id()
    * avilable also with f(idx, rows_iterable) in df.rdd.mapPartitionsWithIndex(f) where idx is used to
      identify spark_partition_id.


References:
[1] https://stackoverflow.com/questions/40416357/spark-sql-difference-between-df-repartition-and-dataframewriter-partitionby/42780452#42780452
[2] https://stackoverflow.com/questions/50775870/pyspark-efficiently-have-partitionby-write-to-same-number-of-total-partitions-a
