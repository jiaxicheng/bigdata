Continue..

Spark SQL higher-order functions:


(8) Modify field values inside a complex data type:
    REF: https://stackoverflow.com/questions/58305514/replace-an-existing-field-within-array-using-spark

    DataFrame is built on top of RDD which is immutable.
    Row object is immutable, you have to recreate a new object in order to update existing Row object.

    Example-1: array of structs

    >>> df.printSchema()
    root
     |-- col1: string (nullable = true)
     |-- col2: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- cola: string (nullable = true)
     |    |    |-- colb: string (nullable = true)
     |-- colb: array (nullable = true)
     |    |-- element: string (containsNull = true)

    #replace col2[*].colb with colb
    df.withColumn('col3', F.expr("""
         transform(sequence(0,size(col2)-1), i -> named_struct('cola', col2[i].cola, 'colb', colb[i]))
       """)).show(truncate=False)

    #+----+------------------------------------+------------------------+---------------------------------------------+
    #|col1|col2                                |colb                    |col3                                         |
    #+----+------------------------------------+------------------------+---------------------------------------------+
    #|t1  |[[a11, b11], [a12, b12]]            |[newb11, newb12]        |[[a11, newb11], [a12, newb12]]               |
    #|t2  |[[a21, b21], [a22, b22], [a13, b13]]|[newb21, newb22, newb23]|[[a21, newb21], [a22, newb22], [a13, newb23]]|
    #|t3  |[[a31, b31]]                        |[newb31]                |[[a31, newb31]]                              |
    #+----+------------------------------------+------------------------+---------------------------------------------+

    Notes: 
     (1) it's fine the number of items in cols2 does not match colb or colb does not exist, all missing items 
         will be filled with empty.
     (2) Method-2: use RDD map function:

        from itertools import zip_longest         

        df.rdd.map(lambda row: 
                Row(col1=row.col1, col2=[ Row(cola=c2.cola, colb=cb) for c2, cb in zip_longest(row.col2, row.colb)]) 
            ).toDF().show(truncate=False)  
        #+----+--------------------------------------+
        #|col1|col2                                  |
        #+----+--------------------------------------+
        #|t1  |[[a11, newb11], [a12, newb12]]        |
        #|t2  |[[a21, newb21], [a22, newb22], [a13,]]|
        #|t3  |[[a31, newb31]]                       |
        #+----+--------------------------------------+

        Adjust zip_longest to the following if colb contain null:

            zip_longest(row.col2, (row.colb if type(row.colb) is list else [row.colb]))

    Example-2: struct of structs (much simpler one)
      REF: https://stackoverflow.com/questions/50123771/change-value-of-nested-column-in-dataframe


(9) Use zip_with + array_repeat + flatten + array_join:
    REF: https://stackoverflow.com/questions/58322574/pyspark-creating-string-with-substring-and-frequency-vector

    df.withColumn('output', expr('''
            array_join(flatten(zip_with(`substr`, `frequency`, (x,y) -> array_repeat(x,int(y)))), ' ')
        ''')).show(truncate=False)
    +-----------------+---------+----------------------------+
    |substr           |frequency|output                      |
    +-----------------+---------+----------------------------+
    |[ham, spam, eggs]|[1, 2, 3]|ham spam spam eggs eggs eggs|
    |[foo, bar]       |[2, 1]   |foo foo bar                 |
    +-----------------+---------+----------------------------+


(10) Use transform, arrays_zip and array_min to find point with cloest distance
     REF: https://stackoverflow.com/questions/58334817/add-column-with-closest-vaues-to-pyspark-dataframe
    
    from pyspark.sql.functions import array, lit, expr
    
    bin_array = np.array([0, 5, 10, 15, 20])
    
    df_new = df.withColumn('bin_array', array([lit(i) for i in bin_array.tolist()])) \
               .withColumn('dist_array', expr('transform(bin_array, x -> abs(x-Score))')) \
               .withColumn('Closest_bin', expr("array_min(arrays_zip(dist_array, bin_array)).bin_array")) \
               .drop('bin_array', 'dist_array')
    
    df_new.show()
    +-----+-----+-----------+
    | Name|Score|Closest_bin|
    +-----+-----+-----------+
    |name1|11.23|         10|
    |name2|14.57|         15|
    |name3| 2.21|          0|
    |name4| 8.76|         10|
    |name5|18.71|         20|
    +-----+-----+-----------+

    Notes: 
     (1) arrays_zip return array of structs, the first field should be used for sorting and finding array_min
     (2) bin_array shuold be created before hands so it can be used in sqlContext






