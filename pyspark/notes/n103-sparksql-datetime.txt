Spark SQL built-in Date/Timestamp functions:


built-in functions:
---
  + current_date:
  + current_timestamp() = now()
  + type casting
    + timestamp(ts/string), cast(ts as 'timestamp')     <-- this can keep the microsecond
    + date(ts/string), cast(ts as 'date')
  + useful functions:
    + string add_months(d, N)                                               <-- exact month
      + HiveQL has one more argument `fmt`
    + string next_day(d, fmt): fmt can be: ['MO', 'TU', .. 'SA', 'SU']
    + string last_day(d)
*    + string trunc(d, fmt): Supported formats: MONTH/MON/MM, YEAR/YYYY/YY
      * compatible with HiveQL
*    + timestamp date_trunc(fmt, ts): 
       fmt can be one of ["YEAR", "YYYY", "YY", "MON", "MONTH", "MM", "DAY", "DD"
                         , "HOUR", "MINUTE", "SECOND", "WEEK", "QUARTER"]
    + date date_add(d, N)  
    + date date_sub(d, N)
    + double months_between(d1, d2)
    + int datediff(end_date, start_date)
  + date/timestamp conversion:
    + bigint unix_timestamp(d, fmt)
      * same as to_unix_timestamp(d, fmt) in SparkSQL
    + date to_date(d, fmt)  
      * same as cast('date') astype('date')
    + timestamp to_timestamp(d, fmt)
      * same to cast('timestamp'), while keep the precision to milliseconds
    + timestamp from_unixtime(bigint d, format='yyyy-MM-dd HH:mm:ss')
    + timestamp from_utc_timestamp(d, tz)
    + timestamp to_utc_timestamp(d, tz)
    + string date_format(d, fmt)
  + Date/time granuity:
    + int year(d)
    + int quarter(date/timestamp/string)
    + int month(d)
    + int dayofmonth(d), day(d)
    + int hour(d)
    + int minute(d)
    + int second(d)
    + int weekofyear(d)
    + int dayofyear(d)
    + int dayofweek(d) or weekday(d): 1:Sun, 2:Mon,..,7:Sat  

  + TimestampType <-> LongType: unix_timestamp, from_unixtime
  + Date/Timestamp/String to String: date_format


Notes: 
  (1) In from_utc_timestamp/to_utc_timestamp,  `d` can be {any primitive type}* = including timestamp/date
      , tinyint/smallint/int/bigint, float/double, decimal
  (2) date_trunc(fmt, ts) convert timestamp to timestamp, when you need a `Timestamp`
      trunc convert DateType into a StringType: when you need a `Date`
  (3) Interval used with Datafram API function:
      interval can not use col_name, i.e. `INTERVAL col1 months` is not working
      df.withColumn('10_days_before', 'datetime_col' - expr('INTERVAL 10 days')) 
  (4) cast(str_col as timestamp) or timestamp(col) is available since Spark 2.1
      def stringToTimestamp(s: UTF8String, timeZoneId: ZoneId)
      Link: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L211
      Trims and parses a given UTF8 timestamp string to the corresponding a corresponding Long Value
      The following formats are allowed:
      * `yyyy`
      * `yyyy-[m]m`
      * `yyyy-[m]m-[d]d`
      * `yyyy-[m]m-[d]d `
      * `yyyy-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`
      * `yyyy-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`
      * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`
      * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`

  (5) cast(string_col as date) or date(col):  available since Spark 2.1+

      def stringToDate(s: UTF8String): Option[SQLDate]
      Parses a given UTF8 date string to the corresponding a corresponding [[Int]] value.
      The following formats are allowed:
      * `yyyy`,
      * `yyyy-[m]m`
      * `yyyy-[m]m-[d]d`
      * `yyyy-[m]m-[d]d `
      * `yyyy-[m]m-[d]d *`        <-- SPACE followed by anything
      * `yyyy-[m]m-[d]dT*`        <-- T followed by any chars, i.e.  `2020-8-15Tdfsdg`

  (6) date_trunc(fmt, ts): Datetime Patterns from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
      `fmt` should be one of ["YEAR", "YYYY", "YY", "MON", "MONTH", "MM", "DAY", "DD", "HOUR", "MINUTE", "SECOND", "WEEK", "QUARTER"]
      * fmt - the format representing the unit to be truncated to
       - "YEAR", "YYYY", "YY" - truncate to the first date of the year that the `ts` falls in, the time part will be zero out
       - "QUARTER" - truncate to the first date of the quarter that the `ts` falls in, the time part will be zero out
       - "MONTH", "MM", "MON" - truncate to the first date of the month that the `ts` falls in, the time part will be zero out
       - "WEEK" - truncate to the Monday of the week that the `ts` falls in, the time part will be zero out
       - "DAY", "DD" - zero out the time part
       - "HOUR" - zero out the minute and second with fraction part
       - "MINUTE"- zero out the second with fraction part
       - "SECOND" -  zero out the second fraction part
       - "MILLISECOND" - zero out the microseconds
       - "MICROSECOND" - everything remains




Some References: 
[1] https://docs-snaplogic.atlassian.net/wiki/spaces/SD/pages/2458071/Date+Functions+and+Properties+Spark+SQL
[2] Java SimpleDateFormats: https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
[3] HiveQL built-in functions: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-ComplexTypeConstructors
[4] SparkSQL: https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#


Example-1: Generating date sequence

   Method-1: use add_months(d, N)       <-- exact month

      transform(sequence(1,6), i -> add_months("2019-01-31", i))
      #[2019-02-28, 2019-03-31, 2019-04-30, 2019-05-31, 2019-06-30, 2019-07-31]

   Method-2: use sequence with interval 1 month

      sequence(to_date("2019-02-01"), to_date("2019-07-05"), interval 1 month)    <-- 30 days
      #[2019-02-01, 2019-03-01, 2019-03-31, 2019-04-30, 2019-05-31, 2019-06-30]

   Example-1.2: generate monthly date sequence for the last 36 months
   https://stackoverflow.com/questions/58341985

      spark.range(36,-1,-1).selectExpr("add_months(date_trunc('MM', current_date()),negative(id)) as date").show(100)


Example-2: Issues with microsecond: 20190111-08:15:45.275753
pyspark only hold to second level:
https://stackoverflow.com/questions/54232494/working-with-microsecond-time-stamps-in-pyspark

    Solution-1: split microsecond from timestamp, convert unix_timestamp(to_timestamp(..)..) to long
                and then plus the substring_index(dt,'.',-1)/1000000 
        REF: https://stackoverflow.com/questions/50648154/microsecond-time-stamps-in-pyspark

        time_df.withColumn("time",  F.expr("""
                unix_timestamp(to_timestamp(substring(dt,0,23), 'yyyyMMdd-HH:mm:ss')) 
              + double(substring_index(dt, '.', -1))/1000000 as time
            """)
        ).show()
        +------------------------+-------------------+
        |dt                      |time               |
        +------------------------+-------------------+
        |20150408-01:12:04.275753|1.428469924275753E9|
        +------------------------+-------------------+

    Solution-2: cast() or timestamp() functions, working since spark 2.1+

        spark.sql("select double(timestamp('2016-07-13 14:33:53.979'))").show(truncate=False)
        +----------------------------------------------------------+
        |CAST(CAST(2016-07-13 14:33:53.979 AS TIMESTAMP) AS DOUBLE)|
        +----------------------------------------------------------+
        |                                          1.468434833979E9|
        +----------------------------------------------------------+

    For standard-formatted timestamp, do type casting is easy to keep resolution:

        spark.sql("select timestamp('2020-08-05 12:34:10.8001234') ts").show(2,0)
        +--------------------------+
        |ts                        |
        +--------------------------+
        |2020-08-05 12:34:10.800123|
        +--------------------------+
        #DataFrame[ts: timestamp]

        # this can also cover timezone converting to utc_timestamp:
        spark.sql("select timestamp('2020-09-08 14:00:00.917+05:00') as ds_with_tz").show(10,0)
        +-----------------------+
        |ds_with_tz             |
        +-----------------------+
        |2020-09-08 09:00:00.917|
        +-----------------------+



Example-3: from year, month and week-number to retrieve a "Friday"(or any dayofweek)
  REF: https://stackoverflow.com/questions/57463921/how-to-truncate-a-date-to-friday-based-on-week-of-month
  Steps:
   (1) Use to_date to create a date column `dt` point to the first date in the month specified
   (2) find the week number `wn`, convert it to integer and minus 1 to point it to the previous week
   (3) use date_add function to find the Monday after `wn` week from dt `date_add(dt, 7*wn-dayofweek(dt))`
   (4) use next_day(.., "Friday") to find the next Friday

    df = spark.read.csv('file:///home/hdfs/test/pyspark/datetime-3.txt', header=True)

    df_new = df.withColumn('dt', F.to_date(F.concat_ws('-', 'Year','Month',F.lit('01')), format="yyyy-MMMM-dd")) \
               .withColumn('wn', F.split('Weeks', ' ')[0].astype('int')-1) \
               .withColumn('ldate', F.expr('next_day(date_add(dt, 7*wn-dayofweek(dt)),"Friday")'))

    df_new.show()
    +----+-----+-----+----------+----------+---+----------+
    |Year|Month|Weeks|      date|        dt| wn|     ldate|
    +----+-----+-----+----------+----------+---+----------+
    |2018|April| 01 W|2018-04-06|2018-04-02|  0|2018-04-06|
    |2018|April| 02 W|2018-04-13|2018-04-02|  1|2018-04-13|
    |2018|April| 03 W|2018-04-20|2018-04-02|  2|2018-04-20|
    |2018|April| 04 W|2018-04-27|2018-04-02|  3|2018-04-27|
    |2018|  May| 01 W|2018-05-04|2018-05-02|  0|2018-05-04|
    |2018|  May| 02 W|2018-05-11|2018-05-02|  1|2018-05-11|
    |2018|  May| 03 W|2018-05-18|2018-05-02|  2|2018-05-18|
    |2018|  May| 04 W|2018-05-25|2018-05-02|  3|2018-05-25|
    |2018| June| 01 W|2018-06-01|2018-06-02|  0|2018-06-01|
    +----+-----+-----+----------+----------+---+----------+



Example-4: get start/end date of Week/Month/Quarter/Year etc

  DateType: make_date and trunc to quarter and week requires Spark 3.0+
       Year start: trunc(`dt`, 'year')
              end: make_date(year(`dt`),12,31) 
    Quarter start: trunc(`dt`, 'quarter') 
              end: last_day(add_months(trunc(`dt`, 'quarter'),2))
      Month start: trunc(`dt`, 'MM')
              end: last_day(trunc(`dt`, 'MM'))
       Week start: trunc(`dt`, 'week')
              end: date_add(trunc(`dt`, 'week'),6)

  TimestampType: 
       Year start: date_trunc('tear', `ts`)
              end: make_timestamp(year(`ts`),12,31,23,59,59)
    Quarter start: date_trunc('quarter', `ts`)
              end: date_trunc('quarter', add_months(`ts`,3)) - interval 1 seconds
      Month start: date_trunc('month', `ts`)
              end: date_trunc('month', add_months(`ts`,1)) - interval 1 seconds
       Week start: date_trunc('week', `ts`)
              end: date_trunc('week', date_add(`ts`,7)) - interval 1 seconds


