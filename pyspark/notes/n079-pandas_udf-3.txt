Continue..

pandas_udf and related applications:
---


Example-7: Solve the linear sum assignment problem using pandas_udf and scipy.optimize.linear_sum_assignment
  REF: https://stackoverflow.com/questions/63160595
  Target: use Hungarian algorithm to find the combination of professor vs student at each timeUnit to 
     have the maximal sum of scores
  Method: 
    (1) use pivot to create professor vs students matrix
    (2) use pandas_udf and scipy.optimize.linear_sum_assignment to calculate the combination of matrix with minimal sum of scores (using applyInPandas)
    (3) use stack to normalize the resulting dataframe from (2) and then calculate is_match column
  Note: for SciPy 2.4.0+, scipy.optimize.linear_sum_assignment support a parameter: maximize
    set this to True, will return combination with max values of the sums

         _, idx = linear_sum_assignment(np.vstack((n1,np.zeros((N-n,N)))), maximize=True)
   
    adjust the corresponding logic as following:
    ---
      (1) Step-1: creating df1:        -first('score')  -->  first('score')
      (2) Step-3: `stack` statement:   -`{c}`           -->  `{c}`  

Code:

    from pyspark.sql.functions import first, expr, dense_rank, broadcast
    from pyspark.sql.types import StructType
    from scipy.optimize import linear_sum_assignment
    from pyspark.sql import Window
    import pandas as pd
    import numpy as np

    df = spark.createDataFrame([        
        ('1596048041', 'p1', 's1', 0.7), ('1596048041', 'p1', 's2', 0.5), ('1596048041', 'p1', 's3', 0.3), 
        ('1596048041', 'p1', 's4', 0.2), ('1596048041', 'p2', 's1', 0.9), ('1596048041', 'p2', 's2', 0.1), 
        ('1596048041', 'p2', 's3', 0.15), ('1596048041', 'p2', 's4', 0.2), ('1596048041', 'p3', 's1', 0.2), 
        ('1596048041', 'p3', 's2', 0.3), ('1596048041', 'p3', 's3', 0.4), ('1596048041', 'p3', 's4', 0.8), 
        ('1596048041', 'p4', 's1', 0.2), ('1596048041', 'p4', 's2', 0.3), ('1596048041', 'p4', 's3', 0.35), 
        ('1596048041', 'p4', 's4', 0.4), ('1596048042', 'p3', 's1', 0.7), ('1596048042', 'p3', 's5', 0.5), 
        ('1596048042', 'p3', 's6', 0.3), ('1596048042', 'p3', 's7', 0.2), ('1596048042', 'p6', 's5', 0.9), 
        ('1596048042', 'p6', 's6', 0.1), ('1596048042', 'p6', 's7', 0.15), ('1596048042', 'p9', 's5', 0.2), 
        ('1596048042', 'p9', 's6', 0.3), ('1596048042', 'p9', 's1', 0.8) 
     ] , ['time', 'professor_id', 'student_id', 'score'])

    N = 4
    cols_student = [*range(1,N+1)]

  Step-0: add an extra column student, and create a new dataframe df3 with all unique combos of time + student_id + student.

    w1 = Window.partitionBy('time').orderBy('student_id')

    df = df.withColumn('student', dense_rank().over(w1))

    df3 = df.select('time','student_id','student').dropDuplicates()
    +----------+----------+-------+                                                 
    |      time|student_id|student|
    +----------+----------+-------+
    |1596048042|        s1|      1|
    |1596048042|        s5|      2|
    |1596048042|        s6|      3|
    |1596048042|        s7|      4|
    |1596048041|        s1|      1|
    |1596048041|        s2|      2|
    |1596048041|        s3|      3|
    |1596048041|        s4|      4|
    +----------+----------+-------+


  Step-1: use pivot to find the matrix of professors vs students, notice we set negative of score
        to the values of pivot so that we can use scipy.optimize.linear_sum_assignment to find the min cost
        of an assignment problem:

    df1 = df.groupby('time','professor_id').pivot('student', cols_student).agg(-first('score'))
    +----------+------------+----+----+-----+-----+                                 
    |      time|professor_id|   1|   2|    3|    4|
    +----------+------------+----+----+-----+-----+
    |1596048042|          p3|-0.7|-0.5| -0.3| -0.2|
    |1596048042|          p9|-0.8|-0.2| -0.3| null|
    |1596048042|          p6|null|-0.9| -0.1|-0.15|
    |1596048041|          p1|-0.7|-0.5| -0.3| -0.2|
    |1596048041|          p2|-0.9|-0.1|-0.15| -0.2|
    |1596048041|          p3|-0.2|-0.3| -0.4| -0.8|
    |1596048041|          p4|-0.2|-0.3|-0.35| -0.4|
    +----------+------------+----+----+-----+-----+


  Step-2: use pandas_udf and scipy.optimize.linear_sum_assignment to get column indices and then 
          assign the corresponding column name to a new column `assigned`:

    # returnSchema contains one more StringType column `assigned` than the input pdf: 
    schema = StructType.fromJson(df1.schema.jsonValue()).add('assigned', 'string')

    def __find_assigned(pdf: pd.DataFrame, sz: int) -> pd.DataFrame:
      cols = pdf.columns[2:]
      n = pdf.shape[0]
      n1 = pdf.iloc[:,2:].fillna(0).values
      _, idx = linear_sum_assignment(np.vstack((n1,np.zeros((sz-n,sz)))))
      return pdf.assign(assigned=[cols[i] for i in idx][:n])

    find_assigned = lambda d: __find_assigned(d, N)

    df2 = df1.groupby('time').applyInPandas(find_assigned, schema) 
    +----------+------------+----+----+-----+-----+--------+                        
    |      time|professor_id|   1|   2|    3|    4|assigned|
    +----------+------------+----+----+-----+-----+--------+
    |1596048042|          p3|-0.7|-0.5| -0.3| -0.2|       3|
    |1596048042|          p9|-0.8|-0.2| -0.3| null|       1|
    |1596048042|          p6|null|-0.9| -0.1|-0.15|       2|
    |1596048041|          p1|-0.7|-0.5| -0.3| -0.2|       2|
    |1596048041|          p2|-0.9|-0.1|-0.15| -0.2|       1|
    |1596048041|          p3|-0.2|-0.3| -0.4| -0.8|       4|
    |1596048041|          p4|-0.2|-0.3|-0.35| -0.4|       3|
    +----------+------------+----+----+-----+-----+--------+


  Step-3: use SparkSQL stack function to normalize the above df2, nagate the score values and 
          filter rows with score is NULL. the `desired` is_match column should have `assigned==student`: 

    df4 = df2.selectExpr(
      'time',
      'professor_id',
      'assigned',
      f"""stack({len(cols_student)},{','.join(f"int('{c}'), -`{c}`" for c in cols_student)}) as (student, score)"""
     ) \
    .filter("score is not NULL") \
    .withColumn('is_match', expr("assigned=student"))

    df4.show(100)
    +----------+------------+--------+-------+-----+--------+                       
    |      time|professor_id|assigned|student|score|is_match|
    +----------+------------+--------+-------+-----+--------+
    |1596048042|          p3|       3|      1|  0.7|   false|
    |1596048042|          p3|       3|      2|  0.5|   false|
    |1596048042|          p3|       3|      3|  0.3|    true|
    |1596048042|          p3|       3|      4|  0.2|   false|
    |1596048042|          p9|       1|      1|  0.8|    true|
    |1596048042|          p9|       1|      2|  0.2|   false|
    |1596048042|          p9|       1|      3|  0.3|   false|
    |1596048042|          p6|       2|      2|  0.9|    true|
    |1596048042|          p6|       2|      3|  0.1|   false|
    |1596048042|          p6|       2|      4| 0.15|   false|
    |1596048041|          p1|       2|      1|  0.7|   false|
    |1596048041|          p1|       2|      2|  0.5|    true|
    |1596048041|          p1|       2|      3|  0.3|   false|
    |1596048041|          p1|       2|      4|  0.2|   false|
    |1596048041|          p2|       1|      1|  0.9|    true|
    |1596048041|          p2|       1|      2|  0.1|   false|
    |1596048041|          p2|       1|      3| 0.15|   false|
    |1596048041|          p2|       1|      4|  0.2|   false|
    |1596048041|          p3|       4|      1|  0.2|   false|
    |1596048041|          p3|       4|      2|  0.3|   false|
    |1596048041|          p3|       4|      3|  0.4|   false|
    |1596048041|          p3|       4|      4|  0.8|    true|
    |1596048041|          p4|       3|      1|  0.2|   false|
    |1596048041|          p4|       3|      2|  0.3|   false|
    |1596048041|          p4|       3|      3| 0.35|    true|
    |1596048041|          p4|       3|      4|  0.4|   false|
    +----------+------------+--------+-------+-----+--------+


  Step-4: use join to convert student back to student_id (use broadcast join if possible):

    df_new = df4.join(df3, on=["time", "student"])
    
    df_new.show(100)
    +----------+-------+------------+--------+-----+--------+----------+            
    |      time|student|professor_id|assigned|score|is_match|student_id|
    +----------+-------+------------+--------+-----+--------+----------+
    |1596048042|      1|          p3|       3|  0.7|   false|        s1|
    |1596048042|      2|          p3|       3|  0.5|   false|        s5|
    |1596048042|      3|          p3|       3|  0.3|    true|        s6|
    |1596048042|      4|          p3|       3|  0.2|   false|        s7|
    |1596048042|      1|          p9|       1|  0.8|    true|        s1|
    |1596048042|      2|          p9|       1|  0.2|   false|        s5|
    |1596048042|      3|          p9|       1|  0.3|   false|        s6|
    |1596048042|      2|          p6|       2|  0.9|    true|        s5|
    |1596048042|      3|          p6|       2|  0.1|   false|        s6|
    |1596048042|      4|          p6|       2| 0.15|   false|        s7|
    |1596048041|      1|          p1|       2|  0.7|   false|        s1|
    |1596048041|      2|          p1|       2|  0.5|    true|        s2|
    |1596048041|      3|          p1|       2|  0.3|   false|        s3|
    |1596048041|      4|          p1|       2|  0.2|   false|        s4|
    |1596048041|      1|          p2|       1|  0.9|    true|        s1|
    |1596048041|      2|          p2|       1|  0.1|   false|        s2|
    |1596048041|      3|          p2|       1| 0.15|   false|        s3|
    |1596048041|      4|          p2|       1|  0.2|   false|        s4|
    |1596048041|      1|          p3|       4|  0.2|   false|        s1|
    |1596048041|      2|          p3|       4|  0.3|   false|        s2|
    |1596048041|      3|          p3|       4|  0.4|   false|        s3|
    |1596048041|      4|          p3|       4|  0.8|    true|        s4|
    |1596048041|      1|          p4|       3|  0.2|   false|        s1|
    |1596048041|      2|          p4|       3|  0.3|   false|        s2|
    |1596048041|      3|          p4|       3| 0.35|    true|        s3|
    |1596048041|      4|          p4|       3|  0.4|   false|        s4|
    +----------+-------+------------+--------+-----+--------+----------+

    df_new = df_new.drop("student", "assigned")



Example-8: use pandas_udf with broadcast variable
  REF: https://stackoverflow.com/questions/63369936
  Target: we have a mapping of iso2 to keywords containing regex patterns, we want to find all 
          matched iso2 in a text column, each match must return its own iso2 code:
  Method: (1) set up regex patterns to a dictionary and broadcast it to worker nodes
          (2) use pandas_udf and re.findall to return an array of arrays of string
          (3) use flatten to merge (2) into array of strings 
  Code:

    from pyspark.sql.functions import expr, flatten, pandas_udf, PandasUDFType
    from pandas import Series, read_csv
    import re
    from typing import Iterator
    from pyspark.broadcast import Broadcast

    df = spark.createDataFrame([
        ["3030 Whispering Pines Circle, Prosper Texas, US","John"],   
        ["Kalverstraat Amsterdam","Mary"],   
        ["Kalverstraat Amsterdam, Netherlands","Lex"],
        ["xvcv", "ddd"]
    ]).toDF("address","name")

    # Pandas dataframe containing country to keywords pattern mapping
    df_regex = read_csv("file:///home/xicheng/test/regex.csv", sep=";")

    df_regex["keywords"] = df_regex["keywords"].str.replace(r'(^|\\.)([^\\]*)', lambda m: m.group(1) + m.group(2).upper())

    df_ptn = spark.sparkContext.broadcast(
        df_regex.groupby('iso2').agg({'keywords':lambda x: '(?m)' + '|'.join(x)})["keywords"].to_dict()
    )
    df_ptn.value
    #{'CA': '(?m)\\bALBERTA\\b|\\bNOVA SCOTIA\\b|\\bNOVA SCOTIA\\b|\\bWHITEHORSE\\b|\\bCA$',
    # 'NL': '(?m)\\bAMSTERDAM\\b|\\bNETHERLANDS\\b|\\bNL$',
    # 'US': '(?m)\\bARIZONA\\b|\\bTEXAS\\b|\\bFLORIDA\\b|\\bCHICAGO\\b|\\bAMSTERDAM\\b|\\bPROSPER\\b|\\bUS$'}

  Method-1: return an array of arrays and do flatten with Spark:

    def __get_iso2_1(addr:Series, ptn:Broadcast) -> Series:    
      return Series([ [[k]*len(re.findall(v,s)) for k,v in ptn.value.items()] for s in addr ])

    get_iso2_1 = pandas_udf(lambda x:__get_iso2_1(x, df_ptn), "array<array<string>>")

    df.withColumn('iso2', flatten(get_iso2_1(expr("upper(address)")))).show()
    +--------------------+----+------------+                                        
    |             address|name|        iso2|
    +--------------------+----+------------+
    |3030 Whispering P...|John|[US, US, US]|
    |Kalverstraat Amst...|Mary|    [NL, US]|
    |Kalverstraat Amst...| Lex|[NL, NL, US]|
    |                xvcv| ddd|          []|
    +--------------------+----+------------+

  # using Iterator[Series] -> Iterator[Series]:

    from typing import Iterator, Callable
    get_iso2_3: Callable[[Iterator[Series]],Iterator[Series]] = pandas_udf(lambda x:__get_iso2_1(x, df_ptn), "array<array<string>>")


  Method-2: flatten the list using Python REF: https://stackoverflow.com/questions/952914

    from operator import iconcat
    from functools import reduce

    def __get_iso2_2(addr:Series, ptn:Broadcast) -> Series: 
      return Series([ reduce(iconcat, [[k]*len(re.findall(v,s)) for k,v in ptn.value.items()]) for s in addr ])

    get_iso2_2 = pandas_udf(lambda x:__get_iso2_2(x, df_ptn), "array<string>")

    df.withColumn('iso2', get_iso2_2(expr("upper(address)"))).show()                                                     
    +--------------------+----+------------+
    |             address|name|        iso2|
    +--------------------+----+------------+
    |3030 Whispering P...|John|[US, US, US]|
    |Kalverstraat Amst...|Mary|    [NL, US]|
    |Kalverstraat Amst...| Lex|[NL, NL, US]|
    |                xvcv| ddd|          []|
    +--------------------+----+------------+

  To find unique contries:

    def __get_iso2_3(addr:Series, ptn:Broadcast) -> Series:
      return Series([ [k for k,v in ptn.value.items() if re.search(v,s)] for s in addr ])

    get_iso2_1 = pandas_udf(lambda x:__get_iso2_3(x, df_ptn), "array<string>") 

    df.withColumn('iso2', get_iso2_1(expr("upper(address)"))).show()
    +--------------------+----+--------+
    |             address|name|    iso2|
    +--------------------+----+--------+
    |3030 Whispering P...|John|    [US]|
    |Kalverstraat Amst...|Mary|[NL, US]|
    |Kalverstraat Amst...| Lex|[NL, US]|
    |                xvcv| ddd|      []|
    +--------------------+----+--------+



Example-9: use reverse_geocoder to find City name from lat+long(Iterator[Series] -> Iterator[Series])
  REF: https://stackoverflow.com/questions/63633808/finding-state-name-from-latlong-in-pysaprk-and-scala
  Method: require a external Python model `reverse_geocoder`, ideal for pandas_udf
  Code:

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    import reverse_geocoder as rg
    from pandas import Series
    from typing import Iterator

    df = spark.createDataFrame([
        (712,[30.60422, -98.39]),
        (714,[37.28816, -121.9]),
        (713,[37.31304, -121.9])
    ], ['id', 'latlong'])

    @pandas_udf("string")
    def get_state_name(it: Iterator[Series]) -> Iterator[Series]:
      for latlon in it:
        yield Series([ e["admin1"] for e in rg.search([*map(tuple,latlon)]) ])

    df.withColumn('state', get_state_name('latlong')).show()
    +---+------------------+----------+
    | id|           latlong|     state|
    +---+------------------+----------+
    |712|[30.60422, -98.39]|     Texas|
    |714|[37.28816, -121.9]|California|
    |713|[37.31304, -121.9]|California|
    +---+------------------+----------+

  Note: `rg.search()` takes a tuple or list of tuples as argument. when the ArrayType column as input of pandas_udf
        we need to map the array into tuple and then retrieve the key "admin1" from the resulting list of OrderedDict's
        see below for an example:

        [OrderedDict([('lat', '30.58908'),
                      ('lon', '-98.38392'),
                      ('name', 'Granite Shoals'),
                      ('admin1', 'Texas'),
                      ('admin2', 'Burnet County'),
                      ('cc', 'US')])]
 


Example-10: pd.Series -> pd.Series with lambda function to decode lat/long from polyline encoded text column:
  REF: https://stackoverflow.com/questions/63670324/pandas-udf-giving-error-related-to-pyarrow

    from pyspark.sql.functions import udf, pandas_udf
    import polyline
    from pandas import Series
    from typing import Callable

    df = spark.createDataFrame([(1, (38.5, -120.2)),(2,(40.7, -120.9)), (3,(43.2, -126.4))],['id', 'latlon'])

    # polyline.encode takes a list of tuple, a pd.Series as input
    geoloc_encode = udf(lambda x: polyline.encode([x]))
    
    # polyline.decode takes string as input and a list of tuple as output
    lat_lang_udf: Callable[[Series],Series] = pandas_udf(
        lambda x: Series([polyline.decode(s)[0] for s in x]), "array<double>")

    df.withColumn('geoloc', geoloc_encode('latlon')).withColumn('lat_long', lat_lang_udf('geoloc')).show()
    +---+--------------+----------+--------------+
    | id|        latlon|    geoloc|      lat_long|
    +---+--------------+----------+--------------+
    |  1|[38.5, -120.2]|_p~iF~ps|U|[38.5, -120.2]|
    |  2|[40.7, -120.9]|_flwF~g|`V|[40.7, -120.9]|
    |  3|[43.2, -126.4]|_otfG~nnbW|[43.2, -126.4]|
    +---+--------------+----------+--------------+

  Some example use of polyline:

    s = Series([(38.5, -120.2), (40.7, -120.9), (43.2, -126.4)])
    polyline.encode(s)
    #'_p~iF~ps|U_ulL~ugC_hgN~eq`@'

    polyline.decode('_p~iF~ps|U_ulL~ugC_hgN~eq`@')
    #[(38.5, -120.2), (40.7, -120.9), (43.2, -126.4)]



Example-11: use pandas_udf and spacy to find 'PERSON' or "ORG" of a name
  REF: https://stackoverflow.com/questions/63681625/using-spacy-with-pandas-udf-pyspark-error-i-cant-fix

    df = spark.createDataFrame([
      ['John Doe'],
      ['Jane Doe'],
      ['Microsoft Corporation'],
      ['Apple Inc.']
    ]).toDF("name",)


    from pandas import Series
    from pyspark.sql.functions import pandas_udf
    from typing import Iterator

    # below requires Spark 3.0+
    def __get_entities(it: Iterator[Series]) -> Iterator[Series]:
      import spacy
      nlp = spacy.load("en_core_web_sm")
      for x in it:
        yield Series([ [e.label_ for e in nlp(s).ents if e.label_ in ('PERSON','ORG')] for s in x ])

    get_entities = pandas_udf(__get_entities, "array<string>")

    df.withColumn('dt', get_entities('name')).show(truncate=False)
    +---------------------+--------+                                                
    |name                 |dt      |
    +---------------------+--------+
    |John Doe             |[PERSON]|
    |Jane Doe             |[PERSON]|
    |Microsoft Corporation|[ORG]   |
    |Apple Inc.           |[ORG]   |
    +---------------------+--------+

  Notes:
   (1) Python modules to be installed:

        pip3 install -U spacy
        python -m spacy download en_core_web_lg
        
   (2) this is one case when package should be imported and loaded locally on executers, there is no
       point to load them on driver and then broadcast to workers.



Example-12: sorting by one column only, other columns keep original order:
  REF: https://stackoverflow.com/questions/63775680
  Method: use pandas_udf to set up the new column, notice to do pdf.sort_values(['time']) as
     rows after groupby is nondeterministic.

    df = spark.createDataFrame([ 
          ('123456', 2, 'A'), ('123457', 4, 'B'), ('123458', 7, 'C') 
        , ('123459', 5, 'D'), ('123460', 3, 'E'), ('123461', 1, 'F') 
        , ('123462', 9, 'G'), ('123463', 8, 'H'), ('123464', 6, 'I') 
    ], ['time', 'col1', 'col2']) 

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType
    from numpy import sort as np_sort
    import pandas as pd

    schema = StructType.fromJson(df.schema.jsonValue()).add('col1_sorted', 'integer')

    def get_col1_sorted(pdf:pd.DataFrame) -> pd.DataFrame:
      return pdf.sort_values(['time']).assign(col1_sorted=np_sort(pdf["col1"]))

    df.groupby().applyInPandas(get_col1_sorted, schema).show()
    +------+----+----+-----------+
    |  time|col1|col2|col1_sorted|
    +------+----+----+-----------+
    |123456|   2|   A|          1|
    |123457|   4|   B|          2|
    |123458|   7|   C|          3|
    |123459|   5|   D|          4|
    |123460|   3|   E|          5|
    |123461|   1|   F|          6|
    |123462|   9|   G|          7|
    |123463|   8|   H|          8|
    |123464|   6|   I|          9|
    +------+----+----+-----------+

  An alternative using higher order functions:

    df_new = df \
        .groupby() \
        .agg(expr("array_sort(collect_list(struct(time,col1,col2))) as data")) \
        .withColumn('col1_sorted', expr("array_sort(transform(data, x -> x.col1))")) \
        .selectExpr("inline(arrays_zip(data,col1_sorted))") \
        .select('data.*', 'col1_sorted') 

