Continue..

pandas_udf and related applications:
---


Example-7: Solve the linear sum assignment problem using pandas_udf and scipy.optimize.linear_sum_assignment
  REF: https://stackoverflow.com/questions/63160595
  Target: use Hungarian algorithm to find the combination of professor vs student at each timeUnit to 
     have the maximal sum of scores
  Method: 
    (1) use pivot to create professor vs students matrix
    (2) use pandas_udf and scipy.optimize.linear_sum_assignment to calculate the combination of matrix with minimal sum of scores
    (3) use stack to normalize the resulting dataframe from (2) and then calculate is_match column
  Note: for SciPy 2.4.0+, scipy.optimize.linear_sum_assignment support a parameter: maximize
    set this to True, will return combination with max values of the sums

         _, idx = linear_sum_assignment(np.vstack((n1,np.zeros((N-n,N)))), maximize=True)
   
    adjust the corresponding logic as following:
    ---
      (1) Step-1: creating df1:        -first('score')  -->  first('score')
      (2) Step-3: `stack` statement:   -`{c}`           -->  `{c}`  

Code:

    from pyspark.sql.functions import pandas_udf, PandasUDFType, first, expr, dense_rank, broadcast
    from pyspark.sql.types import StructType
    from scipy.optimize import linear_sum_assignment
    from pyspark.sql import Window
    import numpy as np

    df = spark.createDataFrame([        
        ('1596048041', 'p1', 's1', 0.7), ('1596048041', 'p1', 's2', 0.5), ('1596048041', 'p1', 's3', 0.3), 
        ('1596048041', 'p1', 's4', 0.2), ('1596048041', 'p2', 's1', 0.9), ('1596048041', 'p2', 's2', 0.1), 
        ('1596048041', 'p2', 's3', 0.15), ('1596048041', 'p2', 's4', 0.2), ('1596048041', 'p3', 's1', 0.2), 
        ('1596048041', 'p3', 's2', 0.3), ('1596048041', 'p3', 's3', 0.4), ('1596048041', 'p3', 's4', 0.8), 
        ('1596048041', 'p4', 's1', 0.2), ('1596048041', 'p4', 's2', 0.3), ('1596048041', 'p4', 's3', 0.35), 
        ('1596048041', 'p4', 's4', 0.4), ('1596048042', 'p3', 's1', 0.7), ('1596048042', 'p3', 's5', 0.5), 
        ('1596048042', 'p3', 's6', 0.3), ('1596048042', 'p3', 's7', 0.2), ('1596048042', 'p6', 's5', 0.9), 
        ('1596048042', 'p6', 's6', 0.1), ('1596048042', 'p6', 's7', 0.15), ('1596048042', 'p9', 's5', 0.2), 
        ('1596048042', 'p9', 's6', 0.3), ('1596048042', 'p9', 's1', 0.8) 
     ] , ['time', 'professor_id', 'student_id', 'score'])

    N = 4
    cols_student = [*range(1,N+1)]

  Step-0: add an extra column student, and create a new dataframe df3 with all unique combos of time + student_id + student.

    w1 = Window.partitionBy('time').orderBy('student_id')

    df = df.withColumn('student', dense_rank().over(w1))

    df3 = df.select('time','student_id','student').dropDuplicates()
    +----------+----------+-------+                                                 
    |      time|student_id|student|
    +----------+----------+-------+
    |1596048042|        s1|      1|
    |1596048042|        s5|      2|
    |1596048042|        s6|      3|
    |1596048042|        s7|      4|
    |1596048041|        s1|      1|
    |1596048041|        s2|      2|
    |1596048041|        s3|      3|
    |1596048041|        s4|      4|
    +----------+----------+-------+


  Step-1: use pivot to find the matrix of professors vs students, notice we set negative of score
        to the values of pivot so that we can use scipy.optimize.linear_sum_assignment to find the min cost
        of an assignment problem:

    df1 = df.groupby('time','professor_id').pivot('student', cols_student).agg(-first('score'))
    +----------+------------+----+----+-----+-----+                                 
    |      time|professor_id|   1|   2|    3|    4|
    +----------+------------+----+----+-----+-----+
    |1596048042|          p3|-0.7|-0.5| -0.3| -0.2|
    |1596048042|          p9|-0.8|-0.2| -0.3| null|
    |1596048042|          p6|null|-0.9| -0.1|-0.15|
    |1596048041|          p1|-0.7|-0.5| -0.3| -0.2|
    |1596048041|          p2|-0.9|-0.1|-0.15| -0.2|
    |1596048041|          p3|-0.2|-0.3| -0.4| -0.8|
    |1596048041|          p4|-0.2|-0.3|-0.35| -0.4|
    +----------+------------+----+----+-----+-----+


  Step-2: use pandas_udf and scipy.optimize.linear_sum_assignment to get column indices and then 
          assign the corresponding column name to a new column `assigned`:

    # returnSchema contains one more StringType column `assigned` than the input pdf: 
    schema = StructType.fromJson(df1.schema.jsonValue()).add('assigned', 'string')

    def __find_assigned(pdf, sz):
      cols = pdf.columns[2:]
      n = pdf.shape[0]
      n1 = pdf.iloc[:,2:].fillna(0).values
      _, idx = linear_sum_assignment(np.vstack((n1,np.zeros((sz-n,sz)))))
      return pdf.assign(assigned=[cols[i] for i in idx][:n])


    find_assigned = pandas_udf(lambda x: __find_assigned(x,N), schema, PandasUDFType.GROUPED_MAP)

    df2 = df1.groupby('time').apply(find_assigned) 
    +----------+------------+----+----+-----+-----+--------+                        
    |      time|professor_id|   1|   2|    3|    4|assigned|
    +----------+------------+----+----+-----+-----+--------+
    |1596048042|          p3|-0.7|-0.5| -0.3| -0.2|       3|
    |1596048042|          p9|-0.8|-0.2| -0.3| null|       1|
    |1596048042|          p6|null|-0.9| -0.1|-0.15|       2|
    |1596048041|          p1|-0.7|-0.5| -0.3| -0.2|       2|
    |1596048041|          p2|-0.9|-0.1|-0.15| -0.2|       1|
    |1596048041|          p3|-0.2|-0.3| -0.4| -0.8|       4|
    |1596048041|          p4|-0.2|-0.3|-0.35| -0.4|       3|
    +----------+------------+----+----+-----+-----+--------+


  Step-3: use SparkSQL stack function to normalize the above df2, nagate the score values and 
          filter rows with score is NULL. the `desired` is_match column should have `assigned==student`: 

    df_new = df2.selectExpr(
      'time',
      'professor_id',
      'assigned',
      f"""stack({len(cols_student)},{','.join(f"int('{c}'), -`{c}`" for c in cols_student)}) as (student, score)"""
     ) \
    .filter("score is not NULL") \
    .withColumn('is_match', expr("assigned=student"))

    df_new.show(100)
    +----------+------------+--------+-------+-----+--------+                       
    |      time|professor_id|assigned|student|score|is_match|
    +----------+------------+--------+-------+-----+--------+
    |1596048042|          p3|       3|      1|  0.7|   false|
    |1596048042|          p3|       3|      2|  0.5|   false|
    |1596048042|          p3|       3|      3|  0.3|    true|
    |1596048042|          p3|       3|      4|  0.2|   false|
    |1596048042|          p9|       1|      1|  0.8|    true|
    |1596048042|          p9|       1|      2|  0.2|   false|
    |1596048042|          p9|       1|      3|  0.3|   false|
    |1596048042|          p6|       2|      2|  0.9|    true|
    |1596048042|          p6|       2|      3|  0.1|   false|
    |1596048042|          p6|       2|      4| 0.15|   false|
    |1596048041|          p1|       2|      1|  0.7|   false|
    |1596048041|          p1|       2|      2|  0.5|    true|
    |1596048041|          p1|       2|      3|  0.3|   false|
    |1596048041|          p1|       2|      4|  0.2|   false|
    |1596048041|          p2|       1|      1|  0.9|    true|
    |1596048041|          p2|       1|      2|  0.1|   false|
    |1596048041|          p2|       1|      3| 0.15|   false|
    |1596048041|          p2|       1|      4|  0.2|   false|
    |1596048041|          p3|       4|      1|  0.2|   false|
    |1596048041|          p3|       4|      2|  0.3|   false|
    |1596048041|          p3|       4|      3|  0.4|   false|
    |1596048041|          p3|       4|      4|  0.8|    true|
    |1596048041|          p4|       3|      1|  0.2|   false|
    |1596048041|          p4|       3|      2|  0.3|   false|
    |1596048041|          p4|       3|      3| 0.35|    true|
    |1596048041|          p4|       3|      4|  0.4|   false|
    +----------+------------+--------+-------+-----+--------+


  Step-4: use join to convert student back to student_id (use broadcast join if possible):

    df_new = df_new.join(df3, on=["time", "student"])
    
    df_new.show(100)
    +----------+-------+------------+--------+-----+--------+----------+            
    |      time|student|professor_id|assigned|score|is_match|student_id|
    +----------+-------+------------+--------+-----+--------+----------+
    |1596048042|      1|          p3|       3|  0.7|   false|        s1|
    |1596048042|      2|          p3|       3|  0.5|   false|        s5|
    |1596048042|      3|          p3|       3|  0.3|    true|        s6|
    |1596048042|      4|          p3|       3|  0.2|   false|        s7|
    |1596048042|      1|          p9|       1|  0.8|    true|        s1|
    |1596048042|      2|          p9|       1|  0.2|   false|        s5|
    |1596048042|      3|          p9|       1|  0.3|   false|        s6|
    |1596048042|      2|          p6|       2|  0.9|    true|        s5|
    |1596048042|      3|          p6|       2|  0.1|   false|        s6|
    |1596048042|      4|          p6|       2| 0.15|   false|        s7|
    |1596048041|      1|          p1|       2|  0.7|   false|        s1|
    |1596048041|      2|          p1|       2|  0.5|    true|        s2|
    |1596048041|      3|          p1|       2|  0.3|   false|        s3|
    |1596048041|      4|          p1|       2|  0.2|   false|        s4|
    |1596048041|      1|          p2|       1|  0.9|    true|        s1|
    |1596048041|      2|          p2|       1|  0.1|   false|        s2|
    |1596048041|      3|          p2|       1| 0.15|   false|        s3|
    |1596048041|      4|          p2|       1|  0.2|   false|        s4|
    |1596048041|      1|          p3|       4|  0.2|   false|        s1|
    |1596048041|      2|          p3|       4|  0.3|   false|        s2|
    |1596048041|      3|          p3|       4|  0.4|   false|        s3|
    |1596048041|      4|          p3|       4|  0.8|    true|        s4|
    |1596048041|      1|          p4|       3|  0.2|   false|        s1|
    |1596048041|      2|          p4|       3|  0.3|   false|        s2|
    |1596048041|      3|          p4|       3| 0.35|    true|        s3|
    |1596048041|      4|          p4|       3|  0.4|   false|        s4|
    +----------+-------+------------+--------+-----+--------+----------+

    df_new = df_new.drop("student", "assigned")



Example-8: use pandas_udf with broadcast variable
  REF: https://stackoverflow.com/questions/63369936
  Target: we have a mapping of iso2 to keywords containing regex patterns, we want to find all 
          matched iso2 in a text column, each match must return its own iso2 code:
  Method: (1) set up regex patterns to a dictionary and broadcast it to worker nodes
          (2) use pandas_udf and re.findall to return an array of arrays of string
          (3) use flatten to merge (2) into array of strings 
  Code:

    from pyspark.sql.functions import expr, flatten, pandas_udf, PandasUDFType
    from pandas import Series, read_csv
    import re

    df = spark.createDataFrame([
        ["3030 Whispering Pines Circle, Prosper Texas, US","John"],   
        ["Kalverstraat Amsterdam","Mary"],   
        ["Kalverstraat Amsterdam, Netherlands","Lex"],
        ["xvcv", "ddd"]
    ]).toDF("address","name")

    # Pandas dataframe containing country to keywords pattern mapping
    df_regex = read_csv("file:///home/xicheng/test/regex.csv", sep=";")

    df_regex["keywords"] = df_regex["keywords"].str.replace(r'(^|\\.)([^\\]*)', lambda m: m.group(1) + m.group(2).upper())

    df_ptn = spark.sparkContext.broadcast(
        df_regex.groupby('iso2').agg({'keywords':lambda x: '(?m)' + '|'.join(x)})["keywords"].to_dict()
    )
    df_ptn.value
    #{'CA': '(?m)\\bALBERTA\\b|\\bNOVA SCOTIA\\b|\\bNOVA SCOTIA\\b|\\bWHITEHORSE\\b|\\bCA$',
    # 'NL': '(?m)\\bAMSTERDAM\\b|\\bNETHERLANDS\\b|\\bNL$',
    # 'US': '(?m)\\bARIZONA\\b|\\bTEXAS\\b|\\bFLORIDA\\b|\\bCHICAGO\\b|\\bAMSTERDAM\\b|\\bPROSPER\\b|\\bUS$'}

  Method-1: return an array of arrays and do flatten with Spark:

    def __get_iso2_1(addr, ptn):    
      return Series([ [[k]*len(re.findall(v,s)) for k,v in ptn.value.items()] for s in addr ])

    get_iso2_1 = pandas_udf(lambda x:__get_iso2_1(x, df_ptn), "array<array<string>>", PandasUDFType.SCALAR)

    df.withColumn('iso2', flatten(get_iso2_1(expr("upper(address)")))).show()
    +--------------------+----+------------+                                        
    |             address|name|        iso2|
    +--------------------+----+------------+
    |3030 Whispering P...|John|[US, US, US]|
    |Kalverstraat Amst...|Mary|    [NL, US]|
    |Kalverstraat Amst...| Lex|[NL, NL, US]|
    |                xvcv| ddd|          []|
    +--------------------+----+------------+

  Method-2: flatten the list using Python REF: https://stackoverflow.com/questions/952914

    from operator import iconcat
    from functools import reduce

    def __get_iso2_2(addr, ptn): 
      return Series([ reduce(iconcat, [[k]*len(re.findall(v,s)) for k,v in ptn.value.items()]) for s in addr ])

    get_iso2_2 = pandas_udf(lambda x:__get_iso2_2(x, df_ptn), "array<string>", PandasUDFType.SCALAR)

    df.withColumn('iso2', get_iso2_2(expr("upper(address)"))).show()                                                     
    +--------------------+----+------------+
    |             address|name|        iso2|
    +--------------------+----+------------+
    |3030 Whispering P...|John|[US, US, US]|
    |Kalverstraat Amst...|Mary|    [NL, US]|
    |Kalverstraat Amst...| Lex|[NL, NL, US]|
    |                xvcv| ddd|          []|
    +--------------------+----+------------+

  To find unique contries:

    def __get_iso2_3(addr, ptn):
      return Series([ [k for k,v in ptn.value.items() if re.search(v,s)] for s in addr ])

    get_iso2_1 = pandas_udf(lambda x:__get_iso2_3(x, df_ptn), "array<string>", PandasUDFType.SCALAR) 

    df.withColumn('iso2', get_iso2_1(expr("upper(address)"))).show()
    +--------------------+----+--------+
    |             address|name|    iso2|
    +--------------------+----+--------+
    |3030 Whispering P...|John|    [US]|
    |Kalverstraat Amst...|Mary|[NL, US]|
    |Kalverstraat Amst...| Lex|[NL, US]|
    |                xvcv| ddd|      []|
    +--------------------+----+--------+



Example-9: use reverse_geocoder to find City name from lat+long
  REF: https://stackoverflow.com/questions/63633808/finding-state-name-from-latlong-in-pysaprk-and-scala
  Method: require a external Python model `reverse_geocoder`, ideal for pandas_udf
  Code:

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    import reverse_geocoder as rg
    from pandas import Series

    df = spark.createDataFrame([
        (712,[30.60422, -98.39]),
        (714,[37.28816, -121.9]),
        (713,[37.31304, -121.9])
    ], ['id', 'latlong'])

    @pandas_udf("string", PandasUDFType.SCALAR)
    def get_state_name(latlon):
      return Series([ e["admin1"] for e in rg.search([*map(tuple,latlon)]) ])

    df.withColumn('state', get_state_name('latlong')).show()
    +---+------------------+----------+
    | id|           latlong|     state|
    +---+------------------+----------+
    |712|[30.60422, -98.39]|     Texas|
    |714|[37.28816, -121.9]|California|
    |713|[37.31304, -121.9]|California|
    +---+------------------+----------+

  Note: `rg.search()` takes a tuple or list of tuples as argument. when the ArrayType column as input of pandas_udf
        we need to map the array into tuple and then retrieve the key "admin1" from the resulting list of OrderedDict's
        see below for an example:

        [OrderedDict([('lat', '30.58908'),
                      ('lon', '-98.38392'),
                      ('name', 'Granite Shoals'),
                      ('admin1', 'Texas'),
                      ('admin2', 'Burnet County'),
                      ('cc', 'US')])]
 


Example-10: decode lat/long from polyline encoded text column:
  REF: https://stackoverflow.com/questions/63670324/pandas-udf-giving-error-related-to-pyarrow

    from pyspark.sql.functions import udf, pandas_udf
    import polyline
    from pandas import Series

    df = spark.createDataFrame([(1, (38.5, -120.2)),(2,(40.7, -120.9)), (3,(43.2, -126.4))],['id', 'latlon'])

    # polyline.encode takes a list of tuple, a pd.Series as input
    geoloc_encode = udf(lambda x: polyline.encode([x]))
    
    # polyline.decode takes string as input and a list of tuple as output
    lat_lang_udf = pandas_udf(lambda x: Series([polyline.decode(s)[0] for s in x]), "array<double>")

    df.withColumn('geoloc', geoloc_encode('latlon')).withColumn('lat_long', lat_lang_udf('geoloc')).show()
    +---+--------------+----------+--------------+
    | id|        latlon|    geoloc|      lat_long|
    +---+--------------+----------+--------------+
    |  1|[38.5, -120.2]|_p~iF~ps|U|[38.5, -120.2]|
    |  2|[40.7, -120.9]|_flwF~g|`V|[40.7, -120.9]|
    |  3|[43.2, -126.4]|_otfG~nnbW|[43.2, -126.4]|
    +---+--------------+----------+--------------+

  Some example use of polyline:

    s = Series([(38.5, -120.2), (40.7, -120.9), (43.2, -126.4)])
    polyline.encode(s)
    #'_p~iF~ps|U_ulL~ugC_hgN~eq`@'

    polyline.decode('_p~iF~ps|U_ulL~ugC_hgN~eq`@')
    #[(38.5, -120.2), (40.7, -120.9), (43.2, -126.4)]




Example-11: use pandas_udf and spacy to find 'PERSON' or "ORG" of a name
  REF: https://stackoverflow.com/questions/63681625/using-spacy-with-pandas-udf-pyspark-error-i-cant-fix

    df = spark.createDataFrame([
      ['John Doe'],
      ['Jane Doe'],
      ['Microsoft Corporation'],
      ['Apple Inc.']
    ]).toDF("name",)


    from pandas import Series
    from pyspark.sql.functions import pandas_udf

    def __get_entities(x: Series) -> Series:
      import spacy
      nlp = spacy.load("en_core_web_lg")
      return Series([ [e.label_ for e in nlp(s).ents if e.label_ in ('PERSON','ORG')] for s in x])

    get_entities = pandas_udf(__get_entities, "array<string>")

    df.withColumn('dt', get_entities('name')).show(truncate=False)
    +---------------------+--------+                                                
    |name                 |dt      |
    +---------------------+--------+
    |John Doe             |[PERSON]|
    |Jane Doe             |[PERSON]|
    |Microsoft Corporation|[ORG]   |
    |Apple Inc.           |[ORG]   |
    +---------------------+--------+

  Notes:
   (1) Python modules to be installed:

        pip3 install -U spacy
        python -m spacy download en_core_web_lg
        
   (2) this is one case when package should be imported and loaded locally on executers, there is no
       point to load them on driver and then broadcast to workers.

