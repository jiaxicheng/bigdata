https://stackoverflow.com/questions/59429032/find-the-start-time-and-end-time-when-temperature-is-changed-1degree-in-spark-sq

Using aggregate function with a complex DataType (struct<ret:array<start_ts:timestamp,end_ts:timestamp>,Temp:double>)
and using concat for array aggregation based on conditions. This can help resolve the following issues which require
udf:
  + https://github.com/jiaxicheng/bigdata/tree/master/pyspark/codes/053-window-with-gaps
  + https://github.com/jiaxicheng/bigdata/tree/master/pyspark/scala_udf

The following code tries to solve the following problem(not the same as OP's question):
---
 (1) For the same ID, sorted by Timestamp, Temperature `Temp(F)` must be continuously dropped
 (2) Start a new Window whenever the Temp(F) drop is greater than 1.0

    from pyspark.sql.functions import (lag, expr, sum as fsum, col, when, coalesce, lit
        , max as fmax, array_sort, struct, collect_list)
    
    from pyspark.sql import Window
    
    df = spark.read.csv('/home/xicheng/test/window-7-1.txt', header=True, inferSchema=True)
    
    w1 = Window.partitionBy('ID').orderBy('TimeStamp')
    w2 = Window.partitionBy('ID', 'g')
    
    df1 = (df.withColumn('is_fallen', coalesce(lag('Temp(F)').over(w1) > col('Temp(F)'),lit(True))) 
        .withColumn('g', fsum(when(col('is_fallen'),0).otherwise(1)).over(w1)) 
        .withColumn('g_is_fallen', fmax('is_fallen').over(w2)) 
        .filter('g_is_fallen'))
           
    df1.show()
    +---+-------------------+-------+---------+---+-----------+                     
    | ID|          TimeStamp|Temp(F)|is_fallen|  g|g_is_fallen|
    +---+-------------------+-------+---------+---+-----------+
    |  1|2019-12-20 10:08:35|   74.1|     true|  0|       true|
    |  1|2019-12-20 10:09:37|   73.7|     true|  0|       true|
    |  1|2019-12-20 10:10:32|   73.5|     true|  0|       true|
    |  1|2019-12-20 10:12:02|   73.0|     true|  0|       true|
    |  1|2019-12-20 10:13:35|   73.3|    false|  1|       true|
    |  1|2019-12-20 10:14:37|   73.0|     true|  1|       true|
    |  1|2019-12-20 10:15:43|   72.7|     true|  1|       true|
    |  1|2019-12-20 10:16:47|   72.4|     true|  1|       true|
    |  1|2019-12-20 10:17:57|   72.0|     true|  1|       true|
    |  3|2019-12-20 10:09:40|   74.0|     true|  0|       true|
    |  3|2019-12-20 10:15:42|   72.7|     true|  0|       true|
    |  2|2019-12-20 10:08:40|   74.3|     true|  0|       true|
    |  2|2019-12-20 10:09:40|   74.2|     true|  0|       true|
    |  2|2019-12-20 10:12:40|   73.3|     true|  0|       true|
    |  2|2019-12-20 10:13:40|   73.1|     true|  0|       true|
    |  2|2019-12-20 10:14:40|   72.9|     true|  0|       true|
    |  2|2019-12-20 10:16:40|   72.5|     true|  0|       true|
    |  2|2019-12-20 10:17:40|   72.3|     true|  0|       true|
    |  2|2019-12-20 10:18:40|   72.0|     true|  0|       true|
    |  2|2019-12-20 10:22:50|   73.0|    false|  3|       true|
    +---+-------------------+-------+---------+---+-----------+
    
    df2 = df1.groupby('ID', 'g').agg(array_sort(collect_list(struct('TimeStamp', col('Temp(F)').alias('Temp')))).alias('lst'))

"""
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
     Window - with Gap solution using aggregate
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
(1) create a struct  with schema: `struct<ret:array<start_ts:timestamp,end_ts:timestamp>,Temp:double>`
(2) x is accumulator with the above schema and y is the new value which is `struct<TimeStamp:timestamp,Temp:double>`
(3) iterate through the array_sort(collect_list(..))
    (3.1) if the last item of x.ret has start_ts is NULL:
       start a new sequence, adjust the last item of x.ret to `named_struct('start_ts', y.TimeStamp, 'end_ts', NULL)` 
       and x.Temp to `y.Temp`
    (3.2) elif x.Temp - y.Temp >= 1.0: 
       close and save an existing sequence `(element_at(x.ret, -1).start_ts as start_ts, y.TimeStamp as end_ts)`
       and append a new NULL item (NULL as start_ts, NULL as end_ts)
       reset `Temp` to 0, this value does not matter, it will be reset in (3.1)
    (3.3) else
       keep as is: named_struct('ret', x.ret, 'Temp', x.Temp)
(4) retrieve the ret field from the above aggregation result which is an array of structs
(5) filter out items with z.end_ts is NULL (incomplete, the sequence does not satisfy condition)
(6) use `inline` function to explode the array of structs 

"""
    df2.selectExpr("ID", "g",""" 
    
       inline(
         filter(
           aggregate(
             lst, 
             (array((timestamp(NULL) as start_ts, timestamp(NULL) as end_ts)) as ret, double(0) as Temp),
             (x, y) ->  
    
                CASE 
                  WHEN element_at(x.ret, -1).start_ts is NULL THEN 
                    named_struct(
                      'ret', concat(
                               slice(x.ret,1,size(x.ret)-1),
                               array(named_struct('start_ts', y.TimeStamp, 'end_ts', NULL))
                             ), 
                      'Temp', y.Temp 
                    )
                  WHEN x.Temp - y.Temp >= 1.0 THEN
                    named_struct(
                      'ret', concat(
                               slice(x.ret,1,size(x.ret)-1),
                               array(
                                 (element_at(x.ret, -1).start_ts as start_ts, y.TimeStamp as end_ts),
                                 (NULL as start_ts, NULL as end_ts)
                               )
                             ), 
                      'Temp', 0
                    ) 
                  ELSE
                    named_struct('ret', x.ret, 'Temp', x.Temp)
                  END 
             ).ret,
    
           z -> z.end_ts is not NULL
         )
       )
    
    """).show(truncate=False) 
    +---+---+-------------------+-------------------+                               
    |ID |g  |start_ts           |end_ts             |
    +---+---+-------------------+-------------------+
    |1  |0  |2019-12-20 10:08:35|2019-12-20 10:12:02|
    |1  |1  |2019-12-20 10:13:35|2019-12-20 10:17:57|
    |3  |0  |2019-12-20 10:09:40|2019-12-20 10:15:42|
    |2  |0  |2019-12-20 10:08:40|2019-12-20 10:12:40|
    |2  |0  |2019-12-20 10:13:40|2019-12-20 10:18:40|
    +---+---+-------------------+-------------------+

