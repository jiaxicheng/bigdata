Continue..

Spark SQL higher-order functions:

(22) Count array items using transform + filter(size) or aggregate functions:
  REF: https://stackoverflow.com/questions/60517738/alternative-to-groupby-for-pyspark-dataframe

  Below example
  Method-1: using transform + filter
  1. we use array_max to find max_value in list and generate a sequence from 0 to this max value
  2. iterate the above list using `x` and transform it into the number of items in array `data` using(filter+size)
    
    df = spark.createDataFrame([(2,[1,2]), (2,[1,2]), (3,[1,2,3]), (3,[1,2])],['timestamp', 'vars'])
    
    from pyspark.sql.functions import flatten, collect_list
    
    df.groupby('timestamp').agg(flatten(collect_list('vars')).alias('data')) \
      .selectExpr("timestamp", "transform(sequence(0, array_max(data)), x -> size(filter(data, y -> y = x)))") \
      .show(truncate=False)
    +---------+------------+                                                        
    |timestamp|vars        |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+
    
  Method-2: using aggregate function:
  1. find max_value in list and set zero_value of `acc` as an Array with `max_value+1` items with all zero values
  2. iterate through the Array `data` using `y` and reduce the array `acc` by its index `i` and value `x` based 
     on the value of `y` using `transform(acc, (x,i) -> IF(i=y, x+1, x))`
  see below:
    
    df.groupby('timestamp').agg(flatten(collect_list('vars')).alias('data')) \ 
       .selectExpr("timestamp", """ 
        
         aggregate( 
           data, 
           /* use an array as zero_value, size = array_max(data))+1 and all values are zero */
           array_repeat(0, int(array_max(data))+1), 
           /* increment the ith value of the Array by 1 if i == y */
           (acc, y) -> transform(acc, (x,i) -> IF(i=y, x+1, x)) 
         ) as vars 
                                      
    """).show(truncate=False)
    +---------+------------+                                                        
    |timestamp|vars        |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+
    

  Method-3: using explode + groupby + count and split data into two arrays: `indices` and `values`
    then use transform and array_position to fill in the final array:

    from pyspark.sql.functions import collect_list, struct

    df.selectExpr("timestamp", "explode(vars) as var") \
        .groupby('timestamp','var') \
        .count() \
        .groupby("timestamp") \
        .agg(collect_list(struct("var","count")).alias("data")) \
        .selectExpr(
            "timestamp",
            "transform(data, x -> x.var) as indices",
            "transform(data, x -> x.count) as values"
        ).selectExpr(
            "timestamp",
            "transform(sequence(0, array_max(indices)), i -> IFNULL(values[array_position(indices,i)-1],0)) as new_vars"
        ).show(truncate=False)
    +---------+------------+                                                        
    |timestamp|new_vars    |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+


  Method-4 using pyspark.ml.feature.CountVectorizer

    from pyspark.ml.feature import CountVectorizerModel
    from pyspark.sql.functions import collect_list, flatten

    # find max N of array vars from all rows 
    N = df.selectExpr('max(array_max(vars)) as m').first().m

    # create models based on the list from range(N) (convert IntegerType to StringType)
    model = CountVectorizerModel.from_vocabulary([*map(str,range(N+1))], inputCol="vars", outputCol="vec_vars")

    # convert array of ints to array of strings, groupby and create a flattened list for each timestamp
    df1 = df.withColumn('vars', df.vars.astype("array<string>")) \
        .groupby('timestamp') \
        .agg(flatten(collect_list("vars")).alias("vars"))

    # use the model to create the resulting SparseVector, then convert it into ArrayType column (udf)
    model.transform(df1).withColumn("new_vars", to_array("vec_vars")).show(truncate=False)                             
    +---------+---------------+-------------------------+--------------------+      
    |timestamp|vars           |vec_vars                 |new_vars            |
    +---------+---------------+-------------------------+--------------------+
    |3        |[1, 2, 3, 1, 2]|(4,[1,2,3],[2.0,2.0,1.0])|[0.0, 2.0, 2.0, 1.0]|
    |2        |[1, 2, 1, 2]   |(4,[1,2],[2.0,2.0])      |[0.0, 2.0, 2.0, 0.0]|
    +---------+---------------+-------------------------+--------------------+
   


(23) Use transform to split a Array list into chunk and take the sum of each chunk for a new list:
    REF:https://stackoverflow.com/questions/60537729/how-to-split-an-array-into-chunks-and-find-the-sum-of-the-chunks-and-store-the-o

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([
      (1, [0, 2, 0, 3, 1, 4, 2, 7]),
      (2, [0, 4, 4, 3, 4, 2, 2, 5])
    ], ["Index", "finalArray"])

    N = 3

    sql_expr = """
        transform(
          /* create a sequence from 0 to number_of_chunks-1 */
          sequence(0,ceil(size(finalArray)/{0})-1),
          /* iterate the above sequence */
          i ->
            /* create a sequence from 0 to chunk_size-1
               calculate the sum of values containing every chunk_size items
            */
            aggregate(
              sequence(0,{0}-1),
              0L,
              (acc, y) -> acc + ifnull(finalArray[i*{0}+y],0)
            )
        )
    """

    df.withColumn('new_vars', expr(sql_expr.format(N))).show(truncate=False)                                           
    +-----+------------------------+---------+
    |Index|finalArray              |new_vars |
    +-----+------------------------+---------+
    |1    |[0, 2, 0, 3, 1, 4, 2, 7]|[2, 8, 9]|
    |2    |[0, 4, 4, 3, 4, 2, 2, 5]|[8, 9, 7]|
    +-----+------------------------+---------+


    

