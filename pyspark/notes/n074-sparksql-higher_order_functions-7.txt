Continue..

Spark SQL higher-order functions:

(22) Count array items using transform + filter(size) or aggregate functions:
  REF: https://stackoverflow.com/questions/60517738/alternative-to-groupby-for-pyspark-dataframe

  Below example
  Method-1: using transform + filter
  1. we use array_max to find max_value in list and generate a sequence from 0 to this max value
  2. iterate the above list using `x` and transform it into the number of items in array `data` using(filter+size)
    
    df = spark.createDataFrame([(2,[1,2]), (2,[1,2]), (3,[1,2,3]), (3,[1,2])],['timestamp', 'vars'])
    
    from pyspark.sql.functions import flatten, collect_list
    
    df.groupby('timestamp').agg(flatten(collect_list('vars')).alias('data')) \
      .selectExpr("timestamp", "transform(sequence(0, array_max(data)), x -> size(filter(data, y -> y = x)))") \
      .show(truncate=False)
    +---------+------------+                                                        
    |timestamp|vars        |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+
    
  Method-2: using aggregate function:
  1. find max_value in list and set zero_value of `acc` as an Array with `max_value+1` items with all zero values
  2. iterate through the Array `data` using `y` and reduce the array `acc` by its index `i` and value `x` based 
     on the value of `y` using `transform(acc, (x,i) -> IF(i=y, x+1, x))`
  see below:
    
    df.groupby('timestamp').agg(flatten(collect_list('vars')).alias('data')) \ 
       .selectExpr("timestamp", """ 
        
         aggregate( 
           data, 
           /* use an array as zero_value, size = array_max(data))+1 and all values are zero */
           array_repeat(0, int(array_max(data))+1), 
           /* increment the ith value of the Array by 1 if i == y */
           (acc, y) -> transform(acc, (x,i) -> IF(i=y, x+1, x)) 
         ) as vars 
                                      
    """).show(truncate=False)
    +---------+------------+                                                        
    |timestamp|vars        |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+
    

  Method-3: using explode + groupby + count and split data into two arrays: `indices` and `values`
    then use transform and array_position to fill in the final array:

    from pyspark.sql.functions import collect_list, struct

    df.selectExpr("timestamp", "explode(vars) as var") \
        .groupby('timestamp','var') \
        .count() \
        .groupby("timestamp") \
        .agg(collect_list(struct("var","count")).alias("data")) \
        .selectExpr(
            "timestamp",
            "transform(data, x -> x.var) as indices",
            "transform(data, x -> x.count) as values"
        ).selectExpr(
            "timestamp",
            "transform(sequence(0, array_max(indices)), i -> IFNULL(values[array_position(indices,i)-1],0)) as new_vars"
        ).show(truncate=False)
    +---------+------------+                                                        
    |timestamp|new_vars    |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+


  Method-4 using pyspark.ml.feature.CountVectorizer

    from pyspark.ml.feature import CountVectorizerModel
    from pyspark.sql.functions import collect_list, flatten

    # find max N of array vars from all rows 
    N = df.selectExpr('max(array_max(vars)) as m').first().m

    # create models based on the list from range(N) (convert IntegerType to StringType)
    model = CountVectorizerModel.from_vocabulary([*map(str,range(N+1))], inputCol="vars", outputCol="vec_vars")

    # convert array of ints to array of strings, groupby and create a flattened list for each timestamp
    df1 = df.withColumn('vars', df.vars.astype("array<string>")) \
        .groupby('timestamp') \
        .agg(flatten(collect_list("vars")).alias("vars"))

    # use the model to create the resulting SparseVector, then convert it into ArrayType column (udf)
    model.transform(df1).withColumn("new_vars", to_array("vec_vars")).show(truncate=False)                             
    +---------+---------------+-------------------------+--------------------+      
    |timestamp|vars           |vec_vars                 |new_vars            |
    +---------+---------------+-------------------------+--------------------+
    |3        |[1, 2, 3, 1, 2]|(4,[1,2,3],[2.0,2.0,1.0])|[0.0, 2.0, 2.0, 1.0]|
    |2        |[1, 2, 1, 2]   |(4,[1,2],[2.0,2.0])      |[0.0, 2.0, 2.0, 0.0]|
    +---------+---------------+-------------------------+--------------------+
   


(23) Use transform to split a Array list into chunk and take the sum of each chunk for a new list:
    REF:https://stackoverflow.com/questions/60537729/how-to-split-an-array-into-chunks-and-find-the-sum-of-the-chunks-and-store-the-o

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([
      (1, [0, 2, 0, 3, 1, 4, 2, 7]),
      (2, [0, 4, 4, 3, 4, 2, 2, 5])
    ], ["Index", "finalArray"])

    N = 3

    sql_expr = """
        transform(
          /* create a sequence from 0 to number_of_chunks-1 */
          sequence(0,ceil(size(finalArray)/{0})-1),
          /* iterate the above sequence */
          i ->
            /* create a sequence from 0 to chunk_size-1
               calculate the sum of values containing every chunk_size items
            */
            aggregate(
              sequence(0,{0}-1),
              0L,
              (acc, y) -> acc + ifnull(finalArray[i*{0}+y],0)
            )
        )
    """

    df.withColumn('new_vars', expr(sql_expr.format(N))).show(truncate=False)                                           
    +-----+------------------------+---------+
    |Index|finalArray              |new_vars |
    +-----+------------------------+---------+
    |1    |[0, 2, 0, 3, 1, 4, 2, 7]|[2, 8, 9]|
    |2    |[0, 4, 4, 3, 4, 2, 2, 5]|[8, 9, 7]|
    +-----+------------------------+---------+

  Note: to handle decimal, will need to force the data type to the merge part of the aggregate function:

     sql_expr = """ 
         transform( 
           /* create a sequence from 0 to number_of_chunks-1 */ 
           sequence(0,ceil(size(finalArray)/{0})-1), i -> 
             aggregate( 
               sequence(0,{0}-1), 
               /* aggregate to a decimal(12,2) */
               cast(0 as decimal(12,2)), 
               /* force the resulting acc to decimal(12,2) 
                * otherwise it yields the `data type mismatch` ERROR
                */
               (acc, y) -> cast(acc + ifnull(finalArray[i*{0}+y],0) as decimal(12,2)) 
             ) 
         ) 
       """
    df.withColumn('new_vars', expr(sql_expr.format(N))).show(truncate=False)                                           
    +-----+------------------------+------------------+
    |Index|finalArray              |new_vars          |
    +-----+------------------------+------------------+
    |1    |[0, 2, 0, 3, 1, 4, 2, 7]|[2.00, 8.00, 9.00]|
    |2    |[0, 4, 4, 3, 4, 2, 2, 5]|[8.00, 9.00, 7.00]|
    +-----+------------------------+------------------+
    #DataFrame[Index: bigint, finalArray: array<bigint>, new_vars: array<decimal(12,2)>]




(24) use aggregate + map_concat to merge a array of maps into a map
  REF: https://stackoverflow.com/questions/60629812/how-to-convert-map-values-using-spark-built-in-functions/60646490#60646490
  Target: retrieve the first non-null values from a map-value in a list of StructType fields
  (a) use transform to convert an array(map_keys()) into an array of maps
  (b) use aggregate to merge array of maps into a map

    # example dataframe
    df = spark.createDataFrame([({"k0": {"b": 100, "d":"ab" }, "k1": {"a": 1 },"k3": {"c": 12.5, "d":"ef" },"k4": {"d":"hh"}},)],"col1:map<string,struct<a:int,b:int,c:double,d:string>>")

    df.selectExpr("""
      aggregate(
        transform(map_keys(col1), x -> map(x, coalesce(col1[x].a,col1[x].b,col1[x].c,col1[x].d))), 
        /* zero_value: empty map */
        map(), 
        /* merge: use map_concat */
        (acc,y) -> map_concat(acc, y)
      )  as col1
    """)

  For Spark 3.0+, use transform_values:

    df.selectExpr("transform_values(col1, (k,v) -> coalesce(v.a, v.b, v.c, v.d)) as col1").show(truncate=False)
    +------------------------------------------+
    |col1                                      |
    +------------------------------------------+
    |[k3 -> 12.5, k4 -> hh, k0 -> 100, k1 -> 1]|
    +------------------------------------------+
    #DataFrame[col1: map<string,string>]




