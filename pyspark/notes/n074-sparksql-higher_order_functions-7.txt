Continue..

Spark SQL higher-order functions:

(22) Count array items using transform + filter(size) or aggregate functions:
  REF: https://stackoverflow.com/questions/60517738/alternative-to-groupby-for-pyspark-dataframe

  Below example
  Method-1: using transform + filter
  1. we use array_max to find max_value in list and generate a sequence from 0 to this max value
  2. iterate the above list using `x` and transform it into the number of items in array `data` using(filter+size)
    
    df = spark.createDataFrame([(2,[1,2]), (2,[1,2]), (3,[1,2,3]), (3,[1,2])],['timestamp', 'vars'])
    
    from pyspark.sql.functions import flatten, collect_list
    
    df.groupby('timestamp').agg(flatten(collect_list('vars')).alias('data')) \
      .selectExpr("timestamp", "transform(sequence(0, array_max(data)), x -> size(filter(data, y -> y = x)))") \
      .show(truncate=False)
    +---------+------------+                                                        
    |timestamp|vars        |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+
    
  Method-2: using aggregate function:
  1. find max_value in list and set zero_value of `acc` as an Array with `max_value+1` items with all zero values
  2. iterate through the Array `data` using `y` and reduce the array `acc` by its index `i` and value `x` based 
     on the value of `y` using `transform(acc, (x,i) -> IF(i=y, x+1, x))`
  see below:
    
    df.groupby('timestamp').agg(flatten(collect_list('vars')).alias('data')) \ 
       .selectExpr("timestamp", """ 
        
         aggregate( 
           data, 
           /* use an array as zero_value, size = array_max(data))+1 and all values are zero */
           array_repeat(0, int(array_max(data))+1), 
           /* increment the ith value of the Array by 1 if i == y */
           (acc, y) -> transform(acc, (x,i) -> IF(i=y, x+1, x)) 
         ) as vars 
                                      
    """).show(truncate=False)
    +---------+------------+                                                        
    |timestamp|vars        |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+
    

  Method-3: using explode + groupby + count and split data into two arrays: `indices` and `values`
    then use transform and array_position to fill in the final array:

    from pyspark.sql.functions import collect_list, struct

    df.selectExpr("timestamp", "explode(vars) as var") \
        .groupby('timestamp','var') \
        .count() \
        .groupby("timestamp") \
        .agg(collect_list(struct("var","count")).alias("data")) \
        .selectExpr(
            "timestamp",
            "transform(data, x -> x.var) as indices",
            "transform(data, x -> x.count) as values"
        ).selectExpr(
            "timestamp",
            "transform(sequence(0, array_max(indices)), i -> IFNULL(values[array_position(indices,i)-1],0)) as new_vars"
        ).show(truncate=False)
    +---------+------------+                                                        
    |timestamp|new_vars    |
    +---------+------------+
    |3        |[0, 2, 2, 1]|
    |2        |[0, 2, 2]   |
    +---------+------------+


  Method-4 using pyspark.ml.feature.CountVectorizer

    from pyspark.ml.feature import CountVectorizerModel
    from pyspark.sql.functions import collect_list, flatten

    # find max N of array vars from all rows 
    N = df.selectExpr('max(array_max(vars)) as m').first().m

    # create models based on the list from range(N) (convert IntegerType to StringType)
    model = CountVectorizerModel.from_vocabulary([*map(str,range(N+1))], inputCol="vars", outputCol="vec_vars")

    # convert array of ints to array of strings, groupby and create a flattened list for each timestamp
    df1 = df.withColumn('vars', df.vars.astype("array<string>")) \
        .groupby('timestamp') \
        .agg(flatten(collect_list("vars")).alias("vars"))

    # use the model to create the resulting SparseVector, then convert it into ArrayType column (udf)
    model.transform(df1).withColumn("new_vars", to_array("vec_vars")).show(truncate=False)                             
    +---------+---------------+-------------------------+--------------------+      
    |timestamp|vars           |vec_vars                 |new_vars            |
    +---------+---------------+-------------------------+--------------------+
    |3        |[1, 2, 3, 1, 2]|(4,[1,2,3],[2.0,2.0,1.0])|[0.0, 2.0, 2.0, 1.0]|
    |2        |[1, 2, 1, 2]   |(4,[1,2],[2.0,2.0])      |[0.0, 2.0, 2.0, 0.0]|
    +---------+---------------+-------------------------+--------------------+
   


(23) Use transform to split a Array list into chunk and take the sum of each chunk for a new list:
    REF:https://stackoverflow.com/questions/60537729/how-to-split-an-array-into-chunks-and-find-the-sum-of-the-chunks-and-store-the-o

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([
      (1, [0, 2, 0, 3, 1, 4, 2, 7]),
      (2, [0, 4, 4, 3, 4, 2, 2, 5])
    ], ["Index", "finalArray"])

    N = 3

    sql_expr = """
        transform(
          /* create a sequence from 0 to number_of_chunks-1 */
          sequence(0,ceil(size(finalArray)/{0})-1),
          /* iterate the above sequence */
          i ->
            /* create a sequence from 0 to chunk_size-1
               calculate the sum of values containing every chunk_size items
            */
            aggregate(
              sequence(0,{0}-1),
              0L,
              (acc, y) -> acc + ifnull(finalArray[i*{0}+y],0)
            )
        )
    """

    df.withColumn('new_vars', expr(sql_expr.format(N))).show(truncate=False)                                           
    +-----+------------------------+---------+
    |Index|finalArray              |new_vars |
    +-----+------------------------+---------+
    |1    |[0, 2, 0, 3, 1, 4, 2, 7]|[2, 8, 9]|
    |2    |[0, 4, 4, 3, 4, 2, 2, 5]|[8, 9, 7]|
    +-----+------------------------+---------+

  Note: to handle decimal, will need to force the data type to the merge part of the aggregate function:

     sql_expr = """ 
         transform( 
           /* create a sequence from 0 to number_of_chunks-1 */ 
           sequence(0,ceil(size(finalArray)/{0})-1), i -> 
             aggregate( 
               sequence(0,{0}-1), 
               /* aggregate to a decimal(12,2) */
               cast(0 as decimal(12,2)), 
               /* force the resulting acc to decimal(12,2) 
                * otherwise it yields the `data type mismatch` ERROR
                */
               (acc, y) -> cast(acc + ifnull(finalArray[i*{0}+y],0) as decimal(12,2)) 
             ) 
         ) 
       """
    df.withColumn('new_vars', expr(sql_expr.format(N))).show(truncate=False)                                           
    +-----+------------------------+------------------+
    |Index|finalArray              |new_vars          |
    +-----+------------------------+------------------+
    |1    |[0, 2, 0, 3, 1, 4, 2, 7]|[2.00, 8.00, 9.00]|
    |2    |[0, 4, 4, 3, 4, 2, 2, 5]|[8.00, 9.00, 7.00]|
    +-----+------------------------+------------------+
    #DataFrame[Index: bigint, finalArray: array<bigint>, new_vars: array<decimal(12,2)>]

 Example-23-2:
  REF:https://stackoverflow.com/q/64873861/9510729
  Target: zip two lists which cover daily usage count and minutes, time is saved in key field with preceeding `bin`, 
          say `bin012` is to 2AM (12*10 minutes), we want to do the hourly summary, which have N=6 chuck for 0-143 
          potential bins. 
  Method: use transform + sequence

    from pyspark.sql import functions as F
    
    j1 = """{"name": "abc1", "usage_count": {"bin102": 1, "bin103": 1, "bin104": 1, "bin105": 1, "bin110": 1, "bin112": 
        1, "bin120": 1, "bin121": 1, "bin122": 1, "bin123": 1, "bin124": 1, "bin136": 2, "bin137": 1, "bin138": 1, "bin139":
         1, "bin140": 1, "bin141": 2, "bin142": 2}, "usage_min": {"bin102": 7.7, "bin103": 10, "bin104": 10, "bin105": 2.5, 
        "bin110": 0.1, "bin112": 0.8, "bin120": 6.8, "bin121": 10, "bin122": 10, "bin123": 10, "bin124": 4.3, "bin136": 2.5,
         "bin137": 10, "bin138": 10, "bin139": 10, "bin140": 10, "bin141": 9.3, "bin142": 3.8}, "p_name": "abc_1"}
    """
    
    rdd = sc.parallelize([j1])
    
    df = spark.read.json(rdd, schema="name string, p_name string, usage_count map<string,int>, usage_min map<string,float>")
    
    df = spark.createDataFrame([(j1,)],['e_data'])
    
    schema = "name string, p_name string, usage_count map<string,int>, usage_min map<string,float>"
    
    df0 = df.withColumn("e_data", F.from_json("e_data", schema)).select("*", "e_data.*").drop("e_data")
    +----+------+--------------------+--------------------+
    |name|p_name|         usage_count|           usage_min|
    +----+------+--------------------+--------------------+
    |abc1| abc_1|[bin102 -> 1, bin...|[bin102 -> 7.7, b...|
    +----+------+--------------------+--------------------+
    
    # merge usage_count and usage_min into a single map, remove `bin` from keys
    df1 = df0.withColumn("map1", F.expr("""
        map_from_entries(
          transform(map_keys(usage_count), k -> 
            struct(int(substr(k,4)), named_struct('count', usage_count[k], 'min', usage_min[k]))
          )
        )
    """))
    df1.show(1,80,vertical=True)
    -RECORD 0---------------------------------------------------------------------------------------
     name        | abc1                                                                             
     p_name      | abc_1                                                                            
     usage_count | [bin102 -> 1, bin103 -> 1, bin104 -> 1, bin105 -> 1, bin110 -> 1, bin112 -> 1... 
     usage_min   | [bin102 -> 7.7, bin103 -> 10.0, bin104 -> 10.0, bin105 -> 2.5, bin110 -> 0.1,... 
     map1        | [102 -> [1, 7.7], 103 -> [1, 10.0], 104 -> [1, 10.0], 105 -> [1, 2.5], 110 ->...
    
    # iterating through 0-23 and split the bins into 6 elements each and use aggregate to sum the count/minute
    df2 = df1.withColumn("usage", F.expr("""
        transform(sequence(0,23), i ->
          aggregate(
            sequence(i*6,(i+1)*6-1), 
            (0L as count, 0D as min),
            (acc, x) -> named_struct(
               'count', acc.count + ifnull(map1[x].count,0),
               'min', acc.min + ifnull(map1[x].min,0.0)
            ),
            acc -> (acc.count as count, round(acc.min,1) as min)
          )
        )
    """))
    
    # format the output count and min
    df_new = df2.selectExpr("name","p_name","array_join(usage.count, ',') as count", "array_join(usage.min, ',') as min")
    
    df_new.show(1,0,vertical=True)
    #-RECORD 0-----------------------------------------------------------------------------------------------------
    # name   | abc1                                                                                                
    # p_name | abc_1                                                                                               
    # count  | 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,2,0,5,0,3,7                                                     
    # min    | 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,30.2,0.9,0.0,41.1,0.0,12.5,43.1 
    

(24) use aggregate + map_concat to merge a array of maps into a map
  REF: https://stackoverflow.com/questions/60629812/how-to-convert-map-values-using-spark-built-in-functions/60646490#60646490
  Target: retrieve the first non-null values from a map-value in a list of StructType fields
  (a) use transform to convert an array(map_keys()) into an array of maps
  (b) use aggregate to merge array of maps into a map

    # example dataframe
    df = spark.createDataFrame([({"k0": {"b": 100, "d":"ab" }, "k1": {"a": 1 },"k3": {"c": 12.5, "d":"ef" },"k4": {"d":"hh"}},)],"col1:map<string,struct<a:int,b:int,c:double,d:string>>")

    df.selectExpr("""
      aggregate(
        transform(map_keys(col1), x -> map(x, coalesce(col1[x].a,col1[x].b,col1[x].c,col1[x].d))), 
        /* zero_value: empty map */
        map(), 
        /* merge: use map_concat */
        (acc,y) -> map_concat(acc, y)
      )  as col1
    """)

  For Spark 3.0+, use transform_values:

    df.selectExpr("transform_values(col1, (k,v) -> coalesce(v.a, v.b, v.c, v.d)) as col1").show(truncate=False)
    +------------------------------------------+
    |col1                                      |
    +------------------------------------------+
    |[k3 -> 12.5, k4 -> hh, k0 -> 100, k1 -> 1]|
    +------------------------------------------+
    #DataFrame[col1: map<string,string>]




