Continue..

Spark SQL higher-order functions:

(22) Count array items using groupby-pivot or pyspark.ml.feature.CountVectorizer:
  REF: https://stackoverflow.com/questions/60517738/alternative-to-groupby-for-pyspark-dataframe

  Below example
    
    df = spark.createDataFrame([(2,[1,2]), (2,[1,2]), (3,[1,2,3]), (3,[1,2])],['timestamp', 'vars'])
    
  Method-1: using explode + groupby + pivot:

    from pyspark.sql.functions import array 

    N = df.selectExpr('max(array_max(vars)) as m').first().m

    df1 = (df.selectExpr("timestamp", "explode(vars) as var") 
        .groupby('timestamp')  
        .pivot('var', range(N+1)) 
        .count() 
        .fillna(0) 
    )

    df1.select("timestamp", array(df1.columns[1:]).alias('vars')).show()
    +---------+------------+
    |timestamp|        vars|
    +---------+------------+
    |        3|[0, 2, 2, 1]|
    |        2|[0, 2, 2, 0]|
    +---------+------------+


  Method-2: using pyspark.ml.feature.CountVectorizer

    from pyspark.ml.feature import CountVectorizerModel
    from pyspark.ml.functions import vector_to_array
    from pyspark.sql.functions import collect_list, flatten

    # find max N of array vars from all rows 
    N = df.selectExpr('max(array_max(vars)) as m').first().m

    # create models based on the list from range(N) (convert IntegerType to StringType)
    model = CountVectorizerModel.from_vocabulary([*map(str,range(N+1))], inputCol="vars", outputCol="vec_vars")

    # convert array of ints to array of strings, groupby and create a flattened list for each timestamp
    df1 = df.withColumn('vars', df.vars.astype("array<string>")) \
        .groupby('timestamp') \
        .agg(flatten(collect_list("vars")).alias("vars"))

    # use the model to create the resulting SparseVector, then convert it into ArrayType column (udf)
    model.transform(df1).withColumn("new_vars", vector_to_array("vec_vars").astype('array<int>')).show(truncate=False) 
    +---------+---------------+-------------------------+------------+
    |timestamp|vars           |vec_vars                 |new_vars    |
    +---------+---------------+-------------------------+------------+
    |3        |[1, 2, 3, 1, 2]|(4,[1,2,3],[2.0,2.0,1.0])|[0, 2, 2, 1]|
    |2        |[1, 2, 1, 2]   |(4,[1,2],[2.0,2.0])      |[0, 2, 2, 0]|
    +---------+---------------+-------------------------+------------+



(23) Use transform to split a Array list into chunk and take the sum of each chunk for a new list:
    REF:https://stackoverflow.com/questions/60537729/

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([
      (1, [0, 2, 0, 3, 1, 4, 2, 7]),
      (2, [0, 4, 4, 3, 4, 2, 2, 5])
    ], ["Index", "finalArray"])

    N = 3

    split_by = lambda x: expr(f"""
        transform(
            /* create a sequence from 0 to `size(finalArray)-1` step with {N} */
            sequence(0,size(finalArray)-1,{x}),
            /* iterate the above sequence */
            i ->
            /* create a sequence from i to i+chunk_size-1
               calculate the sum of values containing every chunk_size items
            */
            aggregate(
                sequence(i,i+{x-1}),
                0L,
                (acc, y) -> acc + ifnull(finalArray[y],0)
            )
        )
    """)
 

    df.withColumn('new_vars', split_by(N)).show(truncate=False)                                           
    +-----+------------------------+---------+
    |Index|finalArray              |new_vars |
    +-----+------------------------+---------+
    |1    |[0, 2, 0, 3, 1, 4, 2, 7]|[2, 8, 9]|
    |2    |[0, 4, 4, 3, 4, 2, 2, 5]|[8, 9, 7]|
    +-----+------------------------+---------+


 Example-23-2:
  REF:https://stackoverflow.com/q/64873861/9510729
  Target: zip two lists which cover daily usage count and minutes, time is saved in key field with preceeding `bin`, 
          say `bin012` is to 2AM (12*10 minutes), we want to do the hourly summary, which have N=6 chuck for 0-143 
          potential bins. 
  Method: use transform + sequence

    from pyspark.sql import functions as F
    
    j1 = """{"name": "abc1", "usage_count": {"bin102": 1, "bin103": 1, "bin104": 1, "bin105": 1, "bin110": 1, "bin112": 
        1, "bin120": 1, "bin121": 1, "bin122": 1, "bin123": 1, "bin124": 1, "bin136": 2, "bin137": 1, "bin138": 1, "bin139":
         1, "bin140": 1, "bin141": 2, "bin142": 2}, "usage_min": {"bin102": 7.7, "bin103": 10, "bin104": 10, "bin105": 2.5, 
        "bin110": 0.1, "bin112": 0.8, "bin120": 6.8, "bin121": 10, "bin122": 10, "bin123": 10, "bin124": 4.3, "bin136": 2.5,
         "bin137": 10, "bin138": 10, "bin139": 10, "bin140": 10, "bin141": 9.3, "bin142": 3.8}, "p_name": "abc_1"}
    """
    
    rdd = sc.parallelize([j1])
    
    df = spark.read.json(rdd, schema="name string, p_name string, usage_count map<string,int>, usage_min map<string,float>")
    +----+------+--------------------+--------------------+
    |name|p_name|         usage_count|           usage_min|
    +----+------+--------------------+--------------------+
    |abc1| abc_1|[bin102 -> 1, bin...|[bin102 -> 7.7, b...|
    +----+------+--------------------+--------------------+
    
    # use map_zip_with to merge usage_count and usage_min into a single map, 
    # use transform_keys to remove `bin` from keys

    df1 = df.withColumn("map1", F.expr("""
        transform_keys(
            map_zip_with(usage_count, usage_min, (k,v1,v2) -> (ifnull(v1,0) AS count, ifnull(v2,0) AS min)),
            (k,v) -> smallint(trim(LEADING 'bin0' FROM k))
        )
    """))

    df1.show(1,80,vertical=True)
    -RECORD 0---------------------------------------------------------------------------------------
     name        | abc1                                                                             
     p_name      | abc_1                                                                            
     usage_count | {bin102 -> 1, bin103 -> 1, bin104 -> 1, bin105 -> 1, bin110 -> 1, bin112 -> 1... 
     usage_min   | {bin102 -> 7.7, bin103 -> 10.0, bin104 -> 10.0, bin105 -> 2.5, bin110 -> 0.1,... 
     map1        | {102 -> {1, 7.7}, 103 -> {1, 10.0}, 104 -> {1, 10.0}, 105 -> {1, 2.5}, 110 ->... 

    # iterating through 0-143 with step=6 and split the bins into 6 elements each 
    # and use aggregate to sum the count/minute
    N = 6
    df2 = df1.selectExpr(
        "name", 
        "p_name",
        f"""
            transform(sequence(0,143,{N}), i ->
                aggregate(
                    sequence(i, i+{N-1}), 
                    (0 AS count, 0D AS min),
                    (acc, x) -> named_struct(
                            'count', acc.count + ifnull(map1[x].count,0),
                            'min', acc.min + ifnull(map1[x].min,0.0)
                        ),
                    acc -> (acc.count AS count, round(acc.min,1) AS min)
                )
            ) AS usage
        """
    )
    
    # format the output count and min
    df_new = df2.selectExpr(
        "name",
        "p_name",
        "array_join(usage.count, ',') AS count", 
        "array_join(usage.min, ',') AS min"
    )
    
    df_new.show(1,0,vertical=True)
    #-RECORD 0-----------------------------------------------------------------------------------------------------
    # name   | abc1                                                                                                
    # p_name | abc_1                                                                                               
    # count  | 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,2,0,5,0,3,7                                                     
    # min    | 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,30.2,0.9,0.0,41.1,0.0,12.5,43.1 
    

(24) use aggregate + map_concat to merge a array of maps into a map
  REF: https://stackoverflow.com/questions/60629812/
  Target: retrieve the first non-null values from a map-value in a list of StructType fields
  (a) use transform to convert an array(map_keys()) into an array of maps
  (b) use aggregate to merge array of maps into a map

    # example dataframe
    df = spark.createDataFrame([
            ({"k0": {"b": 100, "d":"ab" }, "k1": {"a": 1 },"k3": {"c": 12.5, "d":"ef" },"k4": {"d":"hh"}}, )
        ], "col1:map<string,struct<a:int,b:int,c:double,d:string>>"
    )

    df.selectExpr("""
        aggregate(
            transform(map_keys(col1), x -> map(x, coalesce(col1[x].a,col1[x].b,col1[x].c,col1[x].d))), 
            /* zero_value: empty map */
            map(), 
            /* merge: use map_concat */
            (acc,y) -> map_concat(acc, y)
        )  as col1
    """)

  For Spark 3.0+, use transform_values:

    df.selectExpr("transform_values(col1, (k,v) -> coalesce(v.a, v.b, v.c, v.d)) as col1").show(truncate=False)
    +------------------------------------------+
    |col1                                      |
    +------------------------------------------+
    |[k3 -> 12.5, k4 -> hh, k0 -> 100, k1 -> 1]|
    +------------------------------------------+
    #DataFrame[col1: map<string,string>]




