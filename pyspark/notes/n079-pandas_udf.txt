
Pandas UDF is a PySpark DataFrame API function added since Spark 2.3. Unlike the original 
UDF function, the pandas_udf is a vectorized version of UDF which can use some of the Pandas functions.

The drawbacks of the original UDF:
(1) data serialization/deserialization between Python interpretor and JVM
(2) calculation are performed on row-basis which is low efficient

Note: Pandas UDF overcame the 2nd drawback but the data SerDe between Python and JVM
still holds. 

Select functions(if available) in the following order:
(1) Spark SQL builtin functions or Dataframe API functions
(2) Scala-based UDF functions
(3) pyspark.sql.functions.pandas_udf
(4) pyspark.sql.functions.udf

When using udf might be better than using Spark SQL or API functions:
+ if pyspark.sql.functions.explode() involves mass increment on data volume
+ if using udf can avoid shuffling of big volumn of data, i.e. df.join

When to use Pandas UDF:
(1) when the functionality is not available through Spark SQL or DataFrame API functions
(2) only when the function can be vectorized. In another word, if the operation can not be
    vectorized, for example, a stateful list which value relies on the updated values of
    its neighbouring rows. In such case, using the original udf, either backed by
    Scala or Python.
 

Three PandasUDFType (as of Spark 2.4.3):
+ SCALAR:
  + function argument(s): is a series 
  + function returning a scalar
  + where_to_use:
    + pyspark.sql.DataFrame.withColumn()
    + pyspark.sql.DataFrame.select()

+ GROUPED_MAP: 
  + function argument(key, pdf), 
    + key is optional, if exist contains a tuple of all columns in the groupby() function.
    + pdf is the Pandas dataframe containing rows in the current group
  + function returning a dataframe, returnType should be df.schema or any customized StructType()
    Note: the function can return all fields after groupby even though not in groupby and aggregation list
  + where_to_use: 
    + pyspark.sql.GroupedData.apply()
        df.groupby('c1', 'c2').apply(my_pudf_func)
  + examples:
    + Series.interpolate()

+ GROUP_AGG
  + function argument(s1, s2): one or more Series
  + returning a Scalar
  + where to use: 
    + pyspark.sql.GroupedData.agg()
        df.groupby('c1', 'c2').agg(my_pudf_func)
    + pyspark.sql.Window
        df.withColumn('c2', my_udf_func('c1').over(w1))
      Note: for Window function, only support unbounded WindowSpec
  + examples:
    + pyspark.sql.Window:
      Series.is_unique:  is_monotonic_increasing, is_monotonic_decreasing
          udf_p1 = F.pandas_udf(lambda x: x.is_unique, 'boolean', F.PandasUDFType.GROUPED_AGG)
      


Some Examples:

Example-1: pandas.Series.interpolate()

This Pandas function returns Series or DataFrame, so we can only use GROUPED_MAP:

    df = spark.createDataFrame([
          ('A', 't0', None) 
        , ('A', 't1', 1.5) 
        , ('A', 't2', 1.7) 
        , ('B', 't3', 0.5) 
        , ('B', 't4', None) 
        , ('B', 't5', 1.1) 
        , ('C', 't6', 4.3) 
        , ('C', 't7', 3.4) 
        , ('C', 't8', None) 
        , ('C', 't9', 2.7) 
      ], schema='Key:string,time:string,value:double')

    df.show()                                                                                                           
    +---+----+-----+
    |Key|time|value|
    +---+----+-----+
    |  A|  t0| null|
    |  A|  t1|  1.5|
    |  A|  t2|  1.7|
    |  B|  t3|  0.5|
    |  B|  t4| null|
    |  B|  t5|  1.1|
    |  C|  t6|  4.3|
    |  C|  t7|  3.4|
    |  C|  t8| null|
    |  C|  t9|  2.7|
    +---+----+-----+

    # in this function, you can handle any Pandas operations
    def f_interp(pdf): 
        pdf['value'] = pdf.sort_values('time').value.interpolate(method='nearest') 
        return pdf 

    # set up Pandas UDF function:
    udf_interp = F.pandas_udf(f_interp, df.schema, F.PandasUDFType.GROUPED_MAP) 

    df.groupby('Key').apply(udf_interp).show()
    +---+----+-----+                                                                
    |Key|time|value|
    +---+----+-----+
    |  B|  t3|  0.5|
    |  B|  t4|  0.5|
    |  B|  t5|  1.1|
    |  C|  t6|  4.3|
    |  C|  t7|  3.4|
    |  C|  t8|  3.4|
    |  C|  t9|  2.7|
    |  A|  t0| null|
    |  A|  t1|  1.5|
    |  A|  t2|  1.7|
    +---+----+-----+


Example-2: calculate the cosine similarity:
REF: https://stackoverflow.com/questions/58204068/pyspark-cosinesimilarity-over-sataframe

    import numpy as np
    import pandas as pd
    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType

    df = spark.createDataFrame([
          (1, 2, 0.9, 0.1, 0.1)
        , (1, 3, 0.3, 0.4, 0.9)
        , (1, 4, 0.2, 0.9, 0.15)
        , (2, 1, 0.8, 0.8, 1.0)
      ]  , ['Customer1', 'Customer2', 'v_cust1', 'v_cust2', 'cosine_sim'])


    #Target-1: do the groupby aggregate
    @pandas_udf('Customer1:long,cs:double', PandasUDFType.GROUPED_MAP) 
    def cosine_similarity_groupby(key, pdf): 
        cs = np.dot(pdf.v_cust1, pdf.v_cust2) / (np.linalg.norm(pdf.v_cust1) * np.linalg.norm(pdf.v_cust2)) 
        return pd.DataFrame([key + (cs,)]) 

    df.groupby('Customer1').apply(cosine_similarity_groupby).show()
    +---------+------------------+                                                  
    |Customer1|                cs|
    +---------+------------------+
    |        1|0.4063381906012777|
    |        2|               1.0|
    +---------+------------------+

    #Target-2: do the transform()

    schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')

    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)
    def cosine_similarity_transform(pdf):
        pdf['cs'] = np.dot(pdf.v_cust1, pdf.v_cust2) / (np.linalg.norm(pdf.v_cust1) * np.linalg.norm(pdf.v_cust2))
        return pdf

    df.groupby('Customer1').apply(cosine_similarity_transform).show()
    +---------+---------+-------+-------+----------+------------------+             
    |Customer1|Customer2|v_cust1|v_cust2|cosine_sim|                cs|
    +---------+---------+-------+-------+----------+------------------+
    |        1|        2|    0.9|    0.1|       0.1|0.4063381906012777|
    |        1|        3|    0.3|    0.4|       0.9|0.4063381906012777|
    |        1|        4|    0.2|    0.9|      0.15|0.4063381906012777|
    |        2|        1|    0.8|    0.8|       1.0|               1.0|
    +---------+---------+-------+-------+----------+------------------+


Notes: 
 (1) When return type is `PandasUDFType.GROUPED_MAP`
     + the function can take two arguments
       + key: which is a tuple save all fields in groupby() function
       + pdf: which is the subset of the dataframe containing only grouped Rows
     + return value can be a DataFrame with one row(aggregate) or all rows in the current grouped(transform)
       when return one Row dataframe: use the `+` to merge two tuples.

         pd.DataFrame([key + (pdf.v.mean(),)])

 (2) When doing transform with returntype='PandasUDFType.GROUPED_MAP', we can use the existing df.schema,
     but must initialize a new instance instead so that the existing df.schema won't be changed (or use deep-copy)

          # the following two lines is not OK, the existing df.schema will be changed
          schema = df.schema
          schema.add('cs', 'double')

          # the following line is fine, old schema is kept as-is
          schema = StructType.fromJson(df.schema.jsonValue()).add('cs', 'double')

Troubleshooting:
  (1) Error: Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
      Ans: the return value of udf function, if it's a np.dtype, it should be forced into Python types, for example:
           return float(np.log(x))
           return list(np.array(...))

 
