Row-based Calculations:
---
(1) rank/dense_rank/row_number (see Example-1,2)
(2) fillna: nearest, ffill, bfill (see Example-3)
(3) explode into multiple rows (see example-4)

---
Example-1: find rank, dense_rank and row_number on a list of columns of the same Row:
  REF: https://stackoverflow.com/questions/57800266/pyspark-how-to-find-and-convert-top-5-row-values-to-1-and-rest-all-to-0
  Task: select the top 5 values in each row and mark them as '1' and the rest as '0'
  Method: this is a transform based on the ranking, similar to the function rank() but is implemented in row-wise.
  There are 3 different ways to calculate top-N based on how the duplicated values are treated.
   (1) rank(): duplicates will be set as the same rank-number, some rank-number might be missing due to duplicates
   (2) dense_rank(): duplicates will be set as the same rank-number, and there is no gap between rank-numbers
   (3) row_number(): duplicates will be set different rank-numbers, the order might varies based on how data are feeded

  Data Setup:

    df = spark.createDataFrame([
           (0.74, 0.9, 0.52, 0.85, 0.18, 0.23, 0.3, 0.0, 0.1, 0.07)
         , (0.11, 0.57, 0.81, 0.81, 0.45, 0.48, 0.86, 0.38, 0.41, 0.45)
         , (0.03, 0.84, 0.17, 0.96, 0.09, 0.73, 0.25, 0.05, 0.57, 0.66)
         , (0.8, 0.94, 0.06, 0.44, 0.2, 0.89, 0.9, 1.0, 0.48, 0.14)
         , (0.73, 0.86, 0.68, 1.0, 0.78, 0.17, 0.11, 0.19, 0.18, 0.83)
        ], ['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']
    )

    """ set up columns that matter """
    cols = df.columns

    """ select Top-N in row-wise """
    N = 5


  Task-1: to calculate top-N based on Rank()

    """ 
     1. create an array columns, use sort_array()
     2. use sort_array() to sort the array and then retrieve the (N+1)th element to a new column `Nth`
     3. set cond = col > Nth, and use Spark SQL IF(cond, 1, 0) to set up the new value
    """
    df.withColumn('arr', F.array(cols))                \       
      .withColumn('Nth', F.sort_array('arr', False)[N]) \
      .selectExpr(*[ 'IF(`{0}` > Nth, 1, 0) as `{0}`'.format(cols[i]) for i in range(len(cols)) ]) \
      .show()

    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+

    # Using spark 2.4+

    """create an array using F.array(), then use F.sort_array() to sort the array values in decending values
       and retrive the 5th element to a field 'n5'. after that use transform() to convert arr values into 0/1 
       based on IF(x >= n5,1,0), select and dereference the array into columns
    """
    df.withColumn('arr', F.array(cols)) \
      .withColumn('Nth', F.sort_array('arr', False)[N]) \
      .selectExpr('transform(arr, x -> IF(x>=Nth,1,0)) as new_arr')  \
      .select([F.col('new_arr')[i].alias(cols[i]) for i in range(len(cols))]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  2|  3|  4|  5|  6|  7|  8|  9| 10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+


  Task-2: to calculate the top-N based on Row_number()

    """ Below code requires spark 2.4.0+
    given an array of all cols, need to find idx of top-N values, this can be achieved
    by create an named_struct(val, idx), sort by `val` and take the topN slice() of array 
    containing the idx only. transform from struct(val, idx) -> idx. Then we use array_contain() 
    to set up the IF condition. 
    """
    df.withColumn('arr', F.array(cols))  \
      .withColumn('topN_idx', F.expr("""
            transform(
                slice(sort_array(
                    transform(sequence(0,size(arr)-1), i -> named_struct('val',arr[i], 'idx', i))
                  , False
                )
              , 1
              , {0}
            ), x -> x.idx)
          """.format(N)
        )) \
      .select([F.when(F.array_contains('topN_idx',i),1).otherwise(0).alias(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+


  Task-3: to calculate the top-N based on dense_rank()

    """ Below code is based on Spark 2.4.0+
    similar to the calculation based on rank(), the only difference is we
    retrieve `Nth` value from the array_distict() before sort_array() step 
    """
    df.withColumn('arr', F.array(cols)) \
      .withColumn('topN', F.sort_array(F.array_distinct('arr'), False)[N]) \
      .selectExpr(*[ 'IF(`{0}` > topN, 1, 0) as `{0}`'.format(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  1|  1|  1|  0|  0|  1|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+



Example-2: take row-wise ranking:
  REF: https://stackoverflow.com/questions/58065950/ranking-columns-in-pyspark-dataframe

    df = spark.createDataFrame([
             (99, 30, 21) 
            , (20, 10, 44) 
            , (50, 90, 87) 
            , (78, 11, 9) 
          ], ('A', 'B', 'C')
    )
    
    cols = df.columns
    
    # rank, row_number does not make much sense
    df.withColumn('arr', F.array_sort(F.array(cols))).selectExpr('*', '''
        filter(sequence(1,size(arr)), i -> arr[i-1] == A)[0] AS rank_of_A
    ''').show()
    +---+---+---+------------+---------+
    |  A|  B|  C|         arr|rank_of_A|
    +---+---+---+------------+---------+
    | 99| 30| 21|[21, 30, 99]|        3|
    | 20| 10| 44|[10, 20, 44]|        2|
    | 50| 90| 87|[50, 87, 90]|        1|
    | 78| 11|  9| [9, 11, 78]|        3|
    +---+---+---+------------+---------+

    # dense_rank: add array_distinct(), below pesudo-code

        arr = array_sort(array_distinct(array(cols))))


Example-3: fillna using the nearest column on row-wise
  REF: https://stackoverflow.com/q/64809721/9510729

  Task-1: nearest-fill (use pandas_udf and pd.Series.interpolate)

    import pandas as pd
    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        (1, 100, 100, None, None, None, None, 200, 200),
        (2, 180, 180, None, None, 300, 300, 300, 300),
        (3, 400, 400, 400, 400, None, 200, 200, 200),
        (4, 700, 700, None, None, None, 200, 200, 200)
    ], ["id", "col1", "col2", "col3", "col4", "col5", "col6", "col5", "col6"])

    cols = df.columns[1:]

    # define UDF to fillna to the nearest point using pd.Series.interpolate(method='nearest')
    # fillnull = F.udf(lambda x: pd.Series(x).interpolate(method='nearest', downcast='infer').tolist(),'array<int>')

    # or pandas_udf and pd.Series.interpolate
    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html
    # there are more choices under this method: zero, slinear, quadratic, cubic, spline, polynomial etc.
    from typing import Iterator
    @F.pandas_udf("array<int>")
    def pd_fillna(it: Iterator[pd.Series]) -> Iterator[pd.Series]:
      for s in it:
        yield pd.Series([ pd.Series(x).interpolate(method='nearest', downcast='infer').tolist() for x in s ])

    df.select("*", pd_fillna(F.array(cols)).alias("arr")).show(10,0)
    +---+----+----+----+----+----+----+----+----+----------------------------------------+
    |id |col1|col2|col3|col4|col5|col6|col7|col8|arr                                     |
    +---+----+----+----+----+----+----+----+----+----------------------------------------+
    |1  |100 |100 |null|null|null|null|200 |200 |[100, 100, 100, 100, 200, 200, 200, 200]|
    |2  |180 |180 |null|null|300 |300 |300 |300 |[180, 180, 180, 300, 300, 300, 300, 300]|
    |3  |400 |400 |400 |400 |null|200 |200 |200 |[400, 400, 400, 400, 400, 200, 200, 200]|
    |4  |700 |700 |null|null|null|200 |200 |200 |[700, 700, 700, 700, 200, 200, 200, 200]|
    +---+----+----+----+----+----+----+----+----+----------------------------------------+

    df_new = df.select("id", pd_fillna(F.array(cols)).alias("arr")) \
        .select("id", *[ F.col("arr")[i] for i in range(len(cols)) ] ) \
        .toDF(*df.columns)

    df_new.show()
    +---+----+----+----+----+----+----+----+----+                                   
    | id|col1|col2|col3|col4|col5|col6|col7|col8|
    +---+----+----+----+----+----+----+----+----+
    |  1| 100| 100| 100| 100| 200| 200| 200| 200|
    |  2| 180| 180| 180| 300| 300| 300| 300| 300|
    |  3| 400| 400| 400| 400| 400| 200| 200| 200|
    |  4| 700| 700| 700| 700| 200| 200| 200| 200|
    +---+----+----+----+----+----+----+----+----+

  Task-2: forward-fill (use aggregate function)

    df.select("id", F.array(cols).alias("arr")) \
        .selectExpr("*", """
          aggregate(
            slice(arr,2,size(arr)),
            array(arr[0]), 
            (acc,x) -> concat(acc,array(ifnull(x,element_at(acc,-1))))
          ) as new_arr
          """).show(10,0)
    +---+------------------------------------+----------------------------------------+
    |id |arr                                 |new_arr                                 |
    +---+------------------------------------+----------------------------------------+
    |1  |[100, 100,,,,, 200, 200]            |[100, 100, 100, 100, 100, 100, 200, 200]|
    |2  |[180, 180,,, 300, 300, 300, 300]    |[180, 180, 180, 180, 300, 300, 300, 300]|
    |3  |[400, 400, 400, 400,, 200, 200, 200]|[400, 400, 400, 400, 400, 200, 200, 200]|
    |4  |[700, 700,,,, 200, 200, 200]        |[700, 700, 700, 700, 700, 200, 200, 200]|
    +---+------------------------------------+----------------------------------------+

  Task-3: backward-fill (use aggregate + sequence + reverse functions)

    df.select("id", F.array(cols).alias("arr")) \
        .selectExpr("*", """
           reverse(aggregate(
             sequence(size(arr)-2,0,-1),
             array(element_at(arr,-1)), 
             (acc,i) -> concat(acc,array(ifnull(arr[i],element_at(acc,-1))))
           )) as new_arr 
    """).show(10,0)
    +---+------------------------------------+----------------------------------------+
    |id |arr                                 |new_arr                                 |
    +---+------------------------------------+----------------------------------------+
    |1  |[100, 100,,,,, 200, 200]            |[100, 100, 200, 200, 200, 200, 200, 200]|
    |2  |[180, 180,,, 300, 300, 300, 300]    |[180, 180, 300, 300, 300, 300, 300, 300]|
    |3  |[400, 400, 400, 400,, 200, 200, 200]|[400, 400, 400, 400, 200, 200, 200, 200]|
    |4  |[700, 700,,,, 200, 200, 200]        |[700, 700, 200, 200, 200, 200, 200, 200]|
    +---+------------------------------------+----------------------------------------+


Example-4: split row into multiple rows based on change in consecutive values, fill other columns with NULL
  REF: https://stackoverflow.com/q/64860731/9510729
  Method:
   (1) create an array column `dta`
   (2) use aggregate function, set the accumulator as an array of structs with two fields: val and label.
       in `merge` function, update `label` to identify same group with consecutive values and 
       in `finish` function, transform array of structs into array of arrays of val(s)
   (3) exaplde the above array of arrays and then select them as different columns.
  Code:

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        (1, 100, 100, 100, 500, 500, 500, 200, 200, 200), 
        (2, 100, 100, 700, 700, 700, 100, 100, 100, 100)
    ], ['id', 't.n1', 't.n2', 't.n3', 't.n4', 't.n5', 't.n6', 't.n7', 't.n8', 't.n9'])

    cols = df.columns[1:]

    df1 = df.select("id", F.array(*[f'`{c}`' for c in cols]).alias("dta")) \
        .selectExpr("id", """
          explode(
            aggregate(
              /* expr: array dta from 2nd to the last element 
               * notice: 
               *  (1) the indices for slice function is 1-based while dta[i] is 0-based
               *  (2) use `size(dta)` instead of `size(dta)-1` to avoid negative indices ERROR when dta is EMPTY array
               */
              slice(dta,2,size(dta)),
              /* start: set zero value to accumulator acc as array of structs */
              CAST(array((dta[0],1)) as array<struct<val:int,label:int>>),
              /* merge: update acc and set up `label` based on x and element_at(acc,-1).val */
              (acc, x) -> concat(acc, array(named_struct(
                  'val', x,
                  'label', element_at(acc,-1).label + IF(x = element_at(acc,-1).val,0,1)
                ))),
              /* finish: split array into array of arrays by iterating through distinct acc.label array */
              acc -> transform(sequence(1,array_max(acc.label)), i -> transform(acc, x -> IF(x.label = i, x.val, NULL)))
            )
          ) as dta
        """)
    df1.show(truncate=False)
    +---+--------------------------+
    |id |dta                       |
    +---+--------------------------+
    |1  |[100, 100, 100,,,,,,]     |
    |1  |[,,, 500, 500, 500,,,]    |
    |1  |[,,,,,, 200, 200, 200]    |
    |2  |[100, 100,,,,,,,]         |
    |2  |[,, 700, 700, 700,,,,]    |
    |2  |[,,,,, 100, 100, 100, 100]|
    +---+--------------------------+

    df1.select("id", *[ F.col("dta")[i] for i in range(len(cols))]).show()
    +---+------+------+------+------+------+------+------+------+------+
    | id|dta[0]|dta[1]|dta[2]|dta[3]|dta[4]|dta[5]|dta[6]|dta[7]|dta[8]|
    +---+------+------+------+------+------+------+------+------+------+
    |  1|   100|   100|   100|  null|  null|  null|  null|  null|  null|
    |  1|  null|  null|  null|   500|   500|   500|  null|  null|  null|
    |  1|  null|  null|  null|  null|  null|  null|   200|   200|   200|
    |  2|   100|   100|  null|  null|  null|  null|  null|  null|  null|
    |  2|  null|  null|   700|   700|   700|  null|  null|  null|  null|
    |  2|  null|  null|  null|  null|  null|   100|   100|   100|   100|
    +---+------+------+------+------+------+------+------+------+------+


Example-5: slide/split rows into N-columns
  REF: https://stackoverflow.com/q/64860731/9510729
  Method: use array of array or array of structs and then explode:

    from pyspark.sql.functions import struct, explode, array, col

    df = spark.createDataFrame([
        ('a', '150', '110', '130', '80', '136', '150', '190', '110', '150', '110', '130', '136', '100', '150', '190', '110'),
        ('b', '100', '100', '130', '100', '136', '100', '160', '230', '122', '130', '15', '200', '100', '100', '136', '100'),
    ], ['id', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16'])

    # all columns except the first
    cols = df.columns[1:]

    # size of the splits
    N = 6

  Method-1: use array of arrays:

    df_new = df.withColumn('dta', explode(array(*[ array(*cols[i:i+N]) for i in range(len(cols)-N+1) ]))) \
        .select('id', *[ col('dta')[i].alias(str(i+1)) for i in range(N) ])
    df_new.show()
    +---+---+---+---+---+---+---+
    | id|  1|  2|  3|  4|  5|  6|
    +---+---+---+---+---+---+---+
    |  a|150|110|130| 80|136|150|
    |  a|110|130| 80|136|150|190|
    |  a|130| 80|136|150|190|110|
    |  a| 80|136|150|190|110|150|
    |  a|136|150|190|110|150|110|
    |  a|150|190|110|150|110|130|
    |  a|190|110|150|110|130|136|
    |  a|110|150|110|130|136|100|
    |  a|150|110|130|136|100|150|
    |  a|110|130|136|100|150|190|
    |  a|130|136|100|150|190|110|
    |  b|100|100|130|100|136|100|
    +---+---+---+---+---+---+---+

  Method-2: use array of struct:

    df_new = df.withColumn('dta', array(*cols)) \
        .selectExpr("id", f"""
            inline(transform(sequence(0,{len(cols)-N}), i -> ({','.join(f'dta[i+{j}] as `{j+1}`' for j in range(N))})))
          """)



