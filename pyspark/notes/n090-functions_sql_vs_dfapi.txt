Compare functions between DataFrame API and SparkSQL (As of Spark 2.4.0)

Functions in DataFrame API, but not in SparkSQL:
---
 + udf, pandas_udf
 + broadcast, lit, expr, col, column


Functions in DataFrame API, can be handled in SparkSQL
 + asc_nulls_first, asc_nulls_last, desc_nulls_first, desc_nulls_last
   Spark SQL does not support `Order BY col ASC null first` etc
   adding a reference column IF(col is null,1,0) in order By clause of Spark SQL
   Spark SQL default: null is the smallest in ordering

    spark.sql("""

        SELECT c1 FROM VALUES (1), (NULL), (12), (3) AS (c1)
        ORDER BY IF(isnull(c1),1,0), c1 ASC

    """).show()

    Ref: SparkSQL Select: https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#


Functions only in Spark SQL, not in DataFrame API
---
 + higher-order functions: transform, exists, filter, aggregate, zip_with
 + XML related: xpath, xpath_boolean, xpath_double, xpath_float, xpath_int
              , xpath_long, xpath_number, xpath_short, xpath_string
 + parse_url: similar to XML pather, this can retrieve HOST, QUERY etc from a valid URL
 + inline, inline_outer: similar to explode, explode_outer (array of structs)
   (explode + dereference) should be used with select/selectExpr
   withColumn will not working when multiple fields exist in structs
 + named_struct: like struct, map, array, very useful
 + str_to_map: like split, but create a MapType() from string
 + percentile: can be used to calculate exact median
 + stack: can be used to demornalize a df
 + find_in_set: 
 + sentences: split paragraph into array of arrays (sentences of words)
 + elt: similar to choose() in Pentaho
 + java_method, reflect: only applied to static method from Java/Scala [1]
 + functions from Apache Hive:
   + all functions mentioned above except the higher-order functions
   + pmod, e, negative, positive, space, assert_true
   + current_database
   + char_length, character_length, octet_length
 + not from Apache Hive: 
   + bit_length
   + count_min_sketch
   + uuid
   + input_file_block_start, input_file_block_length


Functions only in SparkSQL which can be simulated with DF API:
---
 + if,ifnull,nvl,nvl2,nullif ->  df:{when,otherwise}
 + left, right -> df:substring  left(A,2) -> substring('A',1,2)  right(A,2) -> substring('A',-2,2)
 + position -> df:{instr,locate}
 + printf -> df:format_string
 + replace -> df:regexp_replace, df.replace
 + rollup -> df.rollup
 + like -> df.rlike
 + isnotnull -> df.col.isNotNull()
 + cube -> df.cube
 + cardinality -> df:approx_count_distinct
 + char,chr -> df:hex+unhex+astype, example
     spark.range(1).select(F.unhex(F.hex(F.lit(65))).astype('string')) == spark.sql('select chr(65)')
 + cot -> df:1/tan
 + pi -> df:2*asin(1)


Duplicated functions only in SparkSQL to compatible with Standard SQL:
---
 + to_unix_timestamp -> {df,sql}:unix_timestamp 
 + now -> {df,sql}:current_timestamp
 + weekday -> {df,sql}:dayofweek
 + day -> {df,sql}:dayofmonth
 + std -> {df,sql}:stdev
 + sign -> {df,sql}:signum
 + ln -> {df,sql}:log (with one argument)
 + power -> {df,sql}:pow
 + mod -> {df,sql}:% 


Exist in both but using different names:
---
 + df:create_map -> sql:map
 + df:bitwiseNOT -> sql:~
 + df:countDistince -> sql:count(distinct ..)
 + df:sumDistinct  -> sql:sum(distinct ..)
 + df.approxQuantile(col, probabilities, relativeError) -> sql:{approx_percentile,percentile_approx}
 + df:astype -> sql:{cast,int,bigint,boolean,decimal,double,date,float,string,timestamp,tinyint,smallint,binary}


Exist in both but work differently
---
 + regexp_replace(), regexp_extract():
   DF version support PCRE regex, can use \1, \2 in pattern and $1, $2 in replacement
   Spark SQL version does not support PCRE
 + `column` as function arguments: 
   many of the DF-API functions only support scalar argument, i.e. date_add(d, N)
   the 2nd argument must be an integer, in Spark SQL, this can be a column. This makes
   SparkSQL more powerful.
 + sequence() in DF-API support only integers, while date, timestamp and byte 
   are supported with Spark SQL (extra power)


Some notes:
---
  (1) whenever using column as function arguments is disabled which trigger an Error:
      `#'Column' object is not callable`, try using its SparkSQL corresponding builtin 
      functions. for example, using a column name for array index will work in Spark SQL
      , but not for API functions

  (2) in DF-API, logic '|', '&', but in SQL, need 'or', 'and'


References:
[1] Java_Method: https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-CallMethodViaReflection.html
[2] Apache_Hive_functions: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
