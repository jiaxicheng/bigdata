Compare functions between DataFrame API and SparkSQL (As of Spark 2.4.0)

Functions in DataFrame API, but not in SparkSQL:
---
 + udf, pandas_udf
 + broadcast, lit, expr, col, column
 + asc, asc_nulls_first, asc_nulls_last, desc, desc_nulls_first, desc_nulls_last


Functions only in Spark SQL, not in DataFrame API
---
 + higher-order functions: transform, exists, filter, aggregate, zip_with
 + XML related: xpath, xpath_boolean, xpath_double, xpath_float, xpath_int
              , xpath_long, xpath_number, xpath_short, xpath_string
 + inline, inline_outer: similar to explode, explode_outer (array of structs)
   (explode + dereference) should be used with select/selectExpr
   withColumn will not working when multiple fields exist in structs
 + named_struct: like struct, map, array, very useful
 + str_to_map: like split, but create a MapType() from string
 + stack: can be used to demornalize a df
 + find_in_set: 
 + sentences: split paragraph into array of arrays (sentences of words)
 + percentile: can be used to calculate median
 + elt: similar to choose() in Pentaho
 + other less used:
   + nvl, nvl2, nullif, not, isnotnull
   + cube, cot, e, pi, ln, std, power, sign, mod, pmod, negative, positive, count_min_sketch
   + day, weekday, now, to_unix_timestamp
   + space, chr, binary, char, bit_length, char_length, character_length, octet_length
   + parse_url
   + current_database, input_file_block_length, input_file_block_start
   + java_method, cardinality, assert_true, rollup, reflect, uuid


Functions only in SparkSQL which can be simulated with DF API:
---
 + if,ifnull ->  df:{when,otherwise}
 + left, right -> sql:substring  left(A,2) -> substring('A',1,2)  right(A,2) -> substring('A',-2,2)
 + position -> sql:{instr,locate}
 + replace -> sql:regexp_replace
 + printf -> sql:format_string
 + like -> df.rlike


Exist in both but using different names:
---
 + df:create_map -> sql:map
 + df:bitwiseNOT -> sql:~
 + df:countDistince -> sql:count(distinct ..)
 + df:sumDistinct  -> sql:sum(distinct ..)
 + df.approxQuantile(col, probabilities, relativeError) -> sql:{approx_percentile,percentile_approx}
 + df:astype -> sql:{cast,int,bigint,boolean,decimal,double,date,float,string,timestamp,tinyint,smallint}


Exist in both but work differently
---
 + regexp_replace(), regexp_extract():
   DF version support PCRE regex, can use \1, \2 in pattern and $1, $2 in replacement
   Spark SQL version does not support PCRE
 + `column` as function arguments: 
   many of the DF-API functions only support scalar argument, i.e. date_add(d, N)
   the 2nd argument must be an integer, in Spark SQL, this can be a column. This makes
   SparkSQL more powerful.


Some notes:
---
  (1) whenever using column as function arguments is disabled which trigger an Error:
      `#'Column' object is not callable`, try using its SparkSQL corresponding builtin 
      functions. for example, using a column name for array index will work in Spark SQL
      , but not for API functions

  (2) in DF-API, logic '|', '&', but in SQL, need 'or', 'and'

