Compare functions between DataFrame API and SparkSQL (As of Spark 2.4.0)

Functions in DataFrame API, but not in SparkSQL:
---
 + udf, pandas_udf
 + broadcast, lit, expr, col, column


Functions in DataFrame API, can be handled in SparkSQL
 + asc_nulls_first, asc_nulls_last, desc_nulls_first, desc_nulls_last
   Spark SQL does not support `Order BY col ASC null first` etc
   adding a reference column IF(col is null,1,0) in order By clause of Spark SQL
   Spark SQL default: null is the smallest in ordering

    spark.sql("""

        SELECT c1 FROM VALUES (1), (NULL), (12), (3) AS (c1)
        ORDER BY IF(isnull(c1),1,0), c1 ASC

    """).show()

    Ref: SparkSQL Select: https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#


Functions only in Spark SQL, not in DataFrame API
---
 + higher-order functions: transform, exists, filter, aggregate, zip_with
 + XML related: xpath, xpath_boolean, xpath_double, xpath_float, xpath_int
              , xpath_long, xpath_number, xpath_short, xpath_string
        example: https://github.com/jiaxicheng/stackoverflow/blob/master/pyspark/068-pyspark-sql-xml_xpath_functions.txt
 + parse_url: similar to XML pather, this can retrieve HOST, QUERY etc from a valid URL
 + inline, inline_outer: similar to explode, explode_outer (array of structs)
   (explode + dereference) should be used with select/selectExpr
   withColumn will not working when multiple fields exist in structs
 + named_struct: like struct, map, array, very useful
 + str_to_map: like split, but create a MapType() from string
        example: https://github.com/jiaxicheng/bigdata/blob/master/pyspark/notes/n095-pyspark-extractall.txt
 + percentile: can be used to calculate exact median
 + stack: can be used to demornalize a df
 + find_in_set: 
 + sentences: split paragraph into array of arrays (sentences of words)
     example: https://stackoverflow.com/questions/58330928#58332005
 + elt: similar to choose() in Pentaho
 + java_method, reflect: only applied to static method from Java/Scala [1]
 + functions from Apache Hive:
   + all functions mentioned above except the higher-order functions
   + pmod, e, pi, negative, positive, space, assert_true
   + current_database
   + char_length, character_length, octet_length
 + not from Apache Hive: 
   + bit_length
   + count_min_sketch
   + uuid
   + input_file_block_start, input_file_block_length

 Notes:
  (1) parse_url(string url, string partToExtract[, string keyToExtract])
      Returns the specified part from the URL. Valid values for partToExtract include:
      `HOST`, `PATH`, `QUERY`, `REF`, `PROTOCOL`, `AUTHORITY`, `FILE`, and `USERINFO`
      examples: 

        parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') returns 'facebook.com'. 
        parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') returns 'v1'.

  (2) sentences can take optionally 2nd and 3rd argument for locale:

        sentences(string str, string lang, string locale)  return array<array<string>>



Functions only in SparkSQL which can be simulated with DF API:
---
 + if,nvl2,nullif ->  df:{when,otherwise}
 + nvl,ifnull  -> df:coalesce  coalesce can have more then 2 arguments, 
     notice also that nanvl() is used only on FloatType or DoubleType 
 + left, right -> df:substring  left(A,2) -> substring('A',1,2)  right(A,2) -> substring('A',-2,2)
 + position -> df:{instr,locate}
 + printf -> df:format_string
 + replace -> df:regexp_replace, df.replace
 + rollup -> df.rollup
 + like -> df.rlike
 + isnotnull -> df.col.isNotNull()
 + cube -> df.cube
 + cardinality -> df:approx_count_distinct
 + char,chr -> df:hex+unhex+astype, example
     spark.range(1).select(F.unhex(F.hex(F.lit(65))).astype('string')) == spark.sql('select chr(65)')
 + cot -> df:1/tan
 + pi -> df:2*asin(1)
 + e -> df:exp(1)


Duplicated functions only in SparkSQL to compatible with Standard SQL:
---
 + to_unix_timestamp -> {df,sql}:unix_timestamp 
 + now -> {df,sql}:current_timestamp
 + weekday -> {df,sql}:dayofweek
 + day -> {df,sql}:dayofmonth
 + std -> {df,sql}:stdev
 + sign -> {df,sql}:signum
 + ln -> {df,sql}:log (with one argument)
 + power -> {df,sql}:pow
 + mod -> {df,sql}:% 


Exist in both but using different names:
---
 + df:create_map -> sql:map
 + df:bitwiseNOT -> sql:~
 + df:countDistince -> sql:count(distinct ..)
 + df:sumDistinct  -> sql:sum(distinct ..)
 + df.approxQuantile(col, probabilities, relativeError) -> sql:{approx_percentile,percentile_approx}
 + df:astype -> sql:{cast,int,bigint,boolean,decimal,double,date,float,string,timestamp,tinyint,smallint,binary}


Exist in both but work differently
---
 + regexp_replace(), regexp_extract():
   Notes: 
    (1) DF version support Java-based PCRE regex, can use \1, \2 in pattern and $1, $2 in replacement
     Spark SQL version does not support PCRE
    (2) Python re module use in udf function has some different features than Java-based regex with regexp_* function
      + Example-1: non-fix-sized lookbdhind (?<![a-z]{0,5}) is supported in `regexp_replace()`, but not in `re.sub()`
      + Example-2: Java-based regex use `+` to set non-backtrack, so that `(?:Col1)?+.+`, `?+` matches 0 or 1 times 
                   without backtracking. Python-based regex does not support this
    (3) in DF, do not pre-complie regex using re.compile(), this yields the following error:
         AttributeError: 're.Pattern' object has no attribute '_get_object_id'
 + `column` as function arguments: 
   many of the DF-API functions only support scalar argument, i.e. date_add(d, N)
   the 2nd argument must be an integer, in Spark SQL, this can be a column. This makes
   SparkSQL more powerful.
 + sequence() in DF-API support only integers, while date, timestamp and byte 
   are supported with Spark SQL (extra power)


Some notes:
---
  (1) whenever using column as function arguments is disabled which trigger an Error:
      `#'Column' object is not callable`, try using its SparkSQL corresponding builtin 
      functions. for example, using a column name for array index will work in Spark SQL
      , but not for API functions
  (2) in DF-API, logic '|', '&', but in SQL, need 'or', 'and'
  (3) Spark SQL RLIKE is case-sensitive, the following regex, will match only 'Z' without using `(?i)`
  
        filter(text, x -> x rlike "^(?i)Z?[0-9]{4,6}$")


Some useful functions in Apche Hive but missing in Spark SQL:
---
 + statistics:
   +ngram: ngrams(array<array<string>>, int N, int K, int pf), returns `array<struct<string,double>>`
       Returns the top-k N-grams from a set of tokenized sentences, such as those returned by the sentences() 
   + context_ngrams(): 2nd argument is an array identify context
   + histogram_numeric(col, b): Computes a histogram of a numeric column in the group using b 
       non-uniformly spaced bins. The output is an array of size b of double-valued (x,y) coordinates that 
       represent the bin centers and heights: array<struct<{x,y}>>
   Some examples: https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining
 + data masking functions: mask(), mask_first_n(), mask_last_n(), mask_hash() etc.
 + parse_url_tuple: similar to parse_url, but can take multiple keys. this is semilar to json_tuple to get_json_object
 + create_union(): Spark does not support union data type
 + surrogate_key(): used in ACID table insertion. Spark is not designed for ACID


References:
[1] Java_Method: https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-CallMethodViaReflection.html
[2] Apache_Hive_functions: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
