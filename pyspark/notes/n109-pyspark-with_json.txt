Working with JSON data and PySpark:



+ spark.read.json(path)
---
  + Options:
    + path: can be a list of path, or RDD of strings storing JSON objects
    + schema: simpleString is fine or using DDL
  * + allowSingleQuotes: default true
  * + allowUnquotedFieldNames: default false
    + allowUnquotedControlChars: 
    + allowNumericLeadingZero: default false
    + allowBackslashEscapingAnyCharacter: default false
    + allowComments: default false
  * + dateFormat: default yyyy-MM-dd
  * + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
  * + multiLine: default false
    + mode: PERMISSIVE, DROPMALFORMED, FAILFAST
    + encoding: 
  * + lineSep: default '\r?\n'
  * + samplingRatio: sample for inferSchema, default 1.0, go through file once for all lines
    + prefersDecimal: 
    + primitivesAsString: default false, if true all input columns are StringType
    + dropFieldIfAllNull: default false
    + columnNameOfCorruptRecord: 

  Notes:
   (1) If multiLine=True, then the input file should be wrapped in an array in a valid JSON format:

           [ {
              /* key1 content */
             },
             { /* key2 contents */ }
           ]

       On the other hand, if multiLine=False, each line will be treated as a dataframe Row, only the first 
       dict will be processed (below only `val1` and `val3` are loaded into dataframe)
 
           {'key': 'val1'}{'key': 'val2'}
           {'key': 'val3'}{'key': 'val4'}{'key': 'val8'}{'key': 'val5'}

   (2) JSON's NULL value is `null`, case-sensitive and no-quoting
   (3) no comment /* */ or // is allowed, JSON does not support inline comments, add a field for comments



+ spark.write.json(path)
---
  + Options:
    + path: 
    + mode: append, overwrite, ignore, error(default)
    + compression: none, bzip2, gzip, lz4, snappy and deflate
    + dateFormat:
    + timestampFormat:
    + encoding:
    + lineSep: 



+ pyspark.sql.functions:
---
  + pyspark.sql.functions.get_json_object(col, path)
    + When: if there is only 1-2 keys you want to retrive from a bulk JSON string, then

  + pyspark.sql.functions.from_json(col, schema, options={})
    + When if you can set up the schema, missing keys are fine, they will be filled with null

  + pyspark.sql.json_tuple(col, *fields)
    + It takes a set of names (keys) and a JSON string, and returns a tuple of values using one function. 
      This is much more efficient than calling GET_JSON_OBJECT to retrieve more than one key from a single 
      JSON string. In any case where a single JSON string would be parsed more than once, your query will 
      be more efficient if you parse it once, which is what JSON_TUPLE is for. 

      As JSON_TUPLE is a UDTF, you will need to use the LATERAL VIEW syntax in order to achieve the same goal.

      Notes: this does NOT work on a JSON array

  + pyspark.sql.functions.schema_of_json(json)
    + generate the schema of the corresponding json string in the DDL format (`simpleString`)

  + pyspark.sql.functions.to_json(col, options={})
    + convert a column containing StructType, ArrayType or MapType into JSON string

  Notes:
   (1) get_json_object and json_tuple require `col` having VALID JSON string, invalid JSON like
       single-quoted fields/values will get failed with these functions
       from_json is more robust and can take single-quotes by default and the options argument 
       make it flexible and support all options the same as spark.read.json()
   (2) json_tuple, get_json_object and from_json can all check fields which do not exist in the
       JSON string and will return NULL. For example, a JSON dictionary, if you have a list of 
       all fields(example: https://stackoverflow.com/questions/58769232)

       * json_tuple:

             df.select(json_tuple('json_col', *fields_list).alias(*fields_list))

       * get_json_object:

             df.select([ get_json_object('json_col', f'$.{p}').alias(p) for p in fields_list]) 

       * from_json:

             schema = 'struct<{}>'.format(','.join([ f'{x}:string' for x in fields_list ]))
             df.withColumn('data', from_json('json_col', schema)).select('data.*')



+ pyspark.sql.types
  + json(): 
    put the DataType() metadata into a JSON string
  + jsonValue(): 
    put the DataType() metadata into a Python data structure
  + fromJson():     <-- a classmethod
    specify the DataType using a JSON value. for example:

         col_dtype = StructType.fromJson(json_value)

  JSON data type mapping:
    - Array: ArrayType()
    - Boolean: BooleanType()
    - null: NullType()
    - Object: StructType(), MapType()
    - String: StringType()
    - Number: Numeric types


  
+ DataFrame.toJSON()
  convert a dataframe into a RDD of string


Notes: 
---
  (1) Single-quotes are allowed for JSON quoting:  allowSingleQuotes=true by default for pyspark JSON reader
  (2) pyspark's JSONPath support is limited, check the following page for all supported functionalities:

   https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object 

   JSON Path implementation with Scala:
   https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/jsonExpressions.scala#L54

   the child expression: .name or ['name'], name could be either 
     (1) single-quoted `name` enclosed by brackets, or 
     (2) a dot followed by a string without containing any dot and opening bracket

   Notice: F.get_json_object('_1', "$['element name']") works
           F.get_json_object('_1', "$.element name") works
           F.get_json_object('_1', '$["element name"]') does not work


