Continue..

Spark SQL higher-order functions:

(36) dataframe join using arrays_overlap() and then aggregate
  REF: https://stackoverflow.com/questions/63769895
  Target: for each row in df2, find the topic with highest total sum of termWeights in 
          df2.tokens from the matched df1.terms
  Notes: 
   (1) condition with array_overlaps:
     item-1. return true if both arrays contain at lease one non-null elements
     item-2. else if both arrays are non-EMPTY and contain at least one NULL element in any of the arrays, return NULL
     item-3. else return false
   (2) the difference from size + array_intersect which returns false instead of NULL with the above item-2.
   (3) use arrays_overlap when joining by the true condition, use `size(array_intersect(a1,a2))==0` when looking for
       arrays without overlap.

    from pyspark.sql.functions import expr, desc, broadcast

    # example dataframes
    df1 = spark.createDataFrame([
      (0, [0.023463344607734377, 0.011772322769900843, 0.021], ["cell", "apoptosis", "uptake"]),
      (1, [0.013057307487199429, 0.011453455929929763, 0.002], ["therapy", "cancer", "diet"]),
      (2, [0.03979063871841061, 0.026593954837078836,0.011], ["group", "expression", "performance"]),
      (3, [0.009305626056223443, 0.008840730657888991,0.021], ["pattern", "chemotherapy", "mass"])
    ], ["topic", "termWeights", "terms"])

    df2 = spark.createDataFrame([
      (0, ["The human KCNJ9", "GIRK3", "member", "potassium"]),
      (1, ["BACKGROUND", "the treatment", "breast", "cancer", "chemotherapy", "diet"]),
      (2, ["OBJECTIVE", "PATTERN", "apoptosis"])
    ], ["r_id", "tokens"])
    
   **Step-1:** convert all tokens to lowercase

    df2 = df2.withColumn('tokens', expr("transform(tokens, x -> lower(x))"))

   **Step-2:** left-join df2 with df1 using arrays_overlap 

    df3 = df2.join(broadcast(df1), expr("arrays_overlap(terms, tokens)"), "left")
    +----+--------------------+-----+--------------------+--------------------+
    |r_id|              tokens|topic|         termWeights|               terms|
    +----+--------------------+-----+--------------------+--------------------+
    |   0|[the human kcnj9,...| null|                null|                null|
    |   1|[background, the ...|    1|[0.01305730748719...|[therapy, cancer,...|
    |   1|[background, the ...|    3|[0.00930562605622...|[pattern, chemoth...|
    |   2|[objective, patte...|    0|[0.02346334460773...|[cell, apoptosis,...|
    |   2|[objective, patte...|    3|[0.00930562605622...|[pattern, chemoth...|
    +----+--------------------+-----+--------------------+--------------------+

  **Step-3:** calculate matched_total_weights from terms, termWeights and tokens

    df4 = df3.selectExpr(
        "r_id", 
        "tokens", 
        "topic", 
        """ 
          aggregate( 
            /* find all terms+termWeights which are shown in tokens array */
            filter(arrays_zip(terms,termWeights), x -> array_contains(tokens, x.terms)), 
            0D,
            /* get the sum of all termWeights from the matched terms */
            (acc, y) -> acc + y.termWeights 
          ) as matched_total_weights 
        """)
    df4.show()
    +----+--------------------+-----+---------------------+
    |r_id|              tokens|topic|matched_total_weights|
    +----+--------------------+-----+---------------------+
    |   0|[the human kcnj9,...| null|                 null|
    |   1|[background, the ...|    1| 0.013453455929929763|
    |   1|[background, the ...|    3| 0.008840730657888991|
    |   2|[objective, patte...|    0| 0.011772322769900843|
    |   2|[objective, patte...|    3| 0.009305626056223443|
    +----+--------------------+-----+---------------------+

   Note: if the max_of_weights instead of sum_of_weights is required, change 
         `(acc, y) -> acc + y.termWeights` to `(acc, y) -> greatest(acc, y.termWeights)`
         or just replace aggregate function with array_max()
             array_max(filter(arrays_zip(termWeights,terms), x -> array_contains(tokens, x.terms))).termWeights

  **Step-4:** for each r_id, find the rows with highest `matched_total_weights` using groupby with max_by function
  
    from pyspark.sql.functions import max_by

    df4.groupby('r_id','tokens').agg(max_by('topic','matched_total_weights').alias('topic')).show(5,0)
    +----+---------------------------------------------------------------+-----+
    |r_id|tokens                                                         |topic|
    +----+---------------------------------------------------------------+-----+
    |1   |[background, the treatment, breast, cancer, chemotherapy, diet]|1    |
    |0   |[the human kcnj9, girk3, member, potassium]                    |null |
    |2   |[objective, pattern, apoptosis]                                |0    |
    +----+---------------------------------------------------------------+-----+



(37) split a large Map field by max N keys
  REF: https://stackoverflow.com/questions/63856354
  Method: use transform to convert map into ceil(size(map)/N), then explode
  Code:

    df = spark.createDataFrame([
       ('A', {1:100, 2:200, 3:100}),
       ('B', {4:300, 1:500, 9:300, 11:900, 5:900, 6: 111, 7: 222, 8: 333, 12: 444, 13:555, 19:666}),
       ('C', {6:100, 4:200, 7:100, 8:200, 5:800})
    ], ["id", "map1"])

    split_by = lambda N: f""" 
        explode(transform(
            sequence(0, size(map1)-1, {N}), 
            i -> map_from_entries(
                     transform(slice(map_keys(map1),i+1,{N}), k -> (k, map1[k]))
                 )
        )) as map1"""

    df.selectExpr("id", split_by(5)).show(10,0) 
    +---+----------------------------------------------------+
    |id |map1                                                |
    +---+----------------------------------------------------+
    |A  |{1 -> 100, 2 -> 200, 3 -> 100}                      |
    |B  |{1 -> 500, 19 -> 666, 4 -> 300, 5 -> 900, 6 -> 111} |
    |B  |{7 -> 222, 8 -> 333, 9 -> 300, 11 -> 900, 12 -> 444}|
    |B  |{13 -> 555}                                         |
    |C  |{8 -> 200, 4 -> 200, 5 -> 800, 6 -> 100, 7 -> 100}  |
    +---+----------------------------------------------------+


    df.selectExpr("id", split_by(3)).show(10,0)                                                                         
    +---+-------------------------------+
    |id |map1                           |
    +---+-------------------------------+
    |A  |{1 -> 100, 2 -> 200, 3 -> 100} |
    |B  |{1 -> 500, 19 -> 666, 4 -> 300}|
    |B  |{5 -> 900, 6 -> 111, 7 -> 222} |
    |B  |{8 -> 333, 9 -> 300, 11 -> 900}|
    |B  |{12 -> 444, 13 -> 555}         |
    |C  |{8 -> 200, 4 -> 200, 5 -> 800} |
    |C  |{6 -> 100, 7 -> 100}           |
    +---+-------------------------------+


  Example-37-2: https://stackoverflow.com/q/64312087/9510729

    from pyspark.sql import functions as F

    df = spark.createDataFrame([('1','[{price:100, quantity:1},{price:200, quantity:2},{price:900, quantity:3},{price:500, quantity:5},{price:100, quantity:1},{price:800, quantity:8},{price:700, quantity:7},{price:600, quantity:6}]'),('2','[{price:100, quantity:1}]')],['id','data'])

    split_by = lambda N: f""" 
            posexplode_outer(
                transform(
                    sequence(0, size(data)-1, {N}),    
                    i -> slice(data, i+1, {N})
                )
            )
     """

    df1 = df.withColumn(
            "data", 
            F.from_json("data", "array<struct<price:int,quantity:int>>",{"allowUnquotedFieldNames":"True"})
       ).selectExpr("id", split_by(5))
    df1.show(5,0)
    +---+---+--------------------------------------------------+
    |id |pos|col                                               |
    +---+---+--------------------------------------------------+
    |1  |0  |[{100, 1}, {200, 2}, {900, 3}, {500, 5}, {100, 1}]|
    |1  |1  |[{800, 8}, {700, 7}, {600, 6}]                    |
    |2  |0  |[{100, 1}]                                        |
    +---+---+--------------------------------------------------+


(38) one typical aggregate function use case with stateful accumulation.
  REF: https://stackoverflow.com/questions/64144891
  Method: 
   1. use groupby+collect_list+struct and sort_array to create an array of structs
   2. use arrgregate function. 
      expr: `slice(dta, 2, size(dta)-1)`
      start: `array(dta[0])`
      merge(acc, x): use `concat(acc, array(named_struct(...)))` to update the array of structs `acc`
                     use x.col1, x.coln to refer to values on the current Row
                     use element_at(acc, -1).col1, element_at(acc, -1).coln to refer to values on the previous Row

    from pyspark.sql.functions import sort_array, collect_list, struct

    df1 = spark.createDataFrame([
        ('2020-08-01',0,'M1',3,3,2),('2020-08-02',0,'M1',2,3,1),('2020-08-03',0,'M1',3,3,3),
        ('2020-08-04',0,'M1',3,3,1),('2020-08-01',0,'M2',1,3,1),('2020-08-02',0,'M2',-1,3,2)
    ], ["Date", "col1", "id", "col2", "col3", "coln"])

    cols = ['Date', 'col1', 'col2', 'col3', 'coln']

    df_new = df1.groupby('id') \
        .agg(sort_array(collect_list(struct(*cols))).alias('dta')) \
        .selectExpr("id", """  
            inline( 
                aggregate( 
                    /* expr: iterate through the array `dta` from the 2nd to the last items*/
                    slice(dta,2,size(dta)-1), 
                    /* start: AKA. the zero value which is an array of structs 
                     * with a single element dta[0]
                     */
                    array(dta[0]), 
                    /* merge: do the calculations */
                    (acc, x) -> array_append(acc, named_struct(  
                            'Date', x.Date, 
                            'col1', element_at(acc, -1).coln, 
                            'col2', x.col2, 
                            'col3', element_at(acc, -1).col3 + x.col2, 
                            'coln', x.col3 - x.col2 
                        )
                    )  
                )    
            )    
       """)
    df_new.show()
    +---+----------+----+----+----+----+ 
    | id|      Date|col1|col2|col3|coln|
    +---+----------+----+----+----+----+
    | M1|2020-08-01|   0|   3|   3|   2|
    | M1|2020-08-02|   2|   2|   5|   1|
    | M1|2020-08-03|   1|   3|   8|   0|
    | M1|2020-08-04|   0|   3|  11|   0|
    | M2|2020-08-01|   0|   1|   3|   1|
    | M2|2020-08-02|   1|  -1|   2|   4|
    +---+----------+----+----+----+----+


(39) build a chain of segments, use aggregate function to find the order of related rows
  REF: https://stackoverflow.com/questions/63789750
  NOTE: need improve

    from pyspark.sql.functions import collect_list, struct

    df = spark.createDataFrame([
       (1, "a1", "a2"), (1, "a2", "a3"), (2, "b1", "b2"),
       (3, "c1", "c2"), (3, "c3", "c4"), (3, "c2", "c3")
    ],["SegmentId", "SubSegmentStart", "SubSegmentEnd"])

    # set an array of structs containing 'SubSegmentStart', 'SubSegmentEnd'
    df1 = df.groupby('SegmentId').agg(
        collect_list(struct('SubSegmentStart', 'SubSegmentEnd')).alias('dta'),
    )

    df2 = df1.selectExpr( 
      "SegmentId", 
      """ 
      inline_outer(
          aggregate(
              slice(dta,2,size(dta)), 
              array(dta[0]), 
              (acc,x) -> CASE
                      /* x.SubSegmentStart not in list of SubSegmentStart, then put to the first */
                      WHEN NOT array_contains(dta.SubSegmentEnd, x.SubSegmentStart)
                          THEN array_insert(acc, 1, x)
                      /* if x.SubSegmentEnd found in acc.SubSegmentStart, then replace its position */
                      WHEN array_contains(acc.SubSegmentStart, x.SubSegmentEnd) 
                          THEN array_insert(acc, int(array_position(acc.SubSegmentStart,x.SubSegmentEnd)), x)
                      /* if x.SubSegmentStart found in acc.SubSegmentEnd, then place at its next position */
                      WHEN array_contains(acc.SubSegmentEnd, x.SubSegmentStart)
                          THEN array_insert(acc, int(array_position(acc.SubSegmentEnd,x.SubSegmentStart))+1, x)
                      /* default move to the end of the list */
                      ELSE
                          array_append(acc, x)
                  END,
               acc -> transform(acc, (x,i) -> (x.SubSegmentStart, x.SubSegmentEnd, i as Index))
          )
      )
      """
    )
    df2.show()
    +---------+---------------+-------------+-----+
    |SegmentId|SubSegmentStart|SubSegmentEnd|Index|
    +---------+---------------+-------------+-----+
    |        1|             a1|           a2|    0|
    |        1|             a2|           a3|    1|
    |        3|             c1|           c2|    0|
    |        3|             c2|           c3|    1|
    |        3|             c3|           c4|    2|
    |        2|             b1|           b2|    0|
    +---------+---------------+-------------+-----+


(40) drop duplicates using aggregate
  REF: https://stackoverflow.com/a/64270968/9510729
  Notes:
   (1) to retrieve a single fields from an array of structs, just simply use getItem method and its alternatives
       i.e. to get an array of cities from the below city_set array
         func.col('city_set').getItem('city')
         func.col('city_set')['city']
         func.col('city_set').city
   (2) using aggregate function to drop duplicates, use array_contains() to check if x.city already exists
       in `acc` array. see below two situations:

    from pyspark.sql import functions as func 
    
    d = spark.createDataFrame([
        ('1234', '203957', 2010, 'London', 'CHEM'), ('1234', '203957', 2010, 'London', 'BIOL'), 
        ('1234', '203957', 2011, 'London', 'PHYS'), ('1234', '203957', 2013, 'London', 'PHYS'), 
        ('1234', '288400', 2012, 'Berlin', 'MATH'), ('1234', '288400', 2012, 'Berlin', 'CHEM') 
    ], ['auid', 'eid', 'year', 'city', 'subject'])
    
  Case-1: only city field is required

    df1 = d.groupby('auid').agg(func.collect_set('city').alias('citi_set'))
    +----+----------------+
    |auid|        city_set|
    +----+----------------+
    |1234|[London, Berlin]|
    +----+----------------+
    
  Case-2: the whole struct is required, keep only the first array element upon duplicate city entries

    df1 = (d.groupby('auid','city')
        .agg(func.min('year').alias('year'))
        .groupby('auid')
        .agg(func.collect_list(func.struct('city','year')).alias('city_set'))
    )
    df1.show(1,0)
    +----+--------------------------------+
    |auid|city_set                        |
    +----+--------------------------------+
    |1234|[{Berlin, 2012}, {London, 2010}]|
    +----+--------------------------------+

  Old-method: using aggregate function [sorting is unnecessary, only for illustration purpose]
    df2 = (d.groupby('auid')
        .agg(func.array_sort(func.collect_list(func.struct('city','year'))).alias('city_set'))
        .withColumn('city_set', func.expr("""
            aggregate(
                dta,
                cast(array() as array<struct<city:string,year:long>>),
                (acc, x) -> IF(array_contains(acc.city, x.city), acc, array_append(acc,x))
        """)
    )


(41) use transform + array_except to do combinations of two arrays
    REF: https://stackoverflow.com/q/64791167/9510729
    Targets: for size of arrays including Set of 3 numbers, create a new array of size-4
      which must satisfy both two original arrays are subset of the new array.
    Method: use array_except to find element from element_1 but not in element_2
            use transform to append each to element_2 using concat + array
            do the same to the other direction and then union two arrays
    
    df = spark.createDataFrame([
      ([1, 4, 3],[3, 4, 5]), ([2, 1, 3], [1, 0, 2]), ([4, 3, 1], [3, 5, 6])
    ], ['elements_1','elements_2'])
    
    df_new = df.selectExpr("*", """
        array_union(
          transform(array_except(elements_1,elements_2),x -> concat(elements_2,array(x))),
          transform(array_except(elements_2,elements_1),x -> concat(elements_1,array(x)))
        ) as flag""")
    
    df_new.show(10,0)
    +----------+----------+--------------------------------------------------------+
    |elements_1|elements_2|flag                                                    |
    +----------+----------+--------------------------------------------------------+
    |[1, 4, 3] |[3, 4, 5] |[[3, 4, 5, 1], [1, 4, 3, 5]]                            |
    |[2, 1, 3] |[1, 0, 2] |[[1, 0, 2, 3], [2, 1, 3, 0]]                            |
    |[4, 3, 1] |[3, 5, 6] |[[3, 5, 6, 4], [3, 5, 6, 1], [4, 3, 1, 5], [4, 3, 1, 6]]|
    |[1, 2, 3] |[3, 4, 5] |[[3, 4, 5, 1], [3, 4, 5, 2], [1, 2, 3, 4], [1, 2, 3, 5]]|
    +----------+----------+--------------------------------------------------------+
    


(42) find Last N unique elements from an Array using aggregate function
  REF: https://stackoverflow.com/q/65290397/9510729
  Method: iterate through `list` and save only unique `x`, skip duplicates.

    df = spark.sql(""" with t1( 
        select 0 as index, "F" as x  union all
        select 1,"F,B" as x  union all
        select 2,"F,B,A" as x  union all
        select 3,"F,B,A,F" as x  union all
        select 4,"F,B,A,F,B" as x  union all
        select 5,"F,B,A,F,B,G" as x  union all
        select 6,"F,B,A,F,B,G,E" as x  union all
        select 7,"F,B,A,F,B,G,E,F" as x  union all
        select 8,"F,B,A,F,B,G,E,F,E" as x  union all
        select 9,"F,B,A,F,B,G,E,F,E,B" as x  union all
        select 10,"F,B,A,F,B,G,E,F,E,B,A" as x  union all
        select 11,"F,B,A,F,B,G,E,F,E,B,A,D" as x  union all
        select 12,"F,B,A,F,B,G,E,F,E,B,A,D,F" as x  union all
        select 13,"F,B,A,F,B,G,E,F,E,B,A,D,F,E" as x  union all
        select 14,"F,B,A,F,B,G,E,F,E,B,A,D,F,E,E" as x  union all
        select 15,"F,B,A,F,B,G,E,F,E,B,A,D,F,E,E,D" as x  
      ) select index, split(x,',') as list from t1
    """)

    df.selectExpr("*", """
        aggregate(
            /* expr: reverse of list */
            reverse(list),
            /* start: zero value as empty array of strings */
            CAST(array() as array<string>),
            /* merge: add x into acc only if x does not exists in acc */
            (acc,x) -> IF(array_contains(acc,x), acc, array_append(acc,x)),
            /* finish: take the first 5 element and sort the list */
            acc -> sort_array(slice(acc,1,5))
        ) as window 
    """).show(16,0)
    +-----+------------------------------------------------+---------------+
    |index|list                                            |window         |
    +-----+------------------------------------------------+---------------+
    |0    |[F]                                             |[F]            |
    |1    |[F, B]                                          |[B, F]         |
    |2    |[F, B, A]                                       |[A, B, F]      |
    |3    |[F, B, A, F]                                    |[A, B, F]      |
    |4    |[F, B, A, F, B]                                 |[A, B, F]      |
    |5    |[F, B, A, F, B, G]                              |[A, B, F, G]   |
    |6    |[F, B, A, F, B, G, E]                           |[A, B, E, F, G]|
    |7    |[F, B, A, F, B, G, E, F]                        |[A, B, E, F, G]|
    |8    |[F, B, A, F, B, G, E, F, E]                     |[A, B, E, F, G]|
    |9    |[F, B, A, F, B, G, E, F, E, B]                  |[A, B, E, F, G]|
    |10   |[F, B, A, F, B, G, E, F, E, B, A]               |[A, B, E, F, G]|
    |11   |[F, B, A, F, B, G, E, F, E, B, A, D]            |[A, B, D, E, F]|
    |12   |[F, B, A, F, B, G, E, F, E, B, A, D, F]         |[A, B, D, E, F]|
    |13   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E]      |[A, B, D, E, F]|
    |14   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E]   |[A, B, D, E, F]|
    |15   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E, D]|[A, B, D, E, F]|
    +-----+------------------------------------------------+---------------+

  Another topic: to find the unique elements from the end of array until the first duplicate value appears
    df.selectExpr("*", """
        aggregate(
            reverse(list),
            /* added a `tbc` boolean flag to identify if the first duplicate is found */
            CAST((array(),true) as struct<dta:array<string>,tbc:boolean>),
            (acc,x) -> CASE 
                    WHEN NOT acc.tbc OR array_contains(acc.dta,x) THEN named_struct("dta", acc.dta, "tbc", false)
                    ELSE named_struct("dta", array_append(acc.dta,x), "tbc", true)
                END,
            acc -> acc.dta
        ) as window 
    """).show(16,0)
    +-----+------------------------------------------------+---------------+
    |index|list                                            |window         |
    +-----+------------------------------------------------+---------------+
    |0    |[F]                                             |[F]            |
    |1    |[F, B]                                          |[B, F]         |
    |2    |[F, B, A]                                       |[A, B, F]      |
    |3    |[F, B, A, F]                                    |[F, A, B]      |
    |4    |[F, B, A, F, B]                                 |[B, F, A]      |
    |5    |[F, B, A, F, B, G]                              |[G, B, F, A]   |
    |6    |[F, B, A, F, B, G, E]                           |[E, G, B, F, A]|
    |7    |[F, B, A, F, B, G, E, F]                        |[F, E, G, B]   |
    |8    |[F, B, A, F, B, G, E, F, E]                     |[E, F]         |
    |9    |[F, B, A, F, B, G, E, F, E, B]                  |[B, E, F]      |
    |10   |[F, B, A, F, B, G, E, F, E, B, A]               |[A, B, E, F]   |
    |11   |[F, B, A, F, B, G, E, F, E, B, A, D]            |[D, A, B, E, F]|
    |12   |[F, B, A, F, B, G, E, F, E, B, A, D, F]         |[F, D, A, B, E]|
    |13   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E]      |[E, F, D, A, B]|
    |14   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E]   |[E]            |
    |15   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E, D]|[D, E]         |
    +-----+------------------------------------------------+---------------+

    ## find from the end of the array up to the first duplicate element, max N number of elements returned
    ## can use the above method, but change the finish from `acc -> acc.dta` to acc -> slice(acc.dta, 1, {N}) 
    ## or adjust the accumulator from an tbc:boolean to cnt_uniq:tinyint
    N = 4
    df.selectExpr("*", f"""
        aggregate(
            reverse(list),
            /* use cnt_uniq to identify number of uniq elements detected, 
             * force its value to N whenever the first duplicate is found
             */
            CAST((array(),0) as struct<dta:array<string>,cnt_uniq:tinyint>),
            (acc,x) -> CASE     
                    WHEN acc.cnt_uniq >= {N}Y OR array_contains(acc.dta,x) 
                        THEN named_struct("dta", acc.dta, "cnt_uniq", {N}Y)
                    ELSE named_struct("dta", array_append(acc.dta,x), "cnt_uniq", acc.cnt_uniq+1Y)
                END,
            acc -> acc.dta
        ) as window     
    """).show(16,0
    +-----+------------------------------------------------+------------+
    |index|list                                            |window      |
    +-----+------------------------------------------------+------------+
    |0    |[F]                                             |[F]         |
    |1    |[F, B]                                          |[B, F]      |
    |2    |[F, B, A]                                       |[A, B, F]   |
    |3    |[F, B, A, F]                                    |[F, A, B]   |
    |4    |[F, B, A, F, B]                                 |[B, F, A]   |
    |5    |[F, B, A, F, B, G]                              |[G, B, F, A]|
    |6    |[F, B, A, F, B, G, E]                           |[E, G, B, F]|
    |7    |[F, B, A, F, B, G, E, F]                        |[F, E, G, B]|
    |8    |[F, B, A, F, B, G, E, F, E]                     |[E, F]      |
    |9    |[F, B, A, F, B, G, E, F, E, B]                  |[B, E, F]   |
    |10   |[F, B, A, F, B, G, E, F, E, B, A]               |[A, B, E, F]|
    |11   |[F, B, A, F, B, G, E, F, E, B, A, D]            |[D, A, B, E]|
    |12   |[F, B, A, F, B, G, E, F, E, B, A, D, F]         |[F, D, A, B]|
    |13   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E]      |[E, F, D, A]|
    |14   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E]   |[E]         |
    |15   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E, D]|[D, E]      |
    +-----+------------------------------------------------+------------+)



