Continue..

Spark SQL higher-order functions:

(36) dataframe join using arrays_overlap() and then aggregate
  REF: https://stackoverflow.com/questions/63769895
  Target: for each row in df2, find the topic with highest total sum of termWeights in 
          df2.tokens from the matched df1.terms
  Notes: 
   (1) condition with array_overlaps:
     item-1. return true if both array contain any non-null elements
     item-2. retuen NULL if above is not andboth arrays are non-EMPTY and any of them contains a NULL element
     item-3. return false
   (2) the difference from size + array_intersect which returns false instead of NULL with the above item-2.
   (3) use arrays_overlap when joining by the true condition, use `size(array_intersect(a1,a2))==0` when looking for
       arrays without overlap.

  Method-1: using arrays_overlap on join condition and then do calculations

    from pyspark.sql.functions import expr, desc, row_number, broadcast

    # example dataframes
    df1 = spark.createDataFrame([
      (0, [0.023463344607734377, 0.011772322769900843, 0.021], ["cell", "apoptosis", "uptake"]),
      (1, [0.013057307487199429, 0.011453455929929763, 0.002], ["therapy", "cancer", "diet"]),
      (2, [0.03979063871841061, 0.026593954837078836,0.011], ["group", "expression", "performance"]),
      (3, [0.009305626056223443, 0.008840730657888991,0.021], ["pattern", "chemotherapy", "mass"])
    ], ["topic", "termWeights", "terms"])

    df2 = spark.createDataFrame([
      (0, ["The human KCNJ9", "GIRK3", "member", "potassium"]),
      (1, ["BACKGROUND", "the treatment", "breast", "cancer", "chemotherapy", "diet"]),
      (2, ["OBJECTIVE", "PATTERN", "apoptosis"])
    ], ["r_id", "tokens"])
    
   **Step-1:** convert all tokens to lowercase

    df2 = df2.withColumn('tokens', expr("transform(tokens, x -> lower(x))"))

   **Step-2:** left-join df2 with df1 using arrays_overlap 

    df3 = df2.join(broadcast(df1), expr("arrays_overlap(terms, tokens)"), "left")
    +----+--------------------+-----+--------------------+--------------------+
    |r_id|              tokens|topic|         termWeights|               terms|
    +----+--------------------+-----+--------------------+--------------------+
    |   0|[the human kcnj9,...| null|                null|                null|
    |   1|[background, the ...|    1|[0.01305730748719...|[therapy, cancer,...|
    |   1|[background, the ...|    3|[0.00930562605622...|[pattern, chemoth...|
    |   2|[objective, patte...|    0|[0.02346334460773...|[cell, apoptosis,...|
    |   2|[objective, patte...|    3|[0.00930562605622...|[pattern, chemoth...|
    +----+--------------------+-----+--------------------+--------------------+

  **Step-3:** calculate matched_total_weights from terms, termWeights and tokens

    df4 = df3.selectExpr(
        "r_id", 
        "tokens", 
        "topic", 
        """ 
          aggregate( 
            /* find all terms+termWeights which are shown in tokens array */
            filter(arrays_zip(terms,termWeights), x -> array_contains(tokens, x.terms)), 
            0D,
            /* get the sum of all termWeights from the matched terms */
            (acc, y) -> acc + y.termWeights 
          ) as matched_total_weights 
        """)
    df4.show()
    +----+--------------------+-----+---------------------+
    |r_id|              tokens|topic|matched_total_weights|
    +----+--------------------+-----+---------------------+
    |   0|[the human kcnj9,...| null|                 null|
    |   1|[background, the ...|    1| 0.013453455929929763|
    |   1|[background, the ...|    3| 0.008840730657888991|
    |   2|[objective, patte...|    0| 0.011772322769900843|
    |   2|[objective, patte...|    3| 0.009305626056223443|
    +----+--------------------+-----+---------------------+

   Note: if the max_of_weights instead of sum_of_weights is required, change 
         `(acc, y) -> acc + y.termWeights` to `(acc, y) -> greatest(acc, y.termWeights)`

  **Step-4:** for each r_id, find the rows with highest `matched_total_weights` using Window function and filter

    from pyspark.sql import Window
    w1 = Window.partitionBy('r_id').orderBy(desc('matched_total_weights'))
    
    df4.withColumn('rn', row_number().over(w1)).filter('rn=1').drop('rn', 'matched_total_weights').show()
    +----+---------------------------------------------------------------+-----+    
    |r_id|tokens                                                         |topic|
    +----+---------------------------------------------------------------+-----+
    |0   |[the human kcnj9, girk3, member, potassium]                    |null |
    |1   |[background, the treatment, breast, cancer, chemotherapy, diet]|1    |
    |2   |[objective, pattern, apoptosis]                                |0    |
    +----+---------------------------------------------------------------+-----+


  Method-2: iterate through df1 using withColumn and a loop, this does not require join/groupby, good for small df1

    from pyspark.sql.functions import expr, when, coalesce, array_contains, lit, struct

    d = df1.selectExpr("string(topic)", "arrays_zip(termWeights,terms) as terms").rdd.collectAsMap()

    df2 = df2.withColumn('tokens', expr("transform(tokens, x -> lower(x))"))

    cols = df2.columns

    for x,y in d.items(): 
      df2 = df2.withColumn(x, 
          struct(
            sum([when(array_contains('tokens', t.terms), t.termWeights).otherwise(0) for t in y]).alias('sum_of_weights'),
            lit(x).alias('topic'),
            coalesce(*[when(array_contains('tokens', t.terms),1) for t in y]).isNotNull().alias('has_match')
          )
      )
    df2.show()
    +----+--------------------+--------------------+--------------------+---------------+--------------------+
    |r_id|              tokens|                   0|                   1|              2|                   3|
    +----+--------------------+--------------------+--------------------+---------------+--------------------+
    |   0|[the human kcnj9,...|     [0.0, 0, false]|     [0.0, 1, false]|[0.0, 2, false]|     [0.0, 3, false]|
    |   1|[background, the ...|     [0.0, 0, false]|[0.01345345592992...|[0.0, 2, false]|[0.00884073065788...|
    |   2|[objective, patte...|[0.01177232276990...|     [0.0, 1, false]|[0.0, 2, false]|[0.00930562605622...|
    +----+--------------------+--------------------+--------------------+---------------+--------------------+

    df3 = df2.selectExpr(
        *cols, 
        f"array_max(filter(array({'`{}`'.format('`,`',join(d.keys()))}), x -> x.has_match)).topic as topic1"
    )
    df3.show()
    +----+--------------------+-----+
    |r_id|              tokens|topic|
    +----+--------------------+-----+
    |   0|[the human kcnj9,...| null|
    |   1|[background, the ...|    1|
    |   2|[objective, patte...|    0|
    +----+--------------------+-----+



(37) split a large Map field by max N keys
  REF: https://stackoverflow.com/questions/63856354
  Method: use transform to convert map into ceil(size(map)/N), then explode
  Code:

    df = spark.createDataFrame([
       ('A', {1:100, 2:200, 3:100}),
       ('B', {4:300, 1:500, 9:300, 11:900, 5:900, 6: 111, 7: 222, 8: 333, 12: 444, 13:555, 19:666}),
       ('C', {6:100, 4:200, 7:100, 8:200, 5:800})
    ], ["id", "map1"])

    split_by = lambda N: f"""
      explode(transform( 
        sequence(1,ceil(size(map1)/{N})), 
        i -> map_from_entries(transform(slice(map_keys(map1),(i-1)*{N}+1,{N}), k -> (k, map1[k])))  
      )) as map1"""

    df.selectExpr("id", split_by(5)).show(10,0) 
    +---+----------------------------------------------------+
    |id |col                                                 |
    +---+----------------------------------------------------+
    |A  |[1 -> 100, 2 -> 200, 3 -> 100]                      |
    |B  |[1 -> 500, 19 -> 666, 4 -> 300, 5 -> 900, 6 -> 111] |
    |B  |[7 -> 222, 8 -> 333, 9 -> 300, 11 -> 900, 12 -> 444]|
    |B  |[13 -> 555]                                         |
    |C  |[8 -> 200, 4 -> 200, 5 -> 800, 6 -> 100, 7 -> 100]  |
    +---+----------------------------------------------------+

    df.selectExpr("id", split_by(3)).show(10,0)                                                                         
    +---+-------------------------------+
    |id |map1                           |
    +---+-------------------------------+
    |A  |[1 -> 100, 2 -> 200, 3 -> 100] |
    |B  |[1 -> 500, 19 -> 666, 4 -> 300]|
    |B  |[5 -> 900, 6 -> 111, 7 -> 222] |
    |B  |[8 -> 333, 9 -> 300, 11 -> 900]|
    |B  |[12 -> 444, 13 -> 555]         |
    |C  |[8 -> 200, 4 -> 200, 5 -> 800] |
    |C  |[6 -> 100, 7 -> 100]           |
    +---+-------------------------------+

  Example-37-2: https://stackoverflow.com/q/64312087/9510729

    from pyspark.sql import functions as F

    df = spark.createDataFrame([('1','[{price:100, quantity:1},{price:200, quantity:2},{price:900, quantity:3},{price:500, quantity:5},{price:100, quantity:1},{price:800, quantity:8},{price:700, quantity:7},{price:600, quantity:6}]'),('2','[{price:100, quantity:1}]')],['id','data'])

    N = 5

    (df.withColumn("data", F.from_json("data", "array<struct<price:int,quantity:int>>",{"allowUnquotedFieldNames":"True"}))
       .withColumn("data", F.expr(f"""
           explode_outer(
             transform(
               sequence(1,ceil(size(data)/{N})), i -> 
               (i as id2, slice(data,(i-1)*{N}+1,{N}) as data))
           )
        """)).select('id', 'data.*')
       .show(truncate=False))
    +---+---+--------------------------------------------------+
    |id |id2|data                                              |
    +---+---+--------------------------------------------------+
    |1  |1  |[[100, 1], [200, 2], [900, 3], [500, 5], [100, 1]]|
    |1  |2  |[[800, 8], [700, 7], [600, 6]]                    |
    |2  |1  |[[100, 1]]                                        |
    +---+---+--------------------------------------------------+



(38) a typical question to use values calculated from the previous Row: y[t+1] = func(y[t], y[t+1])
  REF: https://stackoverflow.com/questions/64144891
  Method: 
   1. use groupby+collect_list+struct and sort_array to create an array of structs
   2. use arrgregate function. 
      expr: `slice(dta, 2, size(dta)-1)`
      start: `array(dta[0])`
      merge(acc, x): use `concat(acc, array(named_struct(...)))` to update the array of structs `acc`
                     use x.col1, x.coln to refer to values on the current Row
                     use element_at(acc, -1).col1, element_at(acc, -1).coln to refer to values on the previous Row

    from pyspark.sql.functions import sort_array, collect_list, struct

    df1 = spark.createDataFrame([
        ('2020-08-01',0,'M1',3,3,2),('2020-08-02',0,'M1',2,3,1),('2020-08-03',0,'M1',3,3,3),
        ('2020-08-04',0,'M1',3,3,1),('2020-08-01',0,'M2',1,3,1),('2020-08-02',0,'M2',-1,3,2)
    ], ["Date", "col1", "id", "col2", "col3", "coln"])

    cols = ['Date', 'col1', 'col2', 'col3', 'coln']

    df_new = df1.groupby('id') \
        .agg(sort_array(collect_list(struct(*cols))).alias('dta')) \
        .selectExpr("id", """  
          inline( 
            aggregate( 
              /* expr: iterate through the array `dta` from the 2nd to the last items*/
              slice(dta,2,size(dta)-1), 
              /* start: AKA. the zero value which is an array of structs 
               * with a single element dta[0]
               */
              array(dta[0]), 
              /* merge: do the calculations */
              (acc, x) ->   
                concat(acc, array(named_struct( 
                  'Date', x.Date, 
                  'col1', element_at(acc, -1).coln, 
                  'col2', x.col2, 
                  'col3', element_at(acc, -1).col3 + x.col2, 
                  'coln', x.col3 - x.col2 
                )))  
             )    
           )    
       """)
    df_new.show()
    +---+----------+----+----+----+----+ 
    | id|      Date|col1|col2|col3|coln|
    +---+----------+----+----+----+----+
    | M1|2020-08-01|   0|   3|   3|   2|
    | M1|2020-08-02|   2|   2|   5|   1|
    | M1|2020-08-03|   1|   3|   8|   0|
    | M1|2020-08-04|   0|   3|  11|   0|
    | M2|2020-08-01|   0|   1|   3|   1|
    | M2|2020-08-02|   1|  -1|   2|   4|
    +---+----------+----+----+----+----+



(39) build a chain of segments, use aggregate function to find the order of related rows
  REF: https://stackoverflow.com/questions/63789750
  NOTE: need improve

    from pyspark.sql.functions import collect_list, struct

    df = spark.createDataFrame([
       (1, "a1", "a2"), (1, "a2", "a3"), (2, "b1", "b2"),
       (3, "c1", "c2"), (3, "c3", "c4"), (3, "c2", "c3")
    ],["SegmentId", "SubSegmentStart", "SubSegmentEnd"])

    # set an array of structs containing 'SubSegmentStart', 'SubSegmentEnd'
    df1 = df.groupby('SegmentId').agg(
        collect_list(struct('SubSegmentStart', 'SubSegmentEnd')).alias('dta'),
    )

    df2 = df1.selectExpr( 
      "SegmentId", 
      """ 
      inline_outer(
          aggregate(
              slice(dta,2,size(dta)), 
              array(dta[0]), 
              (acc,x) -> CASE
                      /* x.SubSegmentStart not in list of SubSegmentStart, then put to the first */
                      WHEN NOT array_contains(dta.SubSegmentEnd, x.SubSegmentStart)
                          THEN array_insert(acc, 1, x)
                      /* if x.SubSegmentEnd found in acc.SubSegmentStart, then replace its position */
                      WHEN array_contains(acc.SubSegmentStart, x.SubSegmentEnd) 
                          THEN array_insert(acc, int(array_position(acc.SubSegmentStart,x.SubSegmentEnd)), x)
                      /* if x.SubSegmentStart found in acc.SubSegmentEnd, then place at its next position */
                      WHEN array_contains(acc.SubSegmentEnd, x.SubSegmentStart)
                          THEN array_insert(acc, int(array_position(acc.SubSegmentEnd,x.SubSegmentStart))+1, x)
                      /* default move to the end of the list */
                      ELSE
                          array_append(acc, x)
                  END,
               acc -> transform(acc, (x,i) -> (x.SubSegmentStart, x.SubSegmentEnd, i as Index))
          )
      )
      """
    )
    df2.show()
    +---------+---------------+-------------+-----+
    |SegmentId|SubSegmentStart|SubSegmentEnd|Index|
    +---------+---------------+-------------+-----+
    |        1|             a1|           a2|    0|
    |        1|             a2|           a3|    1|
    |        3|             c1|           c2|    0|
    |        3|             c2|           c3|    1|
    |        3|             c3|           c4|    2|
    |        2|             b1|           b2|    0|
    +---------+---------------+-------------+-----+


(40) drop duplicates using aggregate
  REF: https://stackoverflow.com/a/64270968/9510729
  Notes:
   (1) to retrieve a single fields from an array of structs, just simply use getItem method and its alternatives
       i.e. to get an array of cities from the below city_set array
         func.col('city_set').getItem('city')
         func.col('city_set')['city']
         func.col('city_set').city
   (2) using aggregate function to drop duplicates, use array_contains() to check if x.city already exists
       in `acc` array. see below two situations:

    from pyspark.sql import functions as func 
    
    d = spark.createDataFrame([
        ('1234', '203957', 2010, 'London', 'CHEM'), ('1234', '203957', 2010, 'London', 'BIOL'), 
        ('1234', '203957', 2011, 'London', 'PHYS'), ('1234', '203957', 2013, 'London', 'PHYS'), 
        ('1234', '288400', 2012, 'Berlin', 'MATH'), ('1234', '288400', 2012, 'Berlin', 'CHEM') 
    ], ['auid', 'eid', 'year', 'city', 'subject'])
    
  Case-1: only city field is required

    df1 = d.groupby('auid').agg(func.collect_set('city').alias('citi_set'))
    +----+----------------+
    |auid|        city_set|
    +----+----------------+
    |1234|[London, Berlin]|
    +----+----------------+
    
  Case-2: the whole struct is required, keep only the first array element upon duplicate city entries

    df1 = (d.groupby('auid','city')
        .agg(func.min('year').alias('year'))
        .groupby('auid')
        .agg(func.collect_list(func.struct('city','year')).alias('city_set'))
    )
    df1.show(1,0)
    +----+--------------------------------+
    |auid|city_set                        |
    +----+--------------------------------+
    |1234|[{Berlin, 2012}, {London, 2010}]|
    +----+--------------------------------+

    Use aggregate function [sorting is unnecessary, only for illustration purpose]
    df2 = (d.groupby('auid')
        .agg(func.array_sort(func.collect_list(func.struct('city','year'))).alias('city_set'))
        .withColumn('city_set', func.expr("""
            aggregate(
                dta,
                cast(array() as array<struct<city:string,year:long>>),
                (acc, x) -> IF(array_contains(acc.city, x.city), acc, array_append(acc,x))
        """)
    )


(41) use transform + array_except to do combinations of two arrays
    REF: https://stackoverflow.com/q/64791167/9510729
    Targets: for size of arrays including Set of 3 numbers, create a new array of size-4
      which must satisfy both two original arrays are subset of the new array.
    Method: use array_except to find element from element_1 but not in element_2
            use transform to append each to element_2 using concat + array
            do the same to the other direction and then union two arrays
    
    df = spark.createDataFrame([
      ([1, 4, 3],[3, 4, 5]), ([2, 1, 3], [1, 0, 2]), ([4, 3, 1], [3, 5, 6])
    ], ['elements_1','elements_2'])
    
    df_new = df.selectExpr("*", """
        array_union(
          transform(array_except(elements_1,elements_2),x -> concat(elements_2,array(x))),
          transform(array_except(elements_2,elements_1),x -> concat(elements_1,array(x)))
        ) as flag""")
    
    df_new.show(10,0)
    +----------+----------+--------------------------------------------------------+
    |elements_1|elements_2|flag                                                    |
    +----------+----------+--------------------------------------------------------+
    |[1, 4, 3] |[3, 4, 5] |[[3, 4, 5, 1], [1, 4, 3, 5]]                            |
    |[2, 1, 3] |[1, 0, 2] |[[1, 0, 2, 3], [2, 1, 3, 0]]                            |
    |[4, 3, 1] |[3, 5, 6] |[[3, 5, 6, 4], [3, 5, 6, 1], [4, 3, 1, 5], [4, 3, 1, 6]]|
    |[1, 2, 3] |[3, 4, 5] |[[3, 4, 5, 1], [3, 4, 5, 2], [1, 2, 3, 4], [1, 2, 3, 5]]|
    +----------+----------+--------------------------------------------------------+
    


(42) find Last N unique elements from an Array using aggregate function
  REF: https://stackoverflow.com/q/65290397/9510729
  Method: iterate through `list` and save only unique `x`, skip duplicates.

    df = spark.sql(""" with t1( 
        select 0 as index, "F" as x  union all
        select 1,"F,B" as x  union all
        select 2,"F,B,A" as x  union all
        select 3,"F,B,A,F" as x  union all
        select 4,"F,B,A,F,B" as x  union all
        select 5,"F,B,A,F,B,G" as x  union all
        select 6,"F,B,A,F,B,G,E" as x  union all
        select 7,"F,B,A,F,B,G,E,F" as x  union all
        select 8,"F,B,A,F,B,G,E,F,E" as x  union all
        select 9,"F,B,A,F,B,G,E,F,E,B" as x  union all
        select 10,"F,B,A,F,B,G,E,F,E,B,A" as x  union all
        select 11,"F,B,A,F,B,G,E,F,E,B,A,D" as x  union all
        select 12,"F,B,A,F,B,G,E,F,E,B,A,D,F" as x  union all
        select 13,"F,B,A,F,B,G,E,F,E,B,A,D,F,E" as x  union all
        select 14,"F,B,A,F,B,G,E,F,E,B,A,D,F,E,E" as x  union all
        select 15,"F,B,A,F,B,G,E,F,E,B,A,D,F,E,E,D" as x  
      ) select index, split(x,',') as list from t1
    """)

    df.selectExpr("*", """
      aggregate(
        /* expr: reverse of list */
        reverse(list),
        /* start: zero value as empty array of strings */
        CAST(array() as array<string>),
        /* merge: add x into acc only if x does not exists in acc */
        (acc,x) -> IF(array_contains(acc,x),acc,concat(acc,array(x))),
        /* finish: take the first 5 element and sort the list */
        acc -> sort_array(slice(acc,1,5))
      ) as window """).show(16,0)
    +-----+------------------------------------------------+---------------+
    |index|list                                            |window         |
    +-----+------------------------------------------------+---------------+
    |0    |[F]                                             |[F]            |
    |1    |[F, B]                                          |[B, F]         |
    |2    |[F, B, A]                                       |[A, B, F]      |
    |3    |[F, B, A, F]                                    |[A, B, F]      |
    |4    |[F, B, A, F, B]                                 |[A, B, F]      |
    |5    |[F, B, A, F, B, G]                              |[A, B, F, G]   |
    |6    |[F, B, A, F, B, G, E]                           |[A, B, E, F, G]|
    |7    |[F, B, A, F, B, G, E, F]                        |[A, B, E, F, G]|
    |8    |[F, B, A, F, B, G, E, F, E]                     |[A, B, E, F, G]|
    |9    |[F, B, A, F, B, G, E, F, E, B]                  |[A, B, E, F, G]|
    |10   |[F, B, A, F, B, G, E, F, E, B, A]               |[A, B, E, F, G]|
    |11   |[F, B, A, F, B, G, E, F, E, B, A, D]            |[A, B, D, E, F]|
    |12   |[F, B, A, F, B, G, E, F, E, B, A, D, F]         |[A, B, D, E, F]|
    |13   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E]      |[A, B, D, E, F]|
    |14   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E]   |[A, B, D, E, F]|
    |15   |[F, B, A, F, B, G, E, F, E, B, A, D, F, E, E, D]|[A, B, D, E, F]|
    +-----+------------------------------------------------+---------------+


