https://stackoverflow.com/questions/58396786/how-keep-number-in-text

An example using regexp_replace and split to do findall() in pyspark

Task: finds all numbers with 4-6 digits, followed by a letter or '/' but not preceded by `ABC`
How with pyspark builtin API functions:
---
(1) use regexp_replace to replace texts matching the following pattern with `$1\0`

      .*?(?<!ABC\s{0,5})(?<!\d)(\d{4,6})(?=[A-Z/])

  * (?<!ABC\s{0,5}): not preceded by ABC with optionally 0-5 white spaces
  * (?<!\d) not preceded by a number
  * (?=[A-Z/]) must followed by a Uppercase letter or '/'
  * (\d{4,6}): 4-6 digit captured in `$1`, this is matched NUMBER we need

   this will removed all unrelated text before the matched NUMBER and then attached a NULL char `\0`

(2) use split(text, '\0') to convert this into an array, notice that the last items of the array 
    are always irrelevant which need to be removed

(3) use regexp_replace(text, '\0?[^\0]*$', '') to remove all trailing unrelated text including 
    Rows without any matched NUMBER. do this before running the above split() function

The result is samilar to Pandas Series.str.findall() which we use 2 x regexp_replace + split 
to reach the same goal. Below is the Pyspark code:

    from pyspark.sql.functions import split, regexp_replace

    df = spark.createDataFrame([
        ('23458/ mundo por el mero de NJU/LOK 12345T98789 hablantes',),
        ('con dominio nativo ABC 987480B millones ZES/UJ86758/L87586:residentes en',), 
        ('hispanohablantes  residentes en ABC  98754/ otros pases',)
      ], ['My_column'])


    df.withColumn('new_column'
       , split(
           regexp_replace(
               regexp_replace('My_column', r'.*?(?<!ABC\s{0,5})(?<!\d)(\d{4,6})(?=[A-Z/])', '$1\0')
             , '\0?[^\0]*$'
             , ''
           )
         ,'\0')
       ) \
      .show(truncate=False)
    +-----------------------------------------------------------------------+--------------+
    |My_column                                                              |new_column    |
    +-----------------------------------------------------------------------+--------------+
    |23458/ mundo por el nmero de NJU/LOK 12345T98789 hablantes             |[23458, 12345]|
    |con dominio nativo ABC 987480 millones ZES/UJ86758/L87586:residentes en|[86758]       |
    |hispanohablantes  residentes en ABC98754/ otros pases                  |[]            |
    +-----------------------------------------------------------------------+--------------+

Notes: 
 (1) (?<!ABC\s{0,5}) will allow to test 0-5 whitespaces between ABC and the NUMBER_NEEDED. 
     since regex negative lookbehind does not support (?<!ABC\s*), if the original text contain 
     more spaces in between, then adjust 5 to a larger number. Note: `(?<!ABC\s{0,5})` is fine
     with PySpark, but will fail with Python re module which need a fix-width pattern.
 (2) assumed that the NULL char \0 is not shown in the original texts, you can use another 
     delimiterchar or just prepross the text with regexp_replace('\0|(?<=ABC)\s+', '')
     Notes: if we remove all whitespaces after `ABC`, then in (1), we just need `(?<!ABC)`

        df.withColumn('new_column'
           , split(
               regexp_replace(
                 regexp_replace(
                     regexp_replace('My_column', '\0|(?<=ABC)\s+', '')
                   , r'.*?(?<!ABC)(?<!\d)(\d{4,6})(?=[A-Z/])'
                   , '$1\0'
                 )
                 , '\0?[^\0]*$'
                 , ''
               )
               , '\0'
             )
           ) \
          .show(truncate=False)


 (3) prepend the `ptn` with `(?s)` if any text contain line breaks.

Another solution with UDF:

    import re
    from pyspark.sql.types import ArrayType, StringType
    from pyspark.sql.functions import udf

    ptn = re.compile(r'(?<!ABC)(?<!\d)(\d{4,6})(?=[A-Z/])')

    find_number = udf(lambda x: re.findall(ptn, re.sub(r'(?<=ABC)\s+', '', x)) if x else [], ArrayType(StringType()))

    df.withColumn('new_column', find_number('My_column')).show()

Notes: 
 (1) since Python `re` module does not support `(?<!ABC\s{0,5})`, so use `re.sub(r'(?<=ABC)\s+', '', x)` 
     to remove all whitespaces after `ABC` as preprocessing.
 (2) Sometime udf function is much easier to maintain, and might out-perform the solution with 
     API functions especially when the API function part is overly complex and the regex pattern 
     contains backtracking like `.*?`


###############
Another example using regexp_replace + split to handle find_all() task:
REF: https://stackoverflow.com/questions/58605549

    from pyspark.sql.functions import split, regexp_replace, expr, coalesce, array

    regexp_extract_all = lambda column: \
        split(regexp_replace(regexp_replace(column, r'.*??\b([A-Z]+[0-9]+[a-z]*)\b', '$1\0'),'\0?[^\x00]*$', ''), '\0'
    )

    df.withColumn('code', coalesce(regexp_extract_all(expr("IF(third like '%*%', second, first)")),array())).show()    
    +-------+--------------------+-------------------+------+---------------+
    |  first|              second|              third|   num|           code|
    +-------+--------------------+-------------------+------+---------------+
    |  AB12a|              xxxxxx|    some other data|100000|        [AB12a]|
    |yyyyyyy|XYZ02, but possib...|Look at second col*|120000|[XYZ02, GFH11b]|
    |   null|                 111|                222|  1233|             []|
    +-------+--------------------+-------------------+------+---------------+


