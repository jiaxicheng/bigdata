https://stackoverflow.com/questions/58595189/convert-multiple-array-of-structs-columns-in-pyspark-sql

using to_json and from_json to save/load dataframe with complex data types:

    pickle_file = /path/to/col_dtypes.pkl
    df_file = '/path/to/save/df_file'


Save Dataframe to CSV file
----------------
Step-1: backup the col_name vs col_dtype mapping, save the data somewhere in the local-HD

    from pyspark.sql.functions import to_json
    import pickle

    with open(pickle_file, 'wb') as fp:
        pickle.dump(df.dtypes, fp)
  
Step-2: convert ArrayType, StructType and MapType into StringType using to_json and write the DF to csv

    df.select([ to_json(d[0]).alias(d[0]) if d[1].startswith(('struct', 'array', 'map')) else d[0] for d in df.dtypes ])\
      .write.csv(df_file, header=True)


Load Dataframe from CSV
-----------------

Step-1: load the column dtype mapping using pickle

    from pyspark.sql.functions import from_json, col
    import pickle

    # get the dtype mapping
    with open(pickle_file, 'wb') as fp:
        col_dtypes = pickle.load(fp)

Step-2: use from_json and the above mapping to convert data back to their original dtypes:

    df = spark.read.csv(df_file, header=True).select([ 
        (from_json(d[0], d[1]) if d[1].startswith(('struct', 'array', 'map')) else col(d[0]).astype(d[1])).alias(d[0])
            for d in col_dtypes 
    ])


Caveat: 
-------
the data types(schema) retrieved from df.dtypes are in simpleString format. If any field-names of 
any StructField contain special characters (i.e. SPACE, dot etc), from_json() function will not work. 
In such case, you might need to save df.schema.jsonValue() and use StructType.fromJson to reconstruct 
the column dtype or manually use backticks to enclose such field names.

Method to overcoe the above Caveat:

Below example to iterate through array/struct and convert IntegerType to DoubleType.
This can be handled using to_json/from_json. for the schema, write a function to handle
the Datatype change, either using str.replace() or re.sub().

    Example from https://stackoverflow.com/questions/58697415

    from pyspark.sql.functions import to_json, from_json
    import re

    dtypes_mapping = df.dtypes

    # simple version
    # will fail if column name contains substring `integer`
    # or column names contains special characters like SPACE, dot
    def convert_dtype(x):
        return x.replace('integer', 'double')
    #'struct<rate_1:double,rate_2:double,rate_3:double,card_fee:double,payment_method:string>'

    # better one to use regex sub:
    # so we enclose the field-name with backticks and
    # no worry to get messed up if any fieldnames contain the searched substring
    """
    regex pattern:
        field_name: (?<=[<,])(?!(?:array|struct|map)<)([^:]+)
          + preceded by '<' or ','
          + not be any of the 'array<', 'struct<', or 'map<' which starts a complex data type
          + at least one [^:]+ substring
        field_type:
          + anything before the next 'comma' or '>'
    """
    def convert_dtype(x):
        mapping = {'integer':'double', 'bigint':'double'}
        return re.sub(
              r'(?<=[<,])(?!(?:array|struct|map)<)([^:]+):([^,>]+)'
            , lambda m: '`{0}`:{1}'.format(m.group(1), mapping.get(m.group(2), m.group(2)))
            , x
        )
    #'struct<`rate_1`:double,`rate_2`:double,`rate_3`:double,`card_fee`:double,`payment_method`:string>'

    df_new = df.select([ 
        from_json(to_json(d[0]), convert_dtype(d[1])) if d[1].startswith(('struct', 'array')) else d[0]
            for d in df.dtypes
    ])


Other notes:
-------
Using YAML file as data DeSer methods:

    pip install pyyaml

    import yaml

    # save df.dtypes
    yaml_file = '/home/hdfs/test/pyspark/dtype.yaml'
    with open(yaml_file, 'w') as fp:
        yaml.safe_dump(df.dtypes, fp)

    # load df.dtypes
    with open(yaml_file, 'r') as fp:
        col_dtypes = yaml.safe_load(fp)


