https://stackoverflow.com/questions/61410305/implement-a-page-rank-algorithm-with-pyspark

Google Page Rank: https://en.wikipedia.org/wiki/PageRank

  PR(u) = (1-d)/N + d*sum(PR(e)/L(e) for e in links)

Where: 
---
* PR(e) is the page rank for link `e`
* L(e) is the number of outdound links associated with link `e`
* d is the dampling factor
* N is total number of links

Sample data:
---
page1 page3
page2 page1
page4 page1
page3 page1
page4 page2
page3 page4

Codes with RDD:

    rdd = sc.textFile('/home/xicheng/test/page_rank-1.txt')

    # this is an RDD reused multiple times and thus we persist it
    links = rdd.map(lambda x: tuple(x.split())).distinct().groupByKey().cache()
    
    #links.mapValues(list).collect()                                                                                    
    ##Out[603]: 
    ##[('page1', ['page3']),
    ## ('page2', ['page1']),
    ## ('page4', ['page1', 'page2']),
    ## ('page3', ['page1', 'page4'])]
    
    # find total number of links
    N = links.keys().count()
    
    # initialize the PR for each link
    ranks = links.mapValues(lambda x: 1/N)
    #ranks.collect()                                                                                                    
    ##Out[604]: [('page1', 0.25), ('page2', 0.25), ('page4', 0.25), ('page3', 0.25)]
    
    #damping_factor = 0.85
    d = 0.85
    
    num_iterations = 10
    
    for _ in range(num_iterations):
        # contributions
        contribs = links.join(ranks) \
            .values() \
            .flatMap(lambda x: [ (e, x[1]/len(x[0])) for e in x[0] ])
        # reset ranks based on contributions
        ranks = contribs.reduceByKey(lambda x,y:x+y) \
            .mapValues(lambda x: (1-d)/N + d*x)
    
    ranks.collect()
    #[('page3', 0.343955717634314),
    # ('page1', 0.35784449614646474),
    # ('page2', 0.11582597531596299),
    # ('page4', 0.18237381090325833)]


Use DataFrame API:

    from pyspark.sql.functions import lit, col, collect_set, sum as fsum

    df = rdd.map(lambda x: tuple(x.split())).toDF(['link','to_link'])

    links = df.groupby('link').agg(collect_set('to_link').alias('pages')).cache()
    ranks = links.selectExpr('link', f'1/{N} as rank')

    # number of links
    N = links.count()
    # damping factor
    d = 0.85

    num_iterations = 10
    for _ in range(num_iterations):
      contribs = links.join(ranks, 'link').selectExpr('explode(pages) as link', 'rank/size(pages) as rank')
      ranks = contribs.groupby('link').agg(fsum('rank').alias('rank')) \
          .withColumn('rank', lit((1.0-d)/N) + d*col('rank'))

    ranks.show()                                                                                                       
    +-----+-------------------+                                                     
    | link|               rank|
    +-----+-------------------+
    |page4|0.18237381090325833|
    |page2|0.11582597531596299|
    |page1|0.35784449614646474|
    |page3|  0.343955717634314|
    +-----+-------------------+

  Note: this is quite slow in terms of calculation.


Use Graphframe:

    from graphframes import GraphFrame

    # for testing purpose, reduce the following config setting
    spark.conf.set("spark.sql.shuffle.partitions", 10)

    df = rdd.map(lambda x: tuple(x.split())).toDF(['link','to_link'])

    edges = df.toDF('src','dst') 
    vertices = edges.selectExpr('src as id').union(edges.select('dst')).distinct()
    g = GraphFrame(vertices, edges)

    # calculate page-rank, the result is the same as Google PageRank without dividing `N` in calculating ranks
    pr = g.pageRank(maxIter=10)

    pr.vertices.show()                                                                                                  
    +-----+------------------+
    |   id|          pagerank|
    +-----+------------------+
    |page1| 1.431377984585859|
    |page2|0.4633039012638521|
    |page3|1.3758228705372562|
    |page4|0.7294952436130334|
    +-----+------------------+

