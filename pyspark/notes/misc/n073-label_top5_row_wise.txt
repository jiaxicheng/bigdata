https://stackoverflow.com/questions/57800266/pyspark-how-to-find-and-convert-top-5-row-values-to-1-and-rest-all-to-0

Given the following sample dataframe, select the top 5 values in each row and mark them as '1' and the rest as '0'
+----+----+----+----+----+----+----+----+----+----+
|  _1|  _2|  _3|  _4|  _5|  _6|  _7|  _8|  _9| _10|
+----+----+----+----+----+----+----+----+----+----+
|0.74| 0.9|0.52|0.85|0.18|0.23| 0.3| 0.0| 0.1|0.07|
|0.11|0.57|0.81|0.81|0.45|0.48|0.86|0.38|0.41|0.45|
|0.03|0.84|0.17|0.96|0.09|0.73|0.25|0.05|0.57|0.66|
| 0.8|0.94|0.06|0.44| 0.2|0.89| 0.9| 1.0|0.48|0.14|
|0.73|0.86|0.68| 1.0|0.78|0.17|0.11|0.19|0.18|0.83|
+----+----+----+----+----+----+----+----+----+----+

This is a transform based on the ranking, similar to the function rank() but is implemented in row-wise.
There are 3 different ways to calculate top-N based on how the duplicated values are treated.
(1) rank(): duplicates will be set as the same rank-number, some rank-number might be missing due to duplicates
(2) dense_rank(): duplicates will be set as the same rank-number, and there is no gap between rank-numbers
(3) row_number(): duplicates will be set different rank-numbers, the order might varies based on how data are feeded

Below listed different methods to find row-wise top-N: rank, dense_rank and row_number:

Data Setup:

    df = spark.createDataFrame([
           (0.74, 0.9, 0.52, 0.85, 0.18, 0.23, 0.3, 0.0, 0.1, 0.07)
         , (0.11, 0.57, 0.81, 0.81, 0.45, 0.48, 0.86, 0.38, 0.41, 0.45)
         , (0.03, 0.84, 0.17, 0.96, 0.09, 0.73, 0.25, 0.05, 0.57, 0.66)
         , (0.8, 0.94, 0.06, 0.44, 0.2, 0.89, 0.9, 1.0, 0.48, 0.14)
         , (0.73, 0.86, 0.68, 1.0, 0.78, 0.17, 0.11, 0.19, 0.18, 0.83)
        ], ['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']
    )

    """ set up columns that matter """
    cols = df.columns

    """ select Top-N in row-wise """
    N = 5


## To calculate top-N based on Rank()

    """ 
     1. create an array columns, use sort_array()
     2. use sort_array() to sort the array and then retrieve the (N+1)th element to a new column `Nth`
     3. set cond = col > Nth, and use Spark SQL IF(cond, 1, 0) to set up the new value
    """
    df.withColumn('arr', F.array(cols))                \       
      .withColumn('Nth', F.sort_array('arr', False)[N]) \
      .selectExpr(*[ 'IF(`{0}` > Nth, 1, 0) as `{0}`'.format(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+


## to calculate the top-N based on dense_rank()

    """ Below code is based on Spark 2.4.0+
    similar to the calculation based on rank(), the only difference is we
    retrieve `Nth` value from the array_distict() before sort_array() step 
    """
    df.withColumn('arr', F.array(cols)) \
      .withColumn('topN', F.sort_array(F.array_distinct('arr'), False)[N]) \
      .selectExpr(*[ 'IF(`{0}` > topN, 1, 0) as `{0}`'.format(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  1|  1|  1|  0|  0|  1|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+


## To calculate the top-N based on Row_number()

    """ Below code requires spark 2.4.0+
    given an array of all cols, need to find idx of top-N values, this can be achieved
    by create an named_struct(val, idx), sort by `val` and take the topN slice() of array 
    containing the idx only. transform from struct(val, idx) -> idx. Then we use array_contain() 
    to set up the IF condition. 
    """
    df.withColumn('arr', F.array(cols))  \
      .withColumn('topN_idx', F.expr("""
            transform(
                slice(sort_array(
                    transform(sequence(0,size(arr)-1), i -> named_struct('val',arr[i], 'idx', i))
                  , False
                )
              , 1
              , {0}
            ), x -> x.idx)
          """.format(N)
        )) \
      .select([F.when(F.array_contains('topN_idx',i),1).otherwise(0).alias(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+

Another similiar topic: Take row-wise ranking:
https://stackoverflow.com/questions/58065950/ranking-columns-in-pyspark-dataframe

    df = spark.createDataFrame([
             (99, 30, 21)
            , (20, 10, 44)
            , (50, 90, 87)
            , (78, 11, 9)
          ], ('A', 'B', 'C')
    )

    cols = df.columns

    # rank, row_number does not make much sense
    df.withColumn('arr', F.array_sort(F.array(cols))).selectExpr('*', '''
        filter(sequence(1,size(arr)), i -> arr[i-1] == A)[0] AS rank_of_A
    ''').show()
    +---+---+---+------------+---------+
    |  A|  B|  C|         arr|rank_of_A|
    +---+---+---+------------+---------+
    | 99| 30| 21|[21, 30, 99]|        3|
    | 20| 10| 44|[10, 20, 44]|        2|
    | 50| 90| 87|[50, 87, 90]|        1|
    | 78| 11|  9| [9, 11, 78]|        3|
    +---+---+---+------------+---------+

    # dense_rank: add array_distinct(), below pesudo-code

        arr = array_sort(array_distinct(array(cols))))

