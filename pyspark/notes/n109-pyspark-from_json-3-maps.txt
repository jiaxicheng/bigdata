https://stackoverflow.com/questions/60658754/can-i-transform-a-complex-json-object-to-multiple-rows-in-a-dataframe-in-azure-d

* convert/explode struct of structs into Rows using explode + from_json functions

Use from_json function to convert StructType into MapType, explode the result and then find desired fields:
    
    from pyspark.sql.functions import explode, from_json, to_json, json_tuple, coalesce

    # the file json-21.txt contains a JSON string from the following stackoverflow link
    # https://stackoverflow.com/questions/60658754
    df = spark.read.json('/home/xicheng/test/json-21.txt', multiLine=True)
    
    df.select(explode(from_json(to_json('data.data.players'),"map<string,string>"))) \
      .select(json_tuple('value', 'locationId', 'id', 'name', 'assets', 'dict').alias('Location', 'Player_ID', 'Player', 'assets', 'dict')) \
      .select('*', explode(from_json(coalesce('assets','dict'),"map<string,struct<isActive:boolean,playlists:string>>"))) \
      .selectExpr(
        'Location', 
        'Player_ID', 
        'Player', 
        'key as Asset_ID', 
        'value.isActive', 
        'explode(from_json(value.playlists, "map<string,string>")) as (Playlist_ID, Playlist_Status)'
      ) \
    .show()
    +--------+---------+--------+--------+--------+------------+---------------+
    |Location|Player_ID|  Player|Asset_ID|isActive| Playlist_ID|Playlist_Status|
    +--------+---------+--------+--------+--------+------------+---------------+
    |someGuid| player_1|someName|assetId1|    true|     someId1|           true|
    |someGuid| player_1|someName|assetId1|    true|someOtherId1|          false|
    |someGuid| player_1|someName|assetId2|    true|     someId1|           true|
    |someGuid| player_2|someName|assetId3|    true|     someId1|           true|
    |someGuid| player_2|someName|assetId3|    true|someOtherId1|          false|
    |someGuid| player_2|someName|assetId4|    true|     someId1|           true|
    +--------+---------+--------+--------+--------+------------+---------------+

Notes:

(1) whenever fieldname is uncertain and the order of the fieldname is not a concern, converting
    StructType into MapType is an option to row-exploding.
(2) json_tuple is perfect for a JSON dictionary when the fieldname is known

Another example: REF: https://stackoverflow.com/questions/60958869
  Below use from_json + to_json to convert StructType into MapType and then retrieve the map_keys
  and filter the dataframe based on the first items of the map_keys array:

    from pyspark.sql.functions import from_json, to_json, map_keys

    # json-24.txt contains the JSON string listed in the above mentioned SO-post
    df = spark.read.json('/home/xicheng/test/json-24.txt',multiLine=True)
    
    # regex pattern provided by OP
    ptn = '([a-z,0-9]+):([a-z,0-9]+):([a-z,0-9]+)'

    df.filter(map_keys(from_json(to_json('info.extract'), 'map<string,string>'))[0].rlike(ptn)).show(10,0)
    +---+--------------------------------------+
    |acc|info                                  |
    +---+--------------------------------------+
    |0  |[[[[Doe, somewhere, Joe]],,]]         |
    |1  |[[, [[Smith, Somewhere_else, Will]],]]|
    |2  |[[,, [[Clapton, Eric]]]]              |
    +---+--------------------------------------+
