Pyspark and RDD:

notes about working with RDD:

Basic:
---
  + SparkContext: 
     * entry point for Spark functionality. A SparkContext represents the connection 
       to a Spark cluster and can be used to create RDD and broadcast variables on that cluster.
  + SparkSession: 
     * entry point to programming Spark with Dataset and Dataframe API
  + SQLContext: 
    * entry point for working with structured data (rows and columns) in Spark 1.x
  + SparkConf: for configuring Spark
  + SparkFiles: access files shipped with jobs
  + TaskContext: 



pyspark.SparkContext:
  + property:
    + applicationId
    + defaultMinPartitions
    + defaultParallelism
    + startTime
    + uiWebUrl
    + version
  + method:
    + accumulator(value, accum_param=None)
    + broadcast(value)
    + create RDD from files:
      + emptyRDD()
      + range(start, end=None, step=1, numSlices=None)
      + parallelize(c, numSlices=None)
        read local Python collection to form an RDD, only useful for testing
      + textFile(path, minPartitions=None, use_unicode=True)
        read files in line-mode for RDD elements, can also read multiple files
      + wholeTextFiles(path, minPartitions=None, use_unicode=True)
        read multiple files and generate the parid-RDD with key=filename, value=file_content
        good for system with many many small files
      + newAPIHadoopFile()
        useful to read user-defined delimiter sub-string to read RDD elements
        the resulting RDD is a paired-RDD with key=offset_of_the_delimiter, value=split_text_block
      + sequenceFile()
      + pickleFile()
    + union(rdds)
      build the union of a list of RDDs
    + stop()
      shutdown the SparkContext



pyspark.RDD:
  + map(f, preservesPartitioning=False)
      * argument in f is RDD element
    mapPartitions(f, preservesPartitioning=False)
      * argument in f is an iterable containing RDD elements
    mapPartitionsWithIndex(preservesPartitioning=False)
    mapValues(f)
    flatMap(f, preservesPartitioning=False)
    flatMapValues(f)
  + reduce(f)
    treeReduce(f, depth=2)
    fold(zeroValue, op)
    aggregate(zeroValue, seqOp, comboOp)
    treeAggregate(zeroValue, seqOp, comboOp, depth=2)
    reduceByKey(func, numPartitions=None, partitionFunc=<>)
    foldByKey(zeroValue, func, numPartitions=None, partitionFunc=<>)
    aggregateByKey(zeroValue, seqFunc, comboFunc, numPartitions=None, partitionFunc=<>)
    combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<>)
      * the following two are the same:
        x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)]) 
        x.combineByKey(lambda x: [x], lambda x,y: x+[y], lambda x,y: x+y ).collect()
        x.mapValues(lambda x: [x]).reduceByKey(lambda x,y: x+y).collect()
    reduceByKeyLocally(func)
  + foreach(f) * execute action on each RDD, no need to return another RDD
    foreachPartition(f)
  + keys()
    values()
    keyBy(f): generate a paird-RDD (f(x), x) where x is the RDD element
    lookup(key)
  + cartesian(other) 
    join(other, numPartitions=None)
    leftOuterJoin(other, numPartitions=None)
    rightOuterJoin(other, numPartitions=None)
    fullOuterJoin(other, numPartitions=None)
    union(other)
    intersection(other)
    subtract(other, numPArtitions=None)
    subtractByKey(other, numPArtitions=None)
  + cogroup(other, numPartitions=None)
    groupBy(f, numPartitions=None, partitionFunc=<>)
    groupByKey(numPartitions=None, partitionFunc=<>)
    groupWith(other, *others)
  + filter(f)
    sortBy(keyfunc, ascending=True, numPartitions=None)
    sortByKey(ascending=True, numPartitions=None,keyfunc=<>)
  + collect()
    take(N)
    takeOrdered(N, key=None)
    first()
    top(N, key=None)
    takeSample(withReplacement, num, seed=None)
  + randomSplit(weights, seed=None)
    sample(withReplacement, fraction, seed=None)
    sampleByKey(withReplacement, fractions, seed=None)
  + coalesce(numPartitions, shuffle=False)
    repartition(numPartitions)
    repartitonAndSortWithinPartitions(numPartitions=None, partitionFunc=<>)
  + stats()
    count()
    countByKey()
    countByValue()
    distinct(numPartition=None)
    max(f): f is a function used to generate key for comparison
    min(f)
    mean()
    sum()
    stdev()
    variance()
    sampleStdev()
    sampleVariance()
    countApproxDistinct(relativeSD=0.05)
    countApprox(timeout, confidence=0.95)
    meanApprox(timeout, confidence=0.95)
    sumApprox(timeout, confidence=0.95)
  + zip(other): 
    zipWithIndex(): index, no-gap, trigger a Spark job
    zipWithUniqueId(): unique-id, create gaps between partitions
  + name(): return the name of this RDD
    setName()
    id()
    glom()
^  + histogram(bucket)
^  + barrier()
    Mark the current stage as a barrier stage where Spark must launch all tasks together.
  + cache()
    persist(storageLevel=StorageLevel(False, True, False, False, 1))
    unpersist()
  + checkpoint()
    localCheckpoint()
  + getCheckpointFile()
    getNumPartitions()
    getStorageLevel()
    toDebugString()
    toLocalIterator()
  + isCheckpointed()
    isEmpty()
    isLocallyCheckpointed()
  + saveAsHadoopDataset
    saveAsHadoopFile
    saveAsNewAPIHadoopDataset
    saveAsNewAPIHadoopFile
    saveAsPickleFile
    saveAsSequenceFile
    saveAsTextFile
  + pipe(cmd, env=None, chekCode=False)

  
Notes:
  (1) treeReduce()/treeAggregate() are doing the same as reduce()/aggregate() except that
      the reduced results from partitions are not directly sent to the driver. they will 
      be combined in intermediate executors based on the tree-depth before sending to 
      the driver. this reduce aggregation time especially on dataset with large number of partitions 
  (2) map/mapPartition and foreach/foreachPartition: 
      + map() is to transform a collection into another collections. foreach() just execute an action on an collection.
      + map() is a transformation, foreach() is an action.
      + map() execute on each RDD element, take Row() object as the function argument
        mapPartition() run on each partition, takes an iterator of Rows as the function argument
  (3) the elements of paird-RDD are always 2-item tuples, if there are more than 2 items, the 3rd item
      and the after will be discarded when using *byKey() options. you can convert such RDD into paired-RDD
      using map() or keyBy() functions
  (4) partition wisely, avoid skewed data, know your data (size, type) and how it's distributed:
      * if you have to load a large unsplitable file, repartition data to increase #partitions after loading.
      * for skewed data, repartition using an appropriate key which can evenly spread the load  

    The following commands are used to count numPartitions and related information:

        #Count total # of partitions:
        rdd.getNumPartitions()

        #Count # of elements in each partition:
        rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()

        #Show partition content: use with caution, only for testing small dataset
        rdd.glom().collect() 



pyspark.Broadcast:
---
  + property:
    + value
  + method:
    + load(file)
      load_from_path(path)
    + dump(value, f)
    + unpersist()
    + destroy()

   Notes: values are read-only and accessible to all workers



pyspark.Accumulator(aid, value, accum_param)
---
  + property:
    + value
  + method:
    + add(val2)
  pyspark.AccumulatorParam
  ---
    + zero(value)
    + addInPlace(value1, value2)

  Notes: 
   (1) value is only available on driver, not accessible to any workers
   (2) By default, SparkContext can only use numeric types(int, float) as the accumulator, users can
       extend the AccumulatorParam to include list, dict etc.
   
       Example-1: extend to List (tested)

           class ListAccumulatorParam(AccumulatorParam):
               def zero(self, v):
                   return []
               def addInPlace(self, acc1, acc2):
                   return acc1 + acc2

           """Set up accumulator"""
           acc = sc.accumulator([], ListAccumulatorParam())

       Example-2: extend to dict (untested)

           class DictAccumulatorParam(AccumulatorParam):
               def zero(self, v):
                   return {}
               def addInPlace(self, acc1, acc2):
                   return {**acc1, **acc2}

           """Set up accumulator"""
           acc = sc.accumulator({}, DictAccumulatorParam())

    https://stackoverflow.com/questions/32056445/issue-with-creating-a-global-list-from-map-using-pyspark
    https://stackoverflow.com/questions/44640184/accumulator-in-pyspark-with-dict-as-global-variable



pyspark.StorageLevel(useDisck, useMemory, useOffHeap, deserialized, replication=1)
---
  + DISK_ONLY = StorageLevel(True, False, False, False, 1)
  + DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)
  + MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)
  + MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)
  + MEMORY_ONLY = StorageLevel(False, True, False, False, 1)
  + MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)
  + OFF_HEAP = StorageLevel(True, True, True, False, 1)

  Notes: deserialized=False for all Python code


Some notes:
  (1) PEP 3113: "tuple parameter unpacking", was removed in Python 3. so the following function is illegal under Python-3

       rdd.map(lambda (x, (y,x)): func(x,y,z) )  --changed to--> rdd.map(lambda x: func(x[0], x[1][0], x[1][1]))



References:
[1] treeReduce and treeAggregate: https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html



