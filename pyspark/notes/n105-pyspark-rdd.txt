Pyspark and RDD:

notes about working with RDD:

Basic:
---
  + SparkContext: 
     * entry point for Spark functionality. A SparkContext represents the connection 
       to a Spark cluster and can be used to create RDD and broadcast variables on that cluster.
  + SparkSession: 
     * entry point to programming Spark with Dataset and Dataframe API
  + SQLContext: 
    * entry point for working with structured data (rows and columns) in Spark 1.x
  + SparkConf: for configuring Spark
  + SparkFiles: access files shipped with jobs
  + TaskContext: 



pyspark.RDD:
  + map(f, preservesPartitioning=False)
      * argument in f is RDD element
    mapPartitions(f, preservesPartitioning=False)
      * argument in f is an iterable containing RDD elements
    mapPartitionsWithIndex(preservesPartitioning=False)
    mapValues(f)
    flatMap(f, preservesPartitioning=False)
    flatMapValues(f)
  + reduce(f)
    treeReduce(f, depth=2)
    fold(zeroValue, op)
    aggregate(zeroValue, seqOp, comboOp)
    treeAggregate(zeroValue, seqOp, comboOp, depth=2)
    reduceByKey(func, numPartitions=None, partitionFunc=<>)
    foldByKey(zeroValue, func, numPartitions=None, partitionFunc=<>)
    aggregateByKey(zeroValue, seqFunc, comboFunc, numPartitions=None, partitionFunc=<>)
    combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<>)
      * the following two are the same:
        x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)]) 
        x.combineByKey(lambda x: [x], lambda x,y: x+[y], lambda x,y: x+y ).collect()
        x.mapValues(lambda x: [x]).reduceByKey(lambda x,y: x+y).collect()
    reduceByKeyLocally(func)
  + foreach(f) * execute action on each RDD, no need to return another RDD
    foreachPartition(f)
  + keys()
    values()
    keyBy(f): generate a paird-RDD (f(x), x) where x is the RDD element
    lookup(key)
  + cartesian(other) 
    join(other, numPartitions=None)
    leftOuterJoin(other, numPartitions=None)
    rightOuterJoin(other, numPartitions=None)
    fullOuterJoin(other, numPartitions=None)
    union(other)
    intersection(other)
    subtract(other, numPArtitions=None)
    subtractByKey(other, numPArtitions=None)
  + cogroup(other, numPartitions=None)
    groupBy(f, numPartitions=None, partitionFunc=<>)
    groupByKey(numPartitions=None, partitionFunc=<>)
    groupWith(other, *others)
  + filter(f)
    sortBy(keyfunc, ascending=True, numPartitions=None)
    sortByKey(ascending=True, numPartitions=None,keyfunc=<>)
  + collect()
    take(N)
    takeOrdered(N, key=None)
    first()
    top(N, key=None)
    takeSample(withReplacement, num, seed=None)
  + randomSplit(weights, seed=None)
    sample(withReplacement, fraction, seed=None)
    sampleByKey(withReplacement, fractions, seed=None)
  + coalesce(numPartitions, shuffle=False)
    repartition(numPartitions)
    repartitonAndSortWithinPartitions(numPartitions=None, partitionFunc=<>)
  + stats()
    count()
    countByKey()
    countByValue()
    distinct(numPartition=None)
    max(f): f is a function used to generate key for comparison
    min(f)
    mean()
    sum()
    stdev()
    variance()
    sampleStdev()
    sampleVariance()
    countApproxDistinct(relativeSD=0.05)
    countApprox(timeout, confidence=0.95)
    meanApprox(timeout, confidence=0.95)
    sumApprox(timeout, confidence=0.95)
  + zip(other): 
    zipWithIndex(): index, no-gap, trigger a Spark job
    zipWithUniqueId(): unique-id, create gaps between partitions
  + name(): return the name of this RDD
    setName()
    id()
    glom()
^  + histogram(bucket)
^  + barrier()
    Mark the current stage as a barrier stage where Spark must launch all tasks together.
  + cache()
    persist(storageLevel=StorageLevel(False, True, False, False, 1))
    unpersist()
  + checkpoint()
    localCheckpoint()
  + getCheckpointFile()
    getNumPartitions()
    getStorageLevel()
    toDebugString()
    toLocalIterator()
  + isCheckpointed()
    isEmpty()
    isLocallyCheckpointed()
  + saveAsHadoopDataset
    saveAsHadoopFile
    saveAsNewAPIHadoopDataset
    saveAsNewAPIHadoopFile
    saveAsPickleFile
    saveAsSequenceFile
    saveAsTextFile
  + pipe(cmd, env=None, chekCode=False)

  
Notes:
  (1) treeReduce()/treeAggregate() are doing the same as reduce()/aggregate() except that
      the reduced results from partitions are not directly sent to the driver. they will 
      be combined in intermediate executors based on the tree-depth before sending to 
      the driver. this reduce aggregation time especially on dataset with large number of partitions 
  (2) map/mapPartition and foreach/foreachPartition: 
      + map() is to transform a collection into another collections. foreach() just execute an action on an collection.
      + map() is a transformation, foreach() is an action.
      + map() execute on each RDD element, take Row() object as the function argument
        mapPartition() run on each partition, takes an iterator of Rows as the function argument
  (3) the elements of paird-RDD are always 2-item tuples, if there are more than 2 items, the 3rd item
      and the after will be discarded when using *byKey() options. you can convert such RDD into paired-RDD
      using map() or keyBy() functions


References:
[1] treeReduce and treeAggregate: https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html



