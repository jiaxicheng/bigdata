Pyspark and RDD:

notes about working with RDD:

Basic:
---
  + SparkContext: 
     * entry point for Spark functionality. A SparkContext represents the connection 
       to a Spark cluster and can be used to create RDD and broadcast variables on that cluster.
  + SparkSession: 
     * entry point to programming Spark with Dataset and Dataframe API
  + SQLContext: 
    * entry point for working with structured data (rows and columns) in Spark 1.x
  + SparkConf: for configuring Spark
  + SparkFiles: access files shipped with jobs
    two methods to retrieve file and path info on each worker
     + get(filename)
     + getRootDirectory() 
  + TaskContext: 



pyspark.SparkContext:
  + property:
    + applicationId
    + defaultMinPartitions
    + defaultParallelism
    + startTime
    + uiWebUrl
    + version
  + method:
    + accumulator(value, accum_param=None)
    + broadcast(value)
    + create RDD from files:
      + emptyRDD()
      + range(start, end=None, step=1, numSlices=None)
      + parallelize(c, numSlices=None)
        read local Python collection to form an RDD, only useful for testing
      + textFile(path, minPartitions=None, use_unicode=True)
        read files in line-mode for RDD elements, can also read multiple files
      + wholeTextFiles(path, minPartitions=None, use_unicode=True)
        read multiple files and generate the parid-RDD with key=filename, value=file_content
        good for system with many many small files
      + newAPIHadoopFile()
        useful to read user-defined delimiter sub-string to read RDD elements
        the resulting RDD is a paired-RDD with key=offset_of_the_delimiter, value=split_text_block
      + sequenceFile()
      + pickleFile()
    + addFile(path, recursive=False)
      addPyFile(path)
       Notes: 
        (1) add a file to be downloaded with spark job on every node.
        (2) to fetch the file use the following code:

               from pyspark import SparkFiles
               sc.addFile('/local/path/' + 'test.txt')
               with open(SparkFiles.get('test.txt'), 'rb') as fp:
                   # do something on fp.readline()

    + union(rdds)
      build the union of a list of RDDs
    + stop()
      shutdown the SparkContext



pyspark.RDD:
  + map(f, preservesPartitioning=False)
      * argument in f is RDD element
    mapPartitions(f, preservesPartitioning=False)
      * f(it): argument it is an iterable containing RDD elements
    mapPartitionsWithIndex(preservesPartitioning=False)
      * f(idx, it) where idx is the `spark_partition_id
    mapValues(f)
    flatMap(f, preservesPartitioning=False)
    flatMapValues(f)
  + reduce(f)
    treeReduce(f, depth=2)
    fold(zeroValue, op)
    aggregate(zeroValue, seqOp, comboOp)
    treeAggregate(zeroValue, seqOp, comboOp, depth=2)
    reduceByKey(func, numPartitions=None, partitionFunc=<>)
    foldByKey(zeroValue, func, numPartitions=None, partitionFunc=<>)
    aggregateByKey(zeroValue, seqFunc, comboFunc, numPartitions=None, partitionFunc=<>)
    combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<>)
      * the following two are the same:
        x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)]) 
        x.combineByKey(lambda x: [x], lambda x,y: x+[y], lambda x,y: x+y ).collect()
        x.mapValues(lambda x: [x]).reduceByKey(lambda x,y: x+y).collect()
    reduceByKeyLocally(func)
  + foreach(f) * execute action on each RDD, no need to return another RDD
    foreachPartition(f)
  + keys()
    values()
    keyBy(f): generate a paird-RDD (f(x), x) where x is the RDD element
    lookup(key)
  + cartesian(other) 
    join(other, numPartitions=None)
    leftOuterJoin(other, numPartitions=None)
    rightOuterJoin(other, numPartitions=None)
    fullOuterJoin(other, numPartitions=None)
    union(other)
    intersection(other)
    subtract(other, numPArtitions=None)
    subtractByKey(other, numPArtitions=None)
  + cogroup(other, numPartitions=None)
    groupBy(f, numPartitions=None, partitionFunc=<>)
    groupByKey(numPartitions=None, partitionFunc=<>)
    groupWith(other, *others)
  + filter(f)
    sortBy(keyfunc, ascending=True, numPartitions=None)
    sortByKey(ascending=True, numPartitions=None,keyfunc=<>)
  + collect()
    take(N)
    takeOrdered(N, key=None)
    first()
    top(N, key=None)
    takeSample(withReplacement, num, seed=None)
  + randomSplit(weights, seed=None)
    sample(withReplacement, fraction, seed=None)
    sampleByKey(withReplacement, fractions, seed=None)
  + coalesce(numPartitions, shuffle=False)
    repartition(numPartitions)
    repartitonAndSortWithinPartitions(numPartitions=None, partitionFunc=<>)
  + stats()
    count()
    countByKey()
    countByValue()
    distinct(numPartition=None)
    max(f): f is a function used to generate key for comparison
    min(f)
    mean()
    sum()
    stdev()
    variance()
    sampleStdev()
    sampleVariance()
    countApproxDistinct(relativeSD=0.05)
    countApprox(timeout, confidence=0.95)
    meanApprox(timeout, confidence=0.95)
    sumApprox(timeout, confidence=0.95)
  + zip(other): not_very_useful
    zipWithIndex(): index, no-gap, trigger a Spark job
    zipWithUniqueId(): unique-id, create gaps between partitions
  + name(): return the name of this RDD
    setName()
    id()
    glom()
    isEmpty()
^  + histogram(bucket)
^  + barrier()
    Mark the current stage as a barrier stage where Spark must launch all tasks together.
  + cache()
    persist(storageLevel=StorageLevel(False, True, False, False, 1))
    unpersist()
  + checkpoint()
    localCheckpoint()
  + getCheckpointFile()
    getNumPartitions()
    getStorageLevel()
    toDebugString()
    toLocalIterator()
      ** Notes: this also exists as df.toLocalIterator(), it brings all Rows to driver as iterator 
       by partitions (need RAM to hold max-sized partition) useful in debugging when collect() 
       is often used. check also `df.explain()`
  + isCheckpointed()
    isLocallyCheckpointed()
  + saveAsHadoopDataset
    saveAsHadoopFile
    saveAsNewAPIHadoopDataset
    saveAsNewAPIHadoopFile
    saveAsPickleFile
    saveAsSequenceFile
    saveAsTextFile
  + pipe(cmd, env=None, chekCode=False)

Notes:
  (1) treeReduce()/treeAggregate() are doing the same as reduce()/aggregate() except that
      the reduced results from partitions are not directly sent to the driver. they will 
      be combined in intermediate executors based on the tree-depth before sending to 
      the driver. this reduce aggregation time especially on dataset with large number of partitions 
  (2) map/mapPartition and foreach/foreachPartition: 
      + map() is to transform a collection into another collections. foreach() just execute an action on an collection.
      + map() is a transformation, foreach() is an action.
      + map() execute on each RDD element, take Row() object as the function argument
        mapPartition() run on each partition, takes an iterator of Rows as the function argument
  (3) the elements of paird-RDD are always 2-item tuples, if there are more than 2 items, the 3rd item
      and the after will be discarded when using *byKey() options. you can convert such RDD into paired-RDD
      using map() or keyBy() functions
  (4) partition wisely, avoid skewed data, know your data (size, type) and how it's distributed:
      * if you have to load a large unsplitable file, repartition data to increase #partitions after loading.
      * for skewed data, repartition using an appropriate key which can evenly spread the load  

    The following commands are used to count numPartitions and related information:

        #Count total # of partitions:
        rdd.getNumPartitions()

        #Count # of elements in each partition:
        rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()

        #Show partition content: use with caution, only for testing small dataset
        rdd.glom().collect() 

  (5) in map-related functions, preservesPartitioning is only useful for pair-RDDs.
      when the function might change the key thus potentially modify the partitions.
      keep the default 'false' unless you are dealing with pair-RDD and the functions
      changed the first item of the tuple.
  (6) groupBy example to find top-N elements of each key:
       
        from heapq import nlargest
        rdd.groupBy(lambda x: x[0]).flatMap(lambda x: nlargest(3, x[1], key=lambda x: x[2]))



pyspark.Broadcast:
---
  + property:
    + value
  + method:
    + load(file)
      load_from_path(path)
    + dump(value, f)
    + unpersist()
    + destroy()

   Notes: values are read-only and accessible to all workers



pyspark.Accumulator(aid, value, accum_param)
---
  + property:
    + value
  + method:
    + add(val2)
  pyspark.AccumulatorParam
  ---
    + zero(value)
    + addInPlace(value1, value2)

  Notes: 
   (1) value is only available on driver, not accessible to any workers
   (2) By default, SparkContext can only use numeric types(long, double, float) as the accumulator, users can
       extend the AccumulatorParam to include list, dict etc.
   
       Example-1: extend to List (tested)

           class ListAccumulatorParam(AccumulatorParam):
               def zero(self, v):
                   return []
               def addInPlace(self, acc1, acc2):
                   return acc1 + acc2

           """Set up accumulator"""
           acc = sc.accumulator([], ListAccumulatorParam())

       Example-2: extend to dict (untested)

           class DictAccumulatorParam(AccumulatorParam):
               def zero(self, v):
                   return {}
               def addInPlace(self, acc1, acc2):
                   return {**acc1, **acc2}

           """Set up accumulator"""
           acc = sc.accumulator({}, DictAccumulatorParam())

    https://stackoverflow.com/questions/32056445/issue-with-creating-a-global-list-from-map-using-pyspark
    https://stackoverflow.com/questions/44640184/accumulator-in-pyspark-with-dict-as-global-variable



pyspark.StorageLevel(useDisck, useMemory, useOffHeap, deserialized, replication=1)
---
  + DISK_ONLY = StorageLevel(True, False, False, False, 1)
  + DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)
  + MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)
  + MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)
  + MEMORY_ONLY = StorageLevel(False, True, False, False, 1)
  + MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)
  + OFF_HEAP = StorageLevel(True, True, True, False, 1)

  Notes: 
   (1) Since the data is always serialized on the Python side, all the constants use the serialized formats
   (2) deserialized=False for all Python storages. This flag to check whether to keep the data in memory 
       in a JAVA-specific serialized format


Some notes:
  (1) PEP 3113: "tuple parameter unpacking", was removed in Python 3. so the following function is illegal under Python-3

       rdd.map(lambda (x, (y,x)): func(x,y,z) )  --changed to--> rdd.map(lambda x: func(x[0], x[1][0], x[1][1]))



References:
[1] treeReduce and treeAggregate: https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html



