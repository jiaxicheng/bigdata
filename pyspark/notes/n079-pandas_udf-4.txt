Continue..

Pandas UDFs and Pandas function APIs
---

Example-13: evaluate functions in a column using values from another dataframe
  REF:https://stackoverflow.com/questions/63868572/pyspark-evaluate-formula

  Step-0: setup two dataframes:

    from pyspark.sql.functions import expr, pandas_udf
    from pandas import Series
    import re

    df = spark.createDataFrame([
       (1,"(a/(b+c.12))*100"),(2,"m/n*100"),(3,"d"),(4,"max([a,b,12])"),(5,"a>b"), 
       (6,"2*sin(pi/2)"), (7, "date_add('h',2)")
    ],["ID", "Formula"])

    df_val = spark.createDataFrame([("a","4"),("b","3"),("c.12","8"),("d","7"),("m","2"),("n","5"),("h","2020-09-01")], ["ID", "Value"])

  Step-1: tokenize df.Formula
    # convert all dot preceded by a letter to underscore, then split the string with non-words 
    # remove NULL and EMPTY elements from the resulting array `tokens`

    df1 = df.withColumn('Formula_tmp', expr("regexp_replace(Formula, '(?<=[A-Za-z])\\\.', '_')")) \
        .selectExpr("*", "filter(split(Formula_tmp,'\\\W+'), x -> nullif(x,'') is not NULL) as tokens")
    +---+----------------+----------------+-----------------+
    | ID|         Formula|     Formula_tmp|           tokens|
    +---+----------------+----------------+-----------------+
    |  1|(a/(b+c.12))*100|(a/(b+c_12))*100|[a, b, c_12, 100]|
    |  2|         m/n*100|         m/n*100|      [m, n, 100]|
    |  3|               d|               d|              [d]|
    |  4|   max([a,b,12])|   max([a,b,12])|  [max, a, b, 12]|
    |  5|             a>b|             a>b|           [a, b]|
    |  6|     2*sin(pi/2)|     2*sin(pi/2)|  [2, sin, pi, 2]|
    |  7| date_add('h',2)| date_add('h',2)| [date_add, h, 2]|
    +---+----------------+----------------+-----------------+


  Step-2: left-join with df_val and find all ID/Value that are used in Formula and make them a Map
    # Note: for df_val.Formula, convert all dot preceded by a letter to underscore, rename it to `var`

    df2 = df1.join(
      df_val.withColumn("ID", expr("regexp_replace(ID, '(?<=[A-Za-z])\\\.', '_')")).withColumnRenamed("ID", "var"), 
      expr("array_contains(tokens, var)"), 
      "left"
    ).groupby("ID").agg( 
      expr('first(Formula) as Formula'), 
      expr('first(Formula_tmp) as Formula_tmp'), 
      expr('collect_list(array(var, string(Value))) as map1')
    )

    df2.show(truncate=False)
    +---+----------------+----------------+---------------------------+             
    |ID |Formula         |Formula_tmp     |map1                       |
    +---+----------------+----------------+---------------------------+
    |7  |date_add('h',2) |date_add('h',2) |[[h, 2020-09-01]]          |
    |6  |2*sin(pi/2)     |2*sin(pi/2)     |[[,]]                      |
    |5  |a>b             |a>b             |[[a, 4], [b, 3]]           |
    |1  |(a/(b+c.12))*100|(a/(b+c_12))*100|[[a, 4], [b, 3], [c_12, 8]]|
    |3  |d               |d               |[[d, 7]]                   |
    |2  |m/n*100         |m/n*100         |[[m, 2], [n, 5]]           |
    |4  |max([a,b,12])   |max([a,b,12])   |[[a, 4], [b, 3]]           |
    +---+----------------+----------------+---------------------------+

  Step-3: create a pandas_udf to `eval()` Formula

 # for Spark 2.3+, PandasUDFType.SCALAR
  @pandas_udf("string") 
    def eval_formula(formula, aoa): 
      from math import sin, pi, exp 
      from datetime import datetime, timedelta
      date_add = lambda x,y: (datetime.fromisoformat(x) + timedelta(days=y)).strftime("%Y-%m-%d")
      s = [] 
      for f,a in zip(formula, aoa): 
        d = dict(a) 
        try: 
          s.append(str(eval(re.sub(r'\w+', lambda m: str(d.get(m.group(0),m.group(0))), f)))) 
        except Exception as e: 
          s.append(str(e)) 
      return Series(s) 

  # for Spark 3.0+ Iterator of Multiple Series -> Iterator of Series
    from typing import Iterator, Tuple
    @pandas_udf("string") 
    def eval_formula(iterator: Iterator[Tuple[Series,Series]]) -> Iterator[Series]:
      from math import sin, pi, exp 
      from datetime import datetime, timedelta
      date_add = lambda x,y: (datetime.fromisoformat(x) + timedelta(days=y)).strftime("%Y-%m-%d")
      for formula, aoa in iterator: 
        s = [] 
        for f,a in zip(formula, aoa): 
          d = dict(a) 
          try: 
            s.append(str(eval(re.sub(r'\w+', lambda m: str(d.get(m.group(0),m.group(0))), f)))) 
          except Exception as e: 
            s.append(str(e)) 
        yield Series(s) 

  Step-4: run the pandas_udf function

    df2.withColumn('result', eval_formula('Formula_tmp', 'map1')) \
        .drop('Formula_tmp') \
        .show(truncate=False)
    +---+----------------+---------------------------+-----------------+            
    |ID |Formula         |map1                       |result           |
    +---+----------------+---------------------------+-----------------+
    |7  |date_add('h',2) |[[h, 2020-09-01]]          |2020-09-03       |
    |6  |2*sin(pi/2)     |[[,]]                      |2.0              |
    |5  |a>b             |[[a, 4], [b, 3]]           |True             |
    |1  |(a/(b+c.12))*100|[[a, 4], [b, 3], [c_12, 8]]|36.36363636363637|
    |3  |d               |[[d, 7]]                   |7                |
    |2  |m/n*100         |[[m, 2], [n, 5]]           |40.0             |
    |4  |max([a,b,12])   |[[a, 4], [b, 3]]           |12               |
    +---+----------------+---------------------------+-----------------+

  Notes: 
   (1) all functions used in Formula must be imported from or defined inside the pandas_udf (when import from the 
       main program it yields NameError). the same situation applied also to udf funciton and rdd.map
       and rdd.mapPartitions methods.
        +---+----------------+---------------------------+------------------------------+
        |ID |Formula         |map1                       |result                        |
        +---+----------------+---------------------------+------------------------------+
        |7  |date_add('h',2) |[[h, 2020-09-01]]          |name 'date_add' is not defined|
        |6  |2*sin(pi/2)     |[[,]]                      |name 'sin' is not defined     |
        |5  |a>b             |[[a, 4], [b, 3]]           |True                          |
        |1  |(a/(b+c.12))*100|[[a, 4], [b, 3], [c_12, 8]]|36.36363636363637             |
        |3  |d               |[[d, 7]]                   |7                             |
        |2  |m/n*100         |[[m, 2], [n, 5]]           |40.0                          |
        |4  |max([a,b,12])   |[[a, 4], [b, 3]]           |12                            |
        +---+----------------+---------------------------+------------------------------+

   (2) force the return type to be str() or it yields `pyarrow.lib.ArrowInvalid:` Error. using string as return type
       if more flexible so to capture the Exception message, use astype("decimal(10,2)") after running the 
       pandas_udf function

   (3) map1 used in pandas_udf function is an array of arrays, MapType is not allowed in pandas_udf both input argument
       and return values. there is an array element of `[,]` which has NULL var, to skip this element use nvl2 function:
       `nvl2(var,array(var, string(Value)),null)` to replace `array(var, string(Value))`



Example-14: Google Translate ajax API implementation class <-- use Iterator[pd.Series]
  REF: https://stackoverflow.com/questions/61140263
  Method: A typical example using pandas_udf with Iterator[Series] to Iterator[Series] when 
          we have to create an instance of Translator to use Google Translator API
    
    df = spark.createDataFrame([
      ("Das h?tte ich gerne als Poster",),
      ("Sooo schoen",),
      ("durchaus m?glich",)
    ],['sentence'])
    
    from googletrans import Translator
    from pyspark.sql.functions import pandas_udf
    import pandas as pd
    from typing import Iterator
    
    @pandas_udf("string")
    def text_translate(iterator:Iterator[pd.Series]) -> Iterator[pd.Series]:
      translator = Translator()
      for s in iterator:
        yield pd.Series([ translator.translate(x, dest='en', src='auto').text for x in s ])
    
    df.withColumn('new_sentenct', text_translate('sentence')).show(10,0)
    +------------------------------+-----------------------------+
    |sentence                      |new_sentenct                 |
    +------------------------------+-----------------------------+
    |Das h?tte ich gerne als Poster|I would like that as a poster|
    |Sooo schoen                   |Sooo nice                    |
    |durchaus m?glich              |quite possible               |
    +------------------------------+-----------------------------+



Example-15: split array into chunks and get the list of the sums on these chucks
  REF: https://stackoverflow.com/q/64873861/9510729
  Task: 
    (1) isolate each fileds from json_string using json_tuple or from_json
    (2) retrieve indices and values from an JSON string using pd.Series.findall
    (3) use np.zeros to help reindex the np.array to 144 elements
    (4) use np_array_split, np_sum(axis=1) to split np.array to 24 chunks and then do sum

  Code: for Spark 2.3.0
    
    from pyspark.sql import functions as F
    
    j1 = """{"name": "abc1", "usage_count": {"bin102": 1, "bin103": 1, "bin104": 1, "bin105": 1, "bin110": 1, "bin112": 
        1, "bin120": 1, "bin121": 1, "bin122": 1, "bin123": 1, "bin124": 1, "bin136": 2, "bin137": 1, "bin138": 1, "bin139":
         1, "bin140": 1, "bin141": 2, "bin142": 2}, "usage_min": {"bin102": 7.7, "bin103": 10, "bin104": 10, "bin105": 2.5, 
        "bin110": 0.1, "bin112": 0.8, "bin120": 6.8, "bin121": 10, "bin122": 10, "bin123": 10, "bin124": 4.3, "bin136": 2.5,
         "bin137": 10, "bin138": 10, "bin139": 10, "bin140": 10, "bin141": 9.3, "bin142": 3.8}, "p_name": "abc_1"}
    """
    df = spark.createDataFrame([(j1,),(None,)],['e_data'])

    cols = ["name", "p_name", "usage_count", "usage_min"]

    df1 = df.select(F.json_tuple("e_data", *cols).alias(*cols))
    df1.printSchema()
    #root
    # |-- name: string (nullable = true)
    # |-- p_name: string (nullable = true)
    # |-- usage_count: string (nullable = true)
    # |-- usage_min: string (nullable = true)

    # set up pandas_udf:
    def _pandas_bin_sum(s,N):
      ret = []
      for x in map(np.array, s.str.findall(r'"bin(\d+)":([\d.]+)')):
        try: 
          z = np.zeros(144)
          z[x[:,0].astype(np.int)] = x[:,1].astype(np.float)
          ret.append([ float(e) for e in np.sum(np.array_split(z,N),axis=1) ])
        except:
          ret.append(None)
      return pd.Series(ret)

    pandas_bin_sum = F.pandas_udf(lambda x: _pandas_bin_sum(x,24), "array<float>")

    df1.withColumn('usage_count', F.concat_ws(',', pandas_bin_sum('usage_count').astype("array<int>"))) \
        .withColumn('usage_min', F.concat_ws(',', pandas_bin_sum('usage_min'))) \
        .show(2,100,vertical=True)
    -RECORD 0----------------------------------------------------------------------------------------------------------
     name        | abc1                                                                                                
     p_name      | abc_1                                                                                               
     usage_count | 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,2,0,5,0,3,7                                                     
     usage_min   | 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,30.2,0.9,0.0,41.1,0.0,12.5,43.1 
    -RECORD 1----------------------------------------------------------------------------------------------------------
     name        | null                                                                                                
     p_name      | null                                                                                                
     usage_count |                                                                                                     
     usage_min   |



Example-16: Bucketizer per group using pandas_udf
  REF: https://stackoverflow.com/q/64938114/9510729
  Task: use applyInPandas including function with more 3 arguments, grouping-key, dataframe and external Broadcvast value
  Code:

    from pyspark.sql import functions as F
    import pandas as pd

    df = spark.createDataFrame([
        ('A37', 'Histogram.ratio', 1, 0.20), 
        ('A37', 'Histogram.ratio', 20, 0.34), 
        ('A37', 'Histogram.ratio', 50, 0.04), 
        ('A37', 'Histogram.ratio', 500, 0.13), 
        ('A37', 'Histogram.ratio', 2000, 0.05), 
        ('A37', 'Histogram.ratio', 9999, 0.32), 
        ('A49', 'Histogram.ratio', 1, 0.50), 
        ('A49', 'Histogram.ratio', 20, 0.24), 
        ('A49', 'Histogram.ratio', 25, 0.09), 
        ('A49', 'Histogram.ratio', 55, 0.12), 
        ('A49', 'Histogram.ratio', 120, 0.06), 
        ('A49', 'Histogram.ratio', 300, 0.08)
    ], schema='instance string, name string, value int, percentage float')

    # set up Bucket Map to broadcast value
    bucket_map = spark.sparkContext.broadcast({"A37":[0,30,1000,5000,9000], "A49":[0,10,30,80,998]})

    # set up return_schema
    schema="instance:string,name:string,value:array<string>,percentage:float"

    def bucket_sum(k: tuple, pdf: pd.DataFrame) -> pd.DataFrame:
      def _bucket_sum(k: tuple, pdf: pd.DataFrame, map:dict) -> pd.DataFrame:
        cut_bins = pd.cut(pdf['value'], bins=map[k[0]]).astype(str)
        return pdf.groupby(['instance', 'name', cut_bins])['percentage'].sum().reset_index()
      return _bucket_sum(k, pdf, bucket_map.value)

    df.groupBy("instance", "name").applyInPandas(bucket_sum, schema).show()
    +--------+---------------+----------------+----------+                          
    |instance|           name|           value|percentage|
    +--------+---------------+----------------+----------+
    |     A37|Histogram.ratio|     (0.0, 30.0]|      0.54|
    |     A37|Histogram.ratio|(1000.0, 5000.0]|      0.05|
    |     A37|Histogram.ratio|  (30.0, 1000.0]|0.16999999|
    |     A37|Histogram.ratio|             nan|      0.32|
    |     A49|Histogram.ratio|         (0, 10]|       0.5|
    |     A49|Histogram.ratio|        (10, 30]|0.32999998|
    |     A49|Histogram.ratio|        (30, 80]|      0.12|
    |     A49|Histogram.ratio|       (80, 998]|      0.14|
    +--------+---------------+----------------+----------+

  Notes:
   (1) pd.cut() return an CategoricalData which can not be accepted by Spark directly, need to convert it to string:

         pd.cut(...).astype(str)

   (2) the first argument k: is a tuple, we refer to value by k[0], k[1] where k[0] is used to retrieve 
       Map value: `map[k[0]]`


