Continue..

Spark SQL higher-order functions:

(16) Use aggregate function to find first Row matching a condition:
"""
  REF: https://stackoverflow.com/questions/59548034

  Task: for each group, sort the Rows based on the start_date. set the `end_date` to the first `start_date` which has
        the accumulated sum of used_stock >= `available stock`. if this value does not exit, set end_date to null.

  for aggregate functiom: 

      aggregate(array_argument, zero_expression, merge, finish)
    
  we will implement the logic in the above merge and finish expressions:
"""
    from pyspark.sql.functions import collect_list, array_sort, struct, to_date
    from pyspark.sql import Window
   
    # set the WindowSpec from the current Row to the last Row in the same group
    w1 = Window.partitionBy('group').orderBy('start_date').rowsBetween(0, Window.unboundedFollowing)
    
    # convert the start_date to DateType, find sorted collect_list of stryct('start_date', 'used_stock')
    # use aggregate function to calculate the end_date
    df.withColumn('start_date', to_date('start_date', 'dd/MM/yyyy')) \
      .withColumn('data', array_sort(collect_list(struct('start_date','used_stock')).over(w1))) \
        .selectExpr(
            "group",
            "start_date",
            "`available stock`",
            "used_stock",
            """
              aggregate(
                /* argument */
                data,
                /* zero expression, with datatype of 'struct<end_date:data,total:double>' */
                (date(NULL) as end_date, double(0) as total),
                /* merge: use acc.total to save accumulated sum of used_stock */
                (acc, y) ->
                  IF(acc.total >= `available stock`
                  ,  (acc.end_date as end_date, acc.total as total)
                  ,  (y.start_date as end_date, acc.total + y.used_stock as total)
                  ),
                /* finish */
                z -> IF(z.total >= `available stock`, z.end_date, NULL)
              ) as end_date
            """
        ).show(truncate=False) 
    +-------+----------+---------------+----------+----------+                      
    |group  |start_date|available stock|used_stock|end_date  |
    +-------+----------+---------------+----------+----------+
    |group 1|2019-12-01|100            |80        |2019-12-15|
    |group 1|2019-12-08|60             |10        |2019-12-22|
    |group 1|2019-12-15|60             |10        |2019-12-22|
    |group 1|2019-12-22|150            |200       |2019-12-22|
    |group 2|2019-12-15|80             |90        |2019-12-15|
    |group 2|2019-12-22|150            |30        |null      |
    |group 3|2019-12-22|50             |50        |2019-12-22|
    +-------+----------+---------------+----------+----------+

  Note: for groups with a large collect_list, this might be inefficient, change w1 rowsBetween(0, Window.unboundedFollowing)
        to rowsBetween(0,N) where N is a number which can cover most of the cases, and then do a second round calculation
        for Row with end_date = NULL using the WindowSpec cover the full length.

    from pyspark.sql.functions import collect_list, array_sort, struct, to_date, col, when, expr

    end_date_sql_expr = """
        aggregate(
          /* argument */
          data,
          /* zero expression, with datatype of 'struct<end_date:data,total:double>' */
          (date(NULL) as end_date, double(0) as total),
          /* merge: use acc.total to save accumulated sum of used_stock */
          (acc, y) ->
            IF(acc.total >= `available stock`
            ,  (acc.end_date as end_date, acc.total as total)
            ,  (y.start_date as end_date, acc.total + y.used_stock as total)
            ),
          /* finish */
          z -> IF(z.total >= `available stock`, z.end_date, NULL)
        )
     """

    # 1st scan up to the N following rows which can cover majority of end_date which satisfying the condition
    N = 1
    w2 = Window.partitionBy('group').orderBy('start_date').rowsBetween(0, N)

    # 2nd scan will cover the full length but only to rows with end_date is NULL
    w1 = Window.partitionBy('group').orderBy('start_date').rowsBetween(0, Window.unboundedFollowing)

    df.withColumn('start_date', to_date('start_date', 'dd/MM/yyyy')) \
        .withColumn('data', array_sort(collect_list(struct('start_date','used_stock')).over(w2))) \
        .withColumn('end_date', expr(end_date_sql_expr)) \
        .withColumn('data',
           when(col('end_date').isNull(), array_sort(collect_list(struct('start_date','used_stock')).over(w1)))) \
        .selectExpr(
           "group",
           "start_date",
           "`available stock`",
           "used_stock",
           "IF(end_date is NULL, {0}, end_date) AS end_date_1".format(end_date_sql_expr)
        ).show(truncate=False)
    +-------+----------+---------------+----------+----------+                      
    |group  |start_date|available stock|used_stock|end_date_1|
    +-------+----------+---------------+----------+----------+
    |group 1|2019-12-01|100            |80        |2019-12-15|
    |group 1|2019-12-08|60             |10        |2019-12-22|
    |group 1|2019-12-15|60             |10        |2019-12-22|
    |group 1|2019-12-22|150            |200       |2019-12-22|
    |group 2|2019-12-15|80             |90        |2019-12-15|
    |group 2|2019-12-22|150            |30        |null      |
    |group 3|2019-12-22|50             |50        |2019-12-22|
    +-------+----------+---------------+----------+----------+

    

