Continue..

Spark SQL higher-order functions:

(16) Use aggregate function to find first Row matching a condition:
"""
  REF: https://stackoverflow.com/questions/59548034

  Task: for each group, sort the Rows based on the start_date. set the `end_date` to the first `start_date` which has
        the accumulated sum of used_stock >= `available stock`. if this value does not exit, set end_date to null.

  for aggregate functiom: 

      aggregate(array_argument, zero_expression, merge, finish)
    
  we will implement the logic in the above merge and finish expressions:
"""
    from pyspark.sql.functions import collect_list, struct, to_date
    from pyspark.sql import Window
   
    df = spark.read.csv('/home/xicheng/test/groupby-3.txt', header=True, inferSchema=True)

    # set the WindowSpec from the current Row to the last Row in the same group
    w1 = Window.partitionBy('group').orderBy('start_date').rowsBetween(0, Window.unboundedFollowing)
    
    # convert the start_date to DateType, find sorted collect_list of stryct('start_date', 'used_stock')
    # use aggregate function to calculate the end_date
    df.withColumn('start_date', to_date('start_date', 'dd/MM/yyyy')) \
      .withColumn('data', collect_list(struct('start_date','used_stock')).over(w1)) \
        .selectExpr(
            "group",
            "start_date",
            "`available stock`",
            "used_stock",
            """
              aggregate(
                /* argument */
                data,
                /* zero expression, with datatype of 'struct<end_date:data,total:double>' */
                (date(NULL) as end_date, double(0) as total),
                /* merge: use acc.total to save accumulated sum of used_stock */
                (acc, y) ->
                  IF(acc.total < `available stock`
                  ,  (y.start_date as end_date, acc.total + y.used_stock as total)
                  ,  acc
                  ),
                /* finish */
                z -> IF(z.total >= `available stock`, z.end_date, NULL)
              ) as end_date
            """
        ).show(truncate=False) 
    +-------+----------+---------------+----------+----------+                      
    |group  |start_date|available stock|used_stock|end_date  |
    +-------+----------+---------------+----------+----------+
    |group 1|2019-12-01|100            |80        |2019-12-15|
    |group 1|2019-12-08|60             |10        |2019-12-22|
    |group 1|2019-12-15|60             |10        |2019-12-22|
    |group 1|2019-12-22|150            |200       |2019-12-22|
    |group 2|2019-12-15|80             |90        |2019-12-15|
    |group 2|2019-12-22|150            |30        |null      |
    |group 3|2019-12-22|50             |50        |2019-12-22|
    +-------+----------+---------------+----------+----------+

  Note: for groups with a large collect_list, this might be inefficient, change w1 rowsBetween(0, Window.unboundedFollowing)
        to rowsBetween(0,N) where N is a number which can cover most of the cases, and then do a second round calculation
        for Row with end_date = NULL using the WindowSpec cover the full length.

    from pyspark.sql.functions import collect_list, struct, to_date, col, when, expr

    end_date_sql_expr = """
        aggregate(
          /* argument */
          data,
          /* zero expression, with datatype of 'struct<end_date:data,total:double>' */
          (date(NULL) as end_date, double(0) as total),
          /* merge: use acc.total to save accumulated sum of used_stock */
          (acc, y) ->
            IF(acc.total >= `available stock`
            ,  (acc.end_date as end_date, acc.total as total)
            ,  (y.start_date as end_date, acc.total + y.used_stock as total)
            ),
          /* finish */
          z -> IF(z.total >= `available stock`, z.end_date, NULL)
        )
     """

    # 1st scan up to the N following rows which can cover majority of end_date which satisfying the condition
    N = 1
    w2 = Window.partitionBy('group').orderBy('start_date').rowsBetween(0, N)

    # 2nd scan will cover the full length but only to rows with end_date is NULL
    w1 = Window.partitionBy('group').orderBy('start_date').rowsBetween(0, Window.unboundedFollowing)

    df.withColumn('start_date', to_date('start_date', 'dd/MM/yyyy')) \
        .withColumn('data', collect_list(struct('start_date','used_stock')).over(w2)) \
        .withColumn('end_date', expr(end_date_sql_expr)) \
        .withColumn('data',
           when(col('end_date').isNull(), collect_list(struct('start_date','used_stock')).over(w1))) \
        .selectExpr(
           "group",
           "start_date",
           "`available stock`",
           "used_stock",
           "IF(end_date is NULL, {0}, end_date) AS end_date".format(end_date_sql_expr)
        ).show(truncate=False)
    +-------+----------+---------------+----------+----------+                      
    |group  |start_date|available stock|used_stock|end_date_1|
    +-------+----------+---------------+----------+----------+
    |group 1|2019-12-01|100            |80        |2019-12-15|
    |group 1|2019-12-08|60             |10        |2019-12-22|
    |group 1|2019-12-15|60             |10        |2019-12-22|
    |group 1|2019-12-22|150            |200       |2019-12-22|
    |group 2|2019-12-15|80             |90        |2019-12-15|
    |group 2|2019-12-22|150            |30        |null      |
    |group 3|2019-12-22|50             |50        |2019-12-22|
    +-------+----------+---------------+----------+----------+

    
(17) For a MapType column, use aggregate to find the key with the max value. 
    REF: https://stackoverflow.com/questions/59651570/key-corresponding-to-max-value-in-a-spark-map-column

    import org.apache.spark.sql.functions._
    val mockedDf = (Seq(1, 3)
        .toDF("id")
        .withColumn("optimized_probabilities_map", typedLit(Map("foo"->0.34333337, "bar"->0.23))))

    mockedDf.withColumn("optimizer_ruler_name", expr("""

        aggregate(
          /* map_keys as array entry */
          map_keys(optimized_probabilities_map), 
          /* zero_expr: set up buffer with DataType: struct<name:string,val:double> */
          (string(NULL) as name, double(NULL) as val),
          /* merge: iterate through map_keys, compare the corresponding value with the buffered acc.val 
           * and then update acc.name accordingly
           */
          (acc, y) ->
            IF(acc.val is NULL OR acc.val < optimized_probabilities_map[y]
             , (y as name, optimized_probabilities_map[y] as val)
             , acc
            ),
          /* finish: select only name from the buffer */
          acc -> acc.name
        )

    """)).show(false)
    +---+--------------------------------+--------------------+
    |id |optimized_probabilities_map     |optimizer_ruler_name|
    +---+--------------------------------+--------------------+
    |1  |[foo -> 0.34333337, bar -> 0.23]|foo                 |
    |3  |[foo -> 0.34333337, bar -> 0.23]|foo                 |
    +---+--------------------------------+--------------------+


(18) Use aggregate function to return a named_struct while using EMPTY array() to initialize zero_value
  REF: https://stackoverflow.com/questions/59931770/sum-of-array-elements-depending-on-value-condition-pyspark/59938994#59938994

  Note: Aggregate function using Empty array() to initialize the zero_value, do calculation based on y value and array indices:
    from pyspark.sql.functions import expr

    df = spark.createDataFrame([
        (1,[0.2, 2.1, 3., 4., 3., 0.5]),
        (2,[7., 0.3, 0.3, 8., 2.,]), 
        (3,None), 
        (4,[])
      ],['id','column'])
    df.show()
    +---+--------------------+
    | id|              column|
    +---+--------------------+
    |  1|[0.2, 2.1, 3.0, 4...|
    |  2|[7.0, 0.3, 0.3, 8...|
    |  3|                null|
    |  4|                  []|
    +---+--------------------+

    df.withColumn('data', expr("""
    
        aggregate(
          /* ArrayType argument */
          column,
          /* zero: set empty array to initialize acc */
          array(),
          /* merge: iterate through `column` and reduce based on the values of y and the array indices of acc */
          (acc, y) ->
            CASE
              WHEN y < 2.0 THEN array(IFNULL(acc[0],0) + y, acc[1], acc[2])
              WHEN y > 2.0 THEN array(acc[0], IFNULL(acc[1],0) + y, acc[2])
                           ELSE array(acc[0], acc[1], IFNULL(acc[2],0) + y)
            END,
          /* finish: to convert the array into a named_struct */
          acc -> (acc[0] as `column<2`, acc[1] as `column>2`, acc[2] as `column=2`)
        )
    
    """)).selectExpr('id', 'data.*').show()
    +---+--------+--------+--------+
    | id|column<2|column>2|column=2|
    +---+--------+--------+--------+
    |  1|     0.7|    12.1|    null|
    |  2|     0.6|    15.0|     2.0|
    |  3|    null|    null|    null|
    |  4|    null|    null|    null|
    +---+--------+--------+--------+

