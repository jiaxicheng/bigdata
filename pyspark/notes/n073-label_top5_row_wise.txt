https://stackoverflow.com/questions/57800266/pyspark-how-to-find-and-convert-top-5-row-values-to-1-and-rest-all-to-0

Given the following sample dataframe, select the top 5 values in each row and mark them as '1' and the rest as '0'
+----+----+----+----+----+----+----+----+----+----+
|  _1|  _2|  _3|  _4|  _5|  _6|  _7|  _8|  _9| _10|
+----+----+----+----+----+----+----+----+----+----+
|0.74| 0.9|0.52|0.85|0.18|0.23| 0.3| 0.0| 0.1|0.07|
|0.11|0.57|0.81|0.81|0.45|0.48|0.86|0.38|0.41|0.45|
|0.03|0.84|0.17|0.96|0.09|0.73|0.25|0.05|0.57|0.66|
| 0.8|0.94|0.06|0.44| 0.2|0.89| 0.9| 1.0|0.48|0.14|
|0.73|0.86|0.68| 1.0|0.78|0.17|0.11|0.19|0.18|0.83|
+----+----+----+----+----+----+----+----+----+----+

This is a transform based on the ranking, similar to the function rank() but is implemented in row-wise.
There are 3 different ways to calculate top-N based on how the duplicated values are treated.
(1) rank(): duplicates will be set as the same rank-number, some rank-number might be missing due to duplicates
(2) dense_rank(): duplicates will be set as the same rank-number, and there is no gap between rank-numbers
(3) row_number(): duplicates will be set different rank-numbers, the order might varies based on how data are feeded

Below listed different methods to find row-wise top-N: rank, dense_rank and row_number:

Data Setup:

    df = spark.createDataFrame([
           (0.74, 0.9, 0.52, 0.85, 0.18, 0.23, 0.3, 0.0, 0.1, 0.07)
         , (0.11, 0.57, 0.81, 0.81, 0.45, 0.48, 0.86, 0.38, 0.41, 0.45)
         , (0.03, 0.84, 0.17, 0.96, 0.09, 0.73, 0.25, 0.05, 0.57, 0.66)
         , (0.8, 0.94, 0.06, 0.44, 0.2, 0.89, 0.9, 1.0, 0.48, 0.14)
         , (0.73, 0.86, 0.68, 1.0, 0.78, 0.17, 0.11, 0.19, 0.18, 0.83)
        ], ['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']
    )

    """ set up columns that matter """
    cols = df.columns

    """ select Top-N in row-wise """
    N = 5


## To calculate top-N based on Rank()

    """ 
     1. create an array columns, use sort_array()
     2. use sort_array() to sort the array and then retrieve the (N+1)th element to a new column `Nth`
     3. set cond = col > Nth, and use Spark SQL IF(cond, 1, 0) to set up the new value
    """
    df.withColumn('arr', F.array(cols))                \       
      .withColumn('Nth', F.sort_array('arr', False)[N]) \
      .selectExpr(*[ 'IF(`{0}` > Nth, 1, 0) as `{0}`'.format(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+


## to calculate the top-N based on dense_rank()

    """ Below code is based on Spark 2.40+
    similar to the calculation based on rank(), the only difference is we
    retrieve `Nth` value from the array_distict() before sort_array() step 
    """
    df.withColumn('arr', F.array(cols)) \
      .withColumn('topN', F.sort_array(F.array_distinct('arr'), False)[N]) \
      .selectExpr(*[ 'IF(`{0}` > topN, 1, 0) as `{0}`'.format(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  1|  1|  1|  0|  0|  1|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+


## To calculate the top-N based on Row_number()

    """ Below code requires spark 2.40+
    given an array of all cols, need to find idx of top-N values, this can be achieved
    by create an named_struct(val, idx), sort by `val` and take the topN slice() of array 
    containing the idx only. transform from struct(val, idx) -> idx. Then we use array_contain() 
    to set up the IF condition. 
    """
    df.withColumn('arr', F.array(cols))  \
      .withColumn('topN_idx', F.expr("""
            transform(
                slice(sort_array(
                    transform(sequence(0,size(arr)-1), i -> named_struct('val',arr[i], 'idx', i))
                  , False
                )
              , 1
              , {0}
            ), x -> x.idx)
          """.format(N)
        )) \
      .select([F.when(F.array_contains('topN_idx',i),1).otherwise(0).alias(cols[i]) for i in range(len(cols)) ]) \
      .show()
    +---+---+---+---+---+---+---+---+---+---+
    | _1| _2| _3| _4| _5| _6| _7| _8| _9|_10|
    +---+---+---+---+---+---+---+---+---+---+
    |  1|  1|  1|  1|  0|  0|  1|  0|  0|  0|
    |  0|  1|  1|  1|  0|  1|  1|  0|  0|  0|
    |  0|  1|  0|  1|  0|  1|  0|  0|  1|  1|
    |  1|  1|  0|  0|  0|  1|  1|  1|  0|  0|
    |  1|  1|  0|  1|  1|  0|  0|  0|  0|  1|
    +---+---+---+---+---+---+---+---+---+---+
