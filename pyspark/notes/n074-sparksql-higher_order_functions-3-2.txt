Continue..

Spark SQL higher-order functions:


(11) Solving the window with Gap issues using aggregate function of Spark 2.4+

Example-11-7: using aggregate function to apply logic which require some sort of row iterator 
  REF: https://stackoverflow.com/questions/64660047
  Note: Spark SQL using derived-table/CTE and Lateral View with inline function:

    df = spark.createDataFrame([
        ('01-10-2020', 'x', 10, 0), ('02-10-2020', 'x', 10, 0), ('03-10-2020', 'x', 15, 1), 
        ('04-10-2020', 'x', 15, 1), ('05-10-2020', 'x', 5, 0), ('06-10-2020', 'x', 13, 1), 
        ('07-10-2020', 'x', 10, 1), ('08-10-2020', 'x', 10, 0), ('09-10-2020', 'x', 15, 1), 
        ('01-10-2020', 'y', 10, 0), ('02-10-2020', 'y', 18, 0), ('03-10-2020', 'y', 6, 1), 
        ('04-10-2020', 'y', 10, 0), ('05-10-2020', 'y', 20, 0)
    ], ['date', 'item', 'avg_val', 'conditions'])

    df.createOrReplaceTempView("record")

    spark.sql("""
      SELECT t1.item, m.* 
      FROM (
        SELECT item, sort_array(collect_list(struct(date,avg_val,int(conditions) as conditions,conditions as flag))) as dta
        FROM record
        GROUP BY item
      ) as t1 LATERAL VIEW OUTER inline(
        aggregate(
          /* expr: set up array `dta` from the 2nd element to the last */
          slice(dta,2,size(dta)),
          /* start: set up and initialize `acc` using dta[0] to a struct containing two fields: 
           * - dta: an array of structs with a single element dta[0] 
           * - counter: number of rows after flag=1, can be from `0` to `N+1`
           */
          (array(dta[0]) as dta, dta[0].conditions as counter),
          /* merge: iterate through the `expr` using x and update two fields of `acc`
           * - dta: append values from x to acc.dta array using concat + array functions
           *        update flag using `IF(acc.counter IN (0,5) and x.conditions = 1, 1, 0)` 
           * - counter: increment by 1 if acc.counter is between 1 and 4
           *            , otherwise set value to x.conditions
           */
          (acc, x) -> named_struct(
              'dta', concat(acc.dta, array(named_struct(
                 'date', x.date,
                 'avg_val', x.avg_val,
                 'conditions', x.conditions,
                 'flag', IF(acc.counter IN (0,5) and x.conditions = 1, 1, 0)
               ))),
              'counter', IF(acc.counter > 0 and acc.counter < 5, acc.counter+1, x.conditions)
            ),    
          /* finish: retrieve acc.dta only and discard acc.counter */
          acc -> acc.dta
        )
      ) m  
    """).show(50)
    +----+----------+-------+----------+----+
    |item|      date|avg_val|conditions|flag|
    +----+----------+-------+----------+----+
    |   x|01-10-2020|     10|         0|   0|
    |   x|02-10-2020|     10|         0|   0|
    |   x|03-10-2020|     15|         1|   1|
    |   x|04-10-2020|     15|         1|   0|
    |   x|05-10-2020|      5|         0|   0|
    |   x|06-10-2020|     13|         1|   0|
    |   x|07-10-2020|     10|         1|   0|
    |   x|08-10-2020|     10|         0|   0|
    |   x|09-10-2020|     15|         1|   1|
    |   y|01-10-2020|     10|         0|   0|
    |   y|02-10-2020|     18|         0|   0|
    |   y|03-10-2020|      6|         1|   1|
    |   y|04-10-2020|     10|         0|   0|
    |   y|05-10-2020|     20|         0|   0|
    +----+----------+-------+----------+----+

**Where:**

1. using groupby to collect rows for the same item into an array of structs named `dta` column with 4 fields: date, avg_val, conditions and flag and sorted by date
2. use aggregate function to iterate the above array of structs, update the flag field based on the `counter` and `conditions` 
3. use Lateral VIEW and inline function to explode the resulting array of structs from the above aggregate function

 **Note:** 
  (1) the proposed SQL is for N=4, where we have `acc.counter IN (0,5)` and `acc.counter < 5` in the SQL. For any N, adjust the above to: `acc.counter IN (0,N+1)` and `acc.counter < N+1`, the below shows the result for `N=2`:

    +----+----------+-------+----------+----+                                       
    |item|      date|avg_val|conditions|flag|
    +----+----------+-------+----------+----+
    |   x|01-10-2020|     10|         0|   0|
    |   x|02-10-2020|     10|         0|   0|
    |   x|03-10-2020|     15|         1|   1|
    |   x|04-10-2020|     15|         1|   0|
    |   x|05-10-2020|      5|         0|   0|
    |   x|06-10-2020|     13|         1|   1|
    |   x|07-10-2020|     10|         1|   0|
    |   x|08-10-2020|     10|         0|   0|
    |   x|09-10-2020|     15|         1|   1|
    |   y|01-10-2020|     10|         0|   0|
    |   y|02-10-2020|     18|         0|   0|
    |   y|03-10-2020|      6|         1|   1|
    |   y|04-10-2020|     10|         0|   0|
    |   y|05-10-2020|     20|         0|   0|
    +----+----------+-------+----------+----+

  (2) we use `dta[0]` to initialize `acc` which includes both the values and  datatypes of its fields. 
    Ideally, we should make sure data types of these fields right so that all calculations are correctly 
    conducted. for example when calculating `acc.counter`, if `conditions` is StringType, `acc.counter+1` 
    will return a StringType with a DoubleType value

      spark.sql("select '2'+1").show()
      +---------------------------------------+
      |(CAST(2 AS DOUBLE) + CAST(1 AS DOUBLE))|
      +---------------------------------------+
      |                                    3.0|
      +---------------------------------------+

    Which could yield floating-point errors when comparing their value with integers using `acc.counter IN (0,5)` 
    or `acc.counter < 5`. Based on OP's feedback, this produced incorrect result without any WARNING/ERROR message.

    * One workaround is to specify exact field types using CAST when setting up the 2nd argument of aggregate 
      function so it reports ERROR when any types mismatch, see below:

        CAST((array(dta[0]), dta[0].conditions) as struct<dta:array<struct<date:string,avg_val:string,conditions:int,flag:int>>,counter:int>),

    * Another solution it to force types when creating `dta` column, in this example, 
      see `int(conditions) as conditions` in below code:

        SELECT item,
          sort_array(collect_list(struct(date,avg_val,int(conditions) as conditions,conditions as flag))) as dta
        FROM record
        GROUP BY item

    * we can also force datatype inside the calculating, for example, see `int(acc.counter+1)` below:

        IF(acc.counter > 0 and acc.counter < 5, int(acc.counter+1), x.conditions)      


  (3) in case there are too many columns that are not used in calculation, split flag from other fields into two arrays
      (less efficient though)

    spark.sql("""
      SELECT t1.item, m.dta.*, m.flag 
      FROM (
        SELECT item, sort_array(collect_list(struct(date,avg_val,conditions))) as dta
        FROM record
        GROUP BY item
      ) AS t1 LATERAL VIEW OUTER 
        inline(
          aggregate(
            /* expr: iterate through array dta from the 2nd element to the last */
            slice(dta,2,size(dta)),
            /* start: set up and initialize `acc` using dta[0] to a struct containing three fields: 
             * - dta: an array of structs with the same fields as dta array
             * - flag: an array of 0 or 1 based on conditions and N=4
             * - counter: number of rows after flag=1
             */
            (array(dta[0]) as dta, array(dta[0].conditions) as flag, dta[0].conditions as counter),
            /* merge: iterate the `expr` using x and update three fields
             * - dta: append x to acc.dta array using concat + array functions
             * - flag: calculate new flag by `IF(acc.counter IN (0,5) and x.conditions = 1, 1, 0)` 
             *         and append the result to acc.flag array
             * - counter: increment by 1 if acc.counter is between 1 and 4, otherwise set it to x.conditions
             */
            (acc, x) -> named_struct(
              'dta', concat(acc.dta, array(x)),
              'flag', concat(acc.flag, array(IF(acc.counter IN (0,5) and x.conditions = 1, 1, 0))),
              'counter', IF(acc.counter > 0 and acc.counter < 5, int(acc.counter+1), x.conditions)
            ),
            /* final: merge acc.dta and acc.flag into an array of structs with two fields dta and flag */
            acc -> zip_with(acc.dta, acc.flag, (x,y) -> (x as dta, y as flag))
          )
        ) m
    """).show(50)
    +----+----------+-------+----------+----+                                       
    |item|date      |avg_val|conditions|flag|
    +----+----------+-------+----------+----+
    |x   |01-10-2020|10     |0         |0   |
    |x   |02-10-2020|10     |0         |0   |
    |x   |03-10-2020|15     |1         |1   |
    |x   |04-10-2020|15     |1         |0   |
    |x   |05-10-2020|5      |0         |0   |
    |x   |06-10-2020|13     |1         |0   |
    |x   |07-10-2020|10     |1         |0   |
    |x   |08-10-2020|10     |0         |0   |
    |x   |09-10-2020|15     |1         |1   |
    |y   |01-10-2020|10     |0         |0   |
    |y   |02-10-2020|18     |0         |0   |
    |y   |03-10-2020|6      |1         |1   |
    |y   |04-10-2020|10     |0         |0   |
    |y   |05-10-2020|20     |0         |0   |
    +----+----------+-------+----------+----+



Example-11-8: set up begin/end effective date (type-2 SCD) using aggregate function
  REF:https://stackoverflow.com/q/64721132/9510729
  Note: this can be done using Window function by the typical question of adding a sub-group label `g`
        add example here for an example of using type casting and `(..)` for struct.

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        ('AA', 'foo', '2020-01-01'), ('AA', 'bar', '2020-01-02'), 
        ('AA', 'bar', '2020-01-03'), ('AA', 'baz', '2020-01-04')
    ], ['key', 'attr', 'extract_date'])
    #df = spark.read.csv('/home/xicheng/test/aggregate-5.txt', header=True)

    df.groupby('key') \
        .agg(F.expr("sort_array(collect_list(struct(extract_date,attr)), False) as dta")) \
        .selectExpr("key", """
          inline(
            aggregate(
              slice(dta,2,size(dta)),
              cast(array((dta[0].attr, dta[0].extract_date, '9999-12-31')) 
                as array<struct<attr:string,begin_effective:string,end_effective:string>>),
              (acc, x) -> 
                 IF(x.attr = element_at(acc,-1).attr
                  , concat(slice(acc,1,size(acc)-1), array(named_struct(
                        'attr', x.attr,
                        'begin_effective', x.extract_date,
                        'end_effective', element_at(acc,-1).end_effective
                      )))
                  , concat(acc, array(named_struct(
                        'attr', x.attr, 
                        'begin_effective', x.extract_date, 
                        'end_effective', element_at(acc,-1).begin_effective
                      )))
                 )
            )
          ) 
    """).show()
    +---+----+---------------+-------------+                                        
    |key|attr|begin_effective|end_effective|
    +---+----+---------------+-------------+
    | AA| baz|     2020-01-04|   9999-12-31|
    | AA| bar|     2020-01-02|   2020-01-04|
    | AA| foo|     2020-01-01|   2020-01-02|
    +---+----+---------------+-------------+


Example-11-9: set up min_time in a fixed window, need iterate through the list
  REF: https://stackoverflow.com/questions/64739902/find-min-value-for-every-5-hour-interval
  Method: a simpler version of the type, use array of int as accumulator and element_at(acc,-1) to 
          refer to the previous data.
  Code:

    val df = Seq(
      ("1", 1), ("1", 1), ("1", 2), ("1", 4), ("1", 5), ("1", 6), ("1", 8),
      ("1", 12), ("1", 12), ("1", 13), ("1", 14), ("1", 15), ("1", 16)
    ).toDF("id", "time")
    
    
    val df_new = (df.groupBy('id)
      .agg(sort_array(collect_list('time)) as 'dta)
      .selectExpr("id", """
        inline(
          aggregate(
            /* expr: set up array slice from the 2nd element to the last 
             * notice the indices for slice function is 1-based while dta[i] is 0-based
             */
            slice(dta,2,size(dta)), 
            /* start: initialize `acc` with the first element of dta including value and data type */
            array((dta[0] as time, dta[0] as min_time)), 
            /* merge: iterate through the expr using `x`, 
                element_at(acc,-1).min_time return the min_time in the last iteration */
            (acc,x) -> concat(acc, array(named_struct(
               'time', x, 
               'min_time', IF(element_at(acc,-1).min_time > x-6, element_at(acc,-1).min_time, x)
             )))
          )
        )
       """))
    
    df_new.show
    +---+----+--------+                                                             
    | id|time|min_time|
    +---+----+--------+
    |  1|   1|       1|
    |  1|   1|       1|
    |  1|   2|       1|
    |  1|   4|       1|
    |  1|   5|       1|
    |  1|   6|       1|
    |  1|   8|       8|
    |  1|  12|       8|
    |  1|  12|       8|
    |  1|  13|       8|
    |  1|  14|      14|
    |  1|  15|      14|
    |  1|  16|      14|
    +---+----+--------+
    


Example-11-10: setting up sessionID on streaming data based on conditions
  REF: https://stackoverflow.com/q/56290937/9510729
  Task: create sessionID based on some conditions related to surrounding timestamp:
   (1) Session expires after 30 minutes of inactivity (Means no click stream data within 30 minutes)
   (2) Session remains active for a total duration of 2 hours. After 2 hours, renew the session.
  A typical case using aggregate function:
  Example and Code:

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        ('U1', '2019-01-01T11:00:00Z'), ('U1', '2019-01-01T11:15:00Z'), ('U1', '2019-01-01T12:00:00Z'),
        ('U1', '2019-01-01T12:20:00Z'), ('U1', '2019-01-01T15:00:00Z'), ('U2', '2019-01-01T11:00:00Z'),
        ('U2', '2019-01-02T11:00:00Z'), ('U2', '2019-01-02T11:25:00Z'), ('U2', '2019-01-02T11:50:00Z'),
        ('U2', '2019-01-02T12:15:00Z'), ('U2', '2019-01-02T12:40:00Z'), ('U2', '2019-01-02T13:05:00Z'),
        ('U2', '2019-01-02T13:20:00Z')
    ], ['UserId', 'Click Time'])

    df = df.withColumn('Click Time', F.to_timestamp())

    df1 = df.groupby('UserId').agg(F.sort_array(F.collect_list('Click Time')).alias('dta'))

    df_new = df1.selectExpr("UserId", """
        inline_outer(
           aggregate(
              /* expr */
              slice(dta,2,size(dta)),
              /* start: set up zero_value, always use cast to avoid data-type mismatch issues */
              cast(array((dta[0], 1, dta[0])) as array<struct<`Click Time`:timestamp,sid:bigint,start:timestamp>>),
              /* merge */
              (acc,x)-> concat(acc, array(struct(
                 x as `Click Time`,
                 element_at(acc,-1).sid + int(x > least(element_at(acc,-1)['Click Time'] + interval 30 minutes
                    , element_at(acc,-1)['start'] + interval 2 hours)) as sid,
                 IF(x > least(element_at(acc,-1)['Click Time'] + interval 30 minutes
                    , element_at(acc,-1)['start'] + interval 2 hours), x, element_at(acc,-1).start) as start
                ))),
              /* finish */
              acc -> transform(acc, x -> (x['Click Time'] as `Click Time`, 'Session-'||x.sid as SessionID))
           )
        )
    """)
    df_new.show()
    +------+-------------------+---------+                                          
    |UserId|         Click Time|SessionID|
    +------+-------------------+---------+
    |    U2|2019-01-01 11:00:00|Session-1|
    |    U2|2019-01-02 11:00:00|Session-2|
    |    U2|2019-01-02 11:25:00|Session-2|
    |    U2|2019-01-02 11:50:00|Session-2|
    |    U2|2019-01-02 12:15:00|Session-2|
    |    U2|2019-01-02 12:40:00|Session-2|
    |    U2|2019-01-02 13:05:00|Session-3|
    |    U2|2019-01-02 13:20:00|Session-3|
    |    U1|2019-01-01 11:00:00|Session-1|
    |    U1|2019-01-01 11:15:00|Session-1|
    |    U1|2019-01-01 12:00:00|Session-2|
    |    U1|2019-01-01 12:20:00|Session-2|
    |    U1|2019-01-01 15:00:00|Session-3|
    +------+-------------------+---------+

Another way to setup accumulator as StructType and set `start` as a StructField:

    df_new = df1.selectExpr("UserId", """
       inline_outer(
          aggregate(
             slice(dta,2,size(dta)),
             /* always use cast to avoid data-type mismatch issues */
             cast((array((dta[0], 1)), dta[0]) as 
                 struct<dta:array<struct<`Click Time`:timestamp,sid:bigint>>,start:timestamp>),
             (acc,x)-> struct(
                concat(acc.dta, array(struct(
                   x as `Click Time`,
                   element_at(acc.dta,-1).sid + int(x > least(element_at(acc.dta,-1)['Click Time'] + interval 30 minutes
                     , acc.start + interval 2 hours)) as sid
                  ))) as dta,
                IF(x > least(element_at(acc.dta,-1)['Click Time'] + interval 30 minutes, acc['start'] + interval 2 hours)
                   , x, acc.start) as start
             ),
             acc -> transform(acc.dta, x -> (x['Click Time'] as `Click Time`, 'Session-'||x.sid as SessionID))
          )
       )
    """)
  Notice: using `cast` on setting up zero_value(the 2nd argument of aggregate function) is more reliable.


Example-11-11: island-gap issue using aggregate function.
  REF: https://stackoverflow.com/questions/64810042/pyspark-create-range-adjusting-end-date-dates-based-on-date-column
  Task: break a timeseries into chucks with max range <= 7 days in each chuck. gaps exist between chucks, a typical
        question using aggregate function:
  Code:

    test_df = spark.createDataFrame([
      (1, '2019-01-01'), (1, '2019-01-02'), (1, '2019-01-03'),
      (1, '2019-01-10'), (1, '2019-01-16'), (1, '2019-01-29') 
    ], ['id','dtstart'])

    new_df = test_df.groupby("id") \
        .agg(F.sort_array(F.collect_list("dtstart")).alias("dta")) \
        .selectExpr(
          "id", 
          """
            inline(
              aggregate(
                slice(dta,2,size(dta)),
                cast(array((dta[0], dta[0], 1)) as array<struct<dtstart:string,wdend:string,count:int>>),
                (acc,x)-> 
                  IF(datediff(x,element_at(acc,-1).dtstart) <= 7, 
                    concat(
                      slice(acc,1,size(acc)-1), 
                      array((element_at(acc,-1).dtstart as dtstart, x as wdend, element_at(acc,-1).count+1 as count))), 
                    concat(acc, array((x as dtstart, x as wdend, 1 as count)))
                  )
              )
            )
          """).show()
    +---+----------+----------+-----+                                               
    | id|   dtstart|     wdend|count|
    +---+----------+----------+-----+
    |  1|2019-01-01|2019-01-03|    3|
    |  1|2019-01-10|2019-01-16|    2|
    |  1|2019-01-29|2019-01-29|    1|
    +---+----------+----------+-----+



Example-11-12: set expr=dta and then discard in finish: acc -> slice(acc,2,size(acc))
  REF: https://stackoverflow.com/q/64879930/9510729
  Task: accumulate the value of to_count and reset when the cumsum is zero so that `cumsum >= 0` stands for all rows
  Code:

    df = spark.createDataFrame([(e,) for e in ['-1', '+1', '-1', '-1', '+1', '+1', '-1', '+1']],['to_count'])

    df = df.withColumn('id', F.monotonically_increasing_id())

    df1 = df.groupby().agg(F.sort_array(F.collect_list(F.struct('id','to_count'))).alias('dta'))

    df_new = df1.selectExpr("""
      inline(
        aggregate(
          /* expr: setup array dta.to_count */
          dta.to_count, 
          /* start: setup zero value to array of structs with two fields: count + sum */
          cast(array(('0',0)) as array<struct<to_count:string,sum:int>>),
          /* merge: append x to acc with logic to sum satisfies: greatest(element_at(acc,-1).sum+int(x),0) */
          (acc, x) -> concat(acc, array(named_struct(
              'to_count', x,
              'sum', greatest(element_at(acc,-1).sum+int(x),0) 
            ))), 
          /* finish: discard the first element of acc */
          acc -> slice(acc,2,size(acc))
        )
      )
    """)
    +--------+---+
    |to_count|sum|
    +--------+---+
    |      -1|  0|
    |      +1|  1|
    |      -1|  0|
    |      -1|  0|
    |      +1|  1|
    |      +1|  2|
    |      -1|  1|
    |      +1|  2|
    +--------+---+



