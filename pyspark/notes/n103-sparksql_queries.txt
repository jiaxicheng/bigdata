
Some notes/collections about using the SparkSQL DML and their DataFrame API correspondences

#==================================
#          Spark SQL Queries
#==================================


+ SELECT:

SELECT [hints, ...] [ALL|DISTINCT] named_expression[, named_expression, ...]
  FROM relation[, relation, ...]
  [lateral_view[, lateral_view, ...]]
  [WHERE boolean_expression]
  [aggregation [HAVING boolean_expression]]
  [ORDER BY sort_expressions]
  [CLUSTER BY expressions]
  [DISTRIBUTE BY expressions]
  [SORT BY sort_expressions]
  [WINDOW named_window[, WINDOW named_window, ...]]
  [LIMIT num_rows]


+ CTE(Common Table Expression):
  + an example using CTE: (using split)

      WITH v AS (SELECT bdate, hours, split(hours, "[-:]") AS h FROM tbl_df) 
      SELECT bdate, hours, (h[2]-h[0] + IF(h[2]<h[0],24,0)) AS h1 FROM v 


+ FROM:
  + VALUES (this can not be used with JOIN)

      SELECT id, code FROM VALUES (1, 'a'), (2, 'b'), (3, 'c') AS (id, code)

  + LATERAL_VIEW

      SELECT * FROM df_tbl LATERAL VIEW inline_outer(struct_c) my_view
      SELECT struct_c.a, my_view.val FROM df_tbl LATERAL VIEW OUTER explode(array_b) my_view AS val
      SELECT t.key, b.f1, b.f2  from t_df t LATERAL VIEW OUTER json_tuple(jstring, 'f1', 'f2') b as f1, f2

  Notes:
   (1) Generator: expression to generate zero or more rows (aka, lateral view)
       + explode, explode_outer: ArrayType, MapType
       + posexplode, posexplode_outer: ArrayType, MapType
       + inline, inline_outer: Array of StructType
       + json_tuple: <-- json_tuple does not generate new rows.
       + stack
       ref: https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-Generator.html
       UDTF (User Defined Table Function) limitations:
         + not allowed in SELECT, so in LATERAL VIEW
         + can not be nested, can not be used within another function
         + groupby, sortby, cluster by, distributed by are not supported
   (2) split() can not be used in LATERAL VIEW, reason: split is expected to be a generator. 
       However, its class is org.apache.spark.sql.catalyst.expressions.StringSplit
       , which is not a generator.

   Example CTE + LATERAL_VIEW https://stackoverflow.com/questions/58479019

       spark.sql(''' 
           WITH d AS (SELECT *, from_json(arg3, 'array<struct<name:string,datetime:string,event:string>>') t FROM t_df) 
           SELECT d.arg1, d.arg2, d.arg3, e.* FROM d LATERAL VIEW OUTER inline(d.t) e 
       ''').show()  

+ WHERE:
  + some special cases:
   (1) check if multiple columns are NULL at the same time

        IF(coalesce(col1, col2, col3, col4) is NULL, 'val1', 'val2')

   (2) filter by if (A <=> B) | (A is NULL) | (B is NULL), using coalesce() or nvl()

        df.where('coalesce(A, B) <=> coalesce(B, A)')    <-- cover A is NULL, B is NULL and A <=> B
        df.filter('nullif(A, B) is NULL')                <-- doesnot cover `B is NULL`

      

+ JOIN:
---
  + syntax:

    relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...]))
    relation NATURAL join_type JOIN relation
  
  + join_type:
    + INNER
    + (LEFT|RIGHT|FULL) OUTER
    + (LEFT|RIGHT) SEMI: == exists
    + LEFT ANTI: == not exists


+ Sampling:
---
  + syntax:

      TABLESAMPLE ((integer_expression | decimal_expression) PERCENT)
      TABLESAMPLE (integer_expression ROWS)

  + examples:

     SELECT * FROM df_tbl TABLESAMPLE (200 ROWS)
     SELECT * FROM df_tbl TABLESAMPLE (10 PERCENT)


+ Aggregation:
---
  + syntax:

    GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))]

  + ROLLUP
       GROUP BY c1, c2 WITH ROLLUP
     is the same as:
       GROUP BY c1, c2 GROUPING SETS ((a,b), (a), ())
  + CUBE
       GROUP BY c1, c2 WITH CUBE
     is the same as:
       GROUP BY c1, c2 GROUPING SETS ((a,b), (a), (b), ())



+ Window Functions:
---
  + Window Spec: (below two same) 
    + Partition By Expression Order By sort_expression [frame_bound]
      Distribute By expression Sort By sort_expression [frame_bound]

  + Frame Bound:
    + RANGE Between Unbounded Preceding AND Current Row
    + ROWS Between Unbounded Preceding AND Unbounded Following
    + RANGE BETWEEN -123455 AND 0

  + example: https://stackoverflow.com/questions/58920120

       SELECT itemNo 
       ,      modelnumber 
       ,      itemClass 
       ,      concat_ws('-', sort_array(collect_set(purchaseYear) OVER w1)) AS purchase_years 
       ,      concat_ws('-', sequence(purchaseYear, purchaseYear+2)) AS all_purchase_years
       ,      sum(PurchaseRatio) OVER w1 AS sum_PurchaseRatio 
       ,      sum(ModelRatio) OVER w1 AS sum_ModelRatio 
       FROM test 
       ORDER BY itemNo, purchaseYear 
       WINDOW w1 AS (
           PARTITION BY (itemNo, modelnumber, itemClass)  
           ORDER BY purchaseYear 
           RANGE BETWEEN CURRENT ROW AND 2 FOLLOWING
       ) 
  Notes:
   (1) Window Spec `w1` must be applied to an aggregate function, see example: `collect_set(purchaseYear) OVER w1`
       


+ Hints: 
---
  + the Hint framework was added in Spark SQL 2.2
  + syntax:

      /*+ hint[, hint, ...] */
 
  + examples:

      SELECT /*+ BROADCAST(users) */ * FROM applications as app, users as u WHERE app.user_id = u.id 

  + Available hint names:
    + BROADCAST(tbl_name)    broadcast join or map-side join
        
        SELECT /*+ MAPJOIN(b) */ ...
        SELECT /* BROADCASTJOIN(b) */ ...
        SELECT /* BROADCAST(b) */ ...

    + SKEW('tbl_name')       skew join
      data skew, unevenly distributed among partitions in the cluster
      need the name of the relation(a table, view or subquery) with skew.
      https://docs.databricks.com/delta/join-performance/skew-join.html
    + RANGE_JOIN(tbl_name, bin_size)
      + applied to any join with a range condition (i.e. between, >, <), selecting bin_size is important
      + all values involved must be numeric types
      + range condition must be compared between the same data type
      + in case of left_join, check the range values on the left table, similar to right_join
    + COALESCE and REPARTITION: (added Spark SQL 2.4, SQL comments only)
      
        SELECT /*+ COALESCE(5) */ ...
        SELECT /*+ REPARTITION(3) */ ...

      https://docs.databricks.com/delta/join-performance/range-join.html
      https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-hint-framework.html

  Notes:
   (1) How to select bin_size of range_join by checking the approx_percentile for reference:

        SELECT approx_percentile(cast(end - start as double), array(0.5, 0.9, 0.99, 0.999, 0.9999) FROM ranges
        


#=================================
#         Data Frame API
#=================================

+ SELECT:
---
  + col_names only                : df.select(c1, c2, c3, c4)
  + one list_comprehension only   : df.select([list_comp])  
                                    df.select(*[list_comp])
  + col_names + list_comprehension: df.select(c1, c2, *[list_comp])
  + cols + list_comprehension     : df.select(cols_list + [list_comp])
  + list_comp inside list_comp    : use reduce function to flatten the inside lists, see below example:

        Example: https://stackoverflow.com/questions/58219205
        df = spark.read.json('file:///home/hdfs/test/pyspark/json-12-1.txt', multiLine=True)
        cols = [ c[0] for c in df.dtypes if c[1].startswith('struct') ]
        fields_mapping = { 
                f['name']:[i['name'] for i in f['type']['fields']] 
                    for f in df.schema.jsonValue()['fields'] 
                    if type(f['type']) is dict and f['type']['type'] == 'struct' 
        }
        # below will keep the original column orders
        df.select(
             reduce(lambda x,y: x+y, [
                 [ F.col(c).getItem(f).alias('`{}_{}`'.format(c,f)) for f in fields_mapping[c] ]
                     if c in cols else [c] 
                 for c in df.columns
             ])
        ).show()


+ Hints:
---
  + df.hint(name, *parameters)
    + broadcast:  df1.hint('broadcast').join(df2, on='id', how='left')
      * see also: pyspark.sql.functions.broadcast()
    + skew_join:
    + range_join:

Notes:
 (1) By default Spark SQL commands are case-insensitive, to change it

        sqlContext.sql("set spark.sql.caseSensitive=true")



References:
[1] https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#aggregation
