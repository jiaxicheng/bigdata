Spark SQL built-in Date/Timestamp functions:


built-in functions:
---
  + current_date:
  + current_timestamp() = now()
  + type casting
    + timestamp(ts/string), cast(ts as 'timestamp')     <-- this can keep the microsecond
    + date(ts/string), cast(ts as 'date')
  + useful functions:
    + string add_months(d, N)                                               <-- exact month
      + HiveQL has one more argument `fmt`
    + string next_day(d, fmt): fmt can be: ['MO', 'TU', .. 'SA', 'SU']
    + string last_day(d)
*    + string trunc(d, fmt): Supported formats: MONTH/MON/MM, YEAR/YYYY/YY
      * compatible with HiveQL
*    + timestamp date_trunc(fmt, ts): 
       fmt can be one of ["YEAR", "YYYY", "YY", "MON", "MONTH", "MM", "DAY", "DD"
                         , "HOUR", "MINUTE", "SECOND", "WEEK", "QUARTER"]
    + date date_add(d, N)  
    + date date_sub(d, N)
    + double months_between(d1, d2)
    + int datediff(end_date, start_date)
  + date/timestamp conversion:
    + bigint unix_timestamp(d, fmt)
      * same as to_unix_timestamp(d, fmt) in SparkSQL
    + date to_date(d, fmt)  
      * same as cast('date') astype('date')
    + timestamp to_timestamp(d, fmt)
      * same to cast('timestamp'), while keep the precision to milliseconds
    + timestamp from_unixtime(bigint d, format='yyyy-MM-dd HH:mm:ss')
    + timestamp from_utc_timestamp(d, tz)
    + timestamp to_utc_timestamp(d, tz)
    + string date_format(d, fmt)
  + Date/time granuity:
    + int year(d)
    + int quarter(date/timestamp/string)
    + int month(d)
    + int dayofmonth(d), day(d)
    + int hour(d)
    + int minute(d)
    + int second(d)
    + int weekofyear(d)
    + int dayofyear(d)
    + int dayofweek(d) or weekday(d): 1:Sun, 2:Mon,..,7:Sat  

  + TimestampType <-> LongType: unix_timestamp, from_unixtime
  + Date/Timestamp/String to String: date_format


Notes: 
  (1) In from_utc_timestamp/to_utc_timestamp,  `d` can be {any primitive type}* = including timestamp/date
      , tinyint/smallint/int/bigint, float/double, decimal
  (2) date_trunc(fmt, ts) convert timestamp to timestamp, when you need a `Timestamp`
      trunc convert DateType into a StringType: when you need a `Date`
  (3) Interval used with Datafram API function:
      interval can not use col_name, i.e. `INTERVAL col1 months` is not working
      df.withColumn('10_days_before', 'datetime_col' - expr('INTERVAL 10 days'))
      


Some References: 
[1] https://docs-snaplogic.atlassian.net/wiki/spaces/SD/pages/2458071/Date+Functions+and+Properties+Spark+SQL
[2] Java SimpleDateFormats: https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
[3] HiveQL built-in functions: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-ComplexTypeConstructors
[4] SparkSQL: https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#


Example-1: Generating date sequence

   Method-1: use add_months(d, N)       <-- exact month

      transform(sequence(1,6), i -> add_months("2019-01-31", i))
      #[2019-02-28, 2019-03-31, 2019-04-30, 2019-05-31, 2019-06-30, 2019-07-31]

   Method-2: use sequence with interval 1 month

      sequence(to_date("2019-02-01"), to_date("2019-07-05"), interval 1 month)    <-- 30 days
      #[2019-02-01, 2019-03-01, 2019-03-31, 2019-04-30, 2019-05-31, 2019-06-30]

   Example-1.2: generate monthly date sequence for the last 36 months
   https://stackoverflow.com/questions/58341985

      spark.range(36,-1,-1).selectExpr("add_months(date_trunc('MM', current_date()),negative(id)) as date").show(100)


Example-2: Issues with microsecond: 20190111-08:15:45.275753
pyspark only hold to second level:
https://stackoverflow.com/questions/54232494/working-with-microsecond-time-stamps-in-pyspark

    Solution-1: split microsecond from timestamp, convert unix_timestamp(to_timestamp(..)..) to long
                and then plus the substring_index(dt,'.',-1)/1000000 
        REF: https://stackoverflow.com/questions/50648154/microsecond-time-stamps-in-pyspark

        time_df.withColumn("time",  F.expr("""
                unix_timestamp(to_timestamp(substring(dt,0,23), 'yyyyMMdd-HH:mm:ss')) 
              + double(substring_index(dt, '.', -1))/1000000 as time
            """)
        ).show()
        +------------------------+-------------------+
        |dt                      |time               |
        +------------------------+-------------------+
        |20150408-01:12:04.275753|1.428469924275753E9|
        +------------------------+-------------------+

    Solution-2: cast() or timestamp

        spark.sql("select double(timestamp('2016-07-13 14:33:53.979'))").show(truncate=False)
        +----------------------------------------------------------+
        |CAST(CAST(2016-07-13 14:33:53.979 AS TIMESTAMP) AS DOUBLE)|
        +----------------------------------------------------------+
        |                                          1.468434833979E9|
        +----------------------------------------------------------+

    For standard-formatted timestamp, do type casting is easy to keep resolution:

        spark.sql("select timestamp('2020-08-05 12:34:10.8001234') ts").show(2,0)
        +--------------------------+
        |ts                        |
        +--------------------------+
        |2020-08-05 12:34:10.800123|
        +--------------------------+
        #DataFrame[ts: timestamp]

        # this can also cover timezone converting:
        spark.sql("select timestamp('2020-09-08 14:00:00.917+05:00') as ds_with_tz").show(10,0)
        +-----------------------+
        |ds_with_tz             |
        +-----------------------+
        |2020-09-08 09:00:00.917|
        +-----------------------+



Example-3: from year, month and week-number to retrieve a "Friday"(or any dayofweek)
  REF: https://stackoverflow.com/questions/57463921/how-to-truncate-a-date-to-friday-based-on-week-of-month
  Steps:
   (1) Use to_date to create a date column `dt` point to the first date in the month specified
   (2) find the week number `wn`, convert it to integer and minus 1 to point it to the previous week
   (3) use date_add function to find the Monday after `wn` week from dt `date_add(dt, 7*wn-dayofweek(dt))`
   (4) use next_day(.., "Friday") to find the next Friday

    df = spark.read.csv('file:///home/hdfs/test/pyspark/datetime-3.txt', header=True)

    df_new = df.withColumn('dt', F.to_date(F.concat_ws('-', 'Year','Month',F.lit('01')), format="yyyy-MMMM-dd")) \
               .withColumn('wn', F.split('Weeks', ' ')[0].astype('int')-1) \
               .withColumn('ldate', F.expr('next_day(date_add(dt, 7*wn-dayofweek(dt)),"Friday")'))

    df_new.show()
    +----+-----+-----+----------+----------+---+----------+
    |Year|Month|Weeks|      date|        dt| wn|     ldate|
    +----+-----+-----+----------+----------+---+----------+
    |2018|April| 01 W|2018-04-06|2018-04-02|  0|2018-04-06|
    |2018|April| 02 W|2018-04-13|2018-04-02|  1|2018-04-13|
    |2018|April| 03 W|2018-04-20|2018-04-02|  2|2018-04-20|
    |2018|April| 04 W|2018-04-27|2018-04-02|  3|2018-04-27|
    |2018|  May| 01 W|2018-05-04|2018-05-02|  0|2018-05-04|
    |2018|  May| 02 W|2018-05-11|2018-05-02|  1|2018-05-11|
    |2018|  May| 03 W|2018-05-18|2018-05-02|  2|2018-05-18|
    |2018|  May| 04 W|2018-05-25|2018-05-02|  3|2018-05-25|
    |2018| June| 01 W|2018-06-01|2018-06-02|  0|2018-06-01|
    +----+-----+-----+----------+----------+---+----------+






