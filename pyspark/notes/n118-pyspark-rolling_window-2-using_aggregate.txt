(2) Using higher-order function to calculate the rolling windows:
---    

Example-1: window-size is from a column, so it might be variable for different rows:
  REF: https://stackoverflow.com/questions/61681295/sum-the-values-on-column-using-pyspark
  Notes: should be faster than doing UDF, however complex computation might not be easy to implement.
    
    from pyspark.sql.functions import collect_list, struct, sort_array
    
    # the main dataframe
    df = spark.createDataFrame([
        ('ll','2020-01-05','1','10'), ('ll','2020-01-06','1','10'), ('ll','2020-01-07','1','10'),
        ('ll','2020-01-08','1','10'), ('ll','2020-01-09','1','10'), ('ll','2020-01-10','1','10'),
        ('ll','2020-01-11','1','20'), ('ll','2020-01-12','1','10'), ('ll','2020-01-05','2','30'),
        ('ll','2020-01-06','2','30'), ('ll','2020-01-07','2','30'), ('ll','2020-01-08','2','40'),
        ('ll','2020-01-09','2','30'), ('ll','2020-01-10','2','10'), ('ll','2020-01-11','2','10'),
        ('ll','2020-01-12','2','10'),
        ('yy','2020-01-05','1','20'), ('yy','2020-01-06','1','20'), ('yy','2020-01-07','1','20'),
        ('yy','2020-01-08','1','20'), ('yy','2020-01-09','1','20'), ('yy','2020-01-10','1','40'),
        ('yy','2020-01-11','1','20'), ('yy','2020-01-12','1','20'),
        ('yy','2020-01-05','2','40'), ('yy','2020-01-06','2','40'), ('yy','2020-01-07','2','40'),
        ('yy','2020-01-08','2','40'), ('yy','2020-01-09','2','40'), ('yy','2020-01-10','2','40'),
        ('yy','2020-01-11','2','60'), ('yy','2020-01-12','2','40')
    ], ('x','date','flag','value'))
    
    # window-size from another dataframe
    df1 = spark.createDataFrame([('ll',5), ('yy',6)], ('x','days'))
    
    # join df and df1 to get array column `row` containing all sorted `date and `value` related to `x` and `flag`
    df2 = df.groupby('x', 'flag') \
        .agg(sort_array(collect_list(struct('date','value'))).alias('row')) \
        .join(df1, "x", "left") 
    df2.show()
    +---+----+--------------------+----+                                            
    |  x|flag|                 row|days|
    +---+----+--------------------+----+
    | ll|   1|[[2020-01-05, 10]...|   5|
    | ll|   2|[[2020-01-05, 30]...|   5|
    | yy|   2|[[2020-01-05, 40]...|   6|
    | yy|   1|[[2020-01-05, 20]...|   6|
    +---+----+--------------------+----+
    
    # for each x and flag, take the sum of rows from the current row to the following `days` rows
    df2.selectExpr(
        "x", 
        "flag", 
        """
          inline(
            transform(
              row, (x,i) -> named_struct(
                'date', x.date, 
                'value', x.value, 
                /* result based on the array slice using days and then take the sum */
                'result', aggregate(slice(row,i+1,days),0L,(x,y)->x+int(y.value)) 
              )
            )
          )
        """).orderBy('x','flag','date').show(100)
    +---+----+----------+-----+------+                                              
    |  x|flag|      date|value|result|
    +---+----+----------+-----+------+
    | ll|   1|2020-01-05|   10|    50|
    | ll|   1|2020-01-06|   10|    50|
    | ll|   1|2020-01-07|   10|    60|
    | ll|   1|2020-01-08|   10|    60|
    | ll|   1|2020-01-09|   10|    50|
    | ll|   1|2020-01-10|   10|    40|
    | ll|   1|2020-01-11|   20|    30|
    | ll|   1|2020-01-12|   10|    10|
    | ll|   2|2020-01-05|   30|   160|
    | ll|   2|2020-01-06|   30|   140|
    | ll|   2|2020-01-07|   30|   120|
    | ll|   2|2020-01-08|   40|   100|
    | ll|   2|2020-01-09|   30|    60|
    | ll|   2|2020-01-10|   10|    30|
    | ll|   2|2020-01-11|   10|    20|
    | ll|   2|2020-01-12|   10|    10|
    | yy|   1|2020-01-05|   20|   140|
    | yy|   1|2020-01-06|   20|   140|
    | yy|   1|2020-01-07|   20|   140|
    | yy|   1|2020-01-08|   20|   120|
    | yy|   1|2020-01-09|   20|   100|
    | yy|   1|2020-01-10|   40|    80|
    | yy|   1|2020-01-11|   20|    40|
    | yy|   1|2020-01-12|   20|    20|
    | yy|   2|2020-01-05|   40|   240|
    | yy|   2|2020-01-06|   40|   260|
    | yy|   2|2020-01-07|   40|   260|
    | yy|   2|2020-01-08|   40|   220|
    | yy|   2|2020-01-09|   40|   180|
    | yy|   2|2020-01-10|   40|   140|
    | yy|   2|2020-01-11|   60|   100|
    | yy|   2|2020-01-12|   40|    40|
    +---+----+----------+-----+------+
    
    

Example-2: using aggregate + slice to handle aggregate on variable size and boundaries:
  REF: https://stackoverflow.com/questions/63384238
  Target: a complex aggregation based on different flags, see the original post for details
  Method: use Windows function collect_list to get all related rows, sort the array of structs by `date`
          and then do the aggregation based on a slice of this array. the start_idx and span of 
          this slice are defined based on the following:

    1. If col1 = 1, start_idx = 1 and span = 0, so nothing is aggregated
    2. else if Trigger = 'F', then start_idx = 0 and span = `col2`
    3. else start_idx = col1+1 and span = col2-col1

  Notice that the index for the function `slice` is 1-based. see below code:

    from pyspark.sql.functions import to_date, sort_array, collect_list, struct, expr
    from pyspark.sql import Window

    w1 = Window.orderBy('date').rowsBetween(0, Window.unboundedFollowing)

    # columns used to do calculations, date must be the first field for sorting purpose
    cols = ["date", "value", "start_idx", "span"]

    df_new = (TEST_df 
        .withColumn('start_idx', expr("IF(col1 = -1 OR Trigger = 'F', 1, col1+1)")) 
        .withColumn('span', expr("IF(col1 = -1, 0, IF(Trigger = 'F', col2, col2-col1))")) 
        .withColumn('dta', sort_array(collect_list(struct(*cols)).over(w1))) 
        .withColumn("want1", expr("aggregate(slice(dta,start_idx,span), 0D, (acc,x) -> acc+x.value)"))
    )

    df_new.show() 
    +----------+-------+-----+----+----+----+---------+----+--------------------+------------------+
    |      date|Trigger|value|col1|col2|want|start_idx|span|                 dta|             want1|
    +----------+-------+-----+----+----+----+---------+----+--------------------+------------------+
    |2020-08-01|      T|  0.0|   3|   5| 0.5|        4|   2|[[2020-08-01, 0.0...|0.5000000149011612|
    |2020-08-02|      T|  0.0|  -1|   4| 0.0|        1|   0|[[2020-08-02, 0.0...|               0.0|
    |2020-08-03|      T|  0.0|  -1|   3| 0.0|        1|   0|[[2020-08-03, 0.0...|               0.0|
    |2020-08-04|      F|  0.2|   3|   3| 0.7|        1|   3|[[2020-08-04, 0.2...|0.7000000178813934|
    |2020-08-05|      T|  0.3|   1|   4| 0.9|        2|   3|[[2020-08-05, 0.3...|0.9000000059604645|
    |2020-08-06|      F|  0.2|  -1|   3| 0.0|        1|   0|[[2020-08-06, 0.2...|               0.0|
    |2020-08-07|      T|  0.2|  -1|   4| 0.0|        1|   0|[[2020-08-07, 0.2...|               0.0|
    |2020-08-08|      T|  0.5|  -1|   5| 0.0|        1|   0|[[2020-08-08, 0.5...|               0.0|
    |2020-08-09|      T|  0.0|  -1|   5| 0.0|        1|   0|[[2020-08-09, 0.0...|               0.0|
    +----------+-------+-----+----+----+----+---------+----+--------------------+------------------+

  Note: in case there exists `col2 < col1` when Trigger = 'T' and cols != -1, there could be negative start_idx
        in such case, define the full-size Window spec:

            w1 = Window.orderBy('date').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

        and use `array_position` to find the index of the current row, and then calculate start_idx from 
        relative to the above position





