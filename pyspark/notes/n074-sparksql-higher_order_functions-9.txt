Continue..

Spark SQL higher-order functions:

(30) aggregate: use cast in zero_value:
  REF: https://stackoverflow.com/questions/61854735/how-to-aggregate-on-percentiles-in-pyspark
  Notes: instead of define `array(string(NULL))` in the zero_value, which yield a NULL as the first array item of `acc`
    use `cast(array_repeat(array(),2) as array<array<string>>)` which not only skip the NULL item, but also force the 
    data types of `acc` which is essential in aggregate function.

  Method-1,2: use higher-order function, need spark 2.4+. 
    * this is a very good example setting zero_value with aggregate function

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        ['a', 1, 'w'], ['a', 1, 'y'], ['a', 11, 'x'], ['a', 111, 'zzz'], ['a', 1111, 'zz'], ['a', 1111, 'zz'],
        ['b', 2, 'w'], ['b', 2, 'w'], ['b', 2, 'w'], ['b', 22, 'y'], ['b', 2222, 'x'], ['b', 2222, 'z'],
    ], ['grp', 'val1', 'val2'])

    grouped = df.groupby('grp').agg(
        F.count('*').alias('count'),
        F.expr('percentile(val1, array(0.5, 0.75)) as percentiles'),
        F.collect_list(F.struct('val1','val2')).alias('vals')
    ) 

    grouped.selectExpr(
      "grp",
      "count",
      "percentiles",
      """
        /* less efficient version */
        transform(percentiles, x -> 
          size(
            array_distinct(
              filter(vals, y -> y.val1 > x).val2
            )
          )
        ) as distinct_count
      """,
      """
        /* more efficient version */
        aggregate( 
            vals, 
            /* more robust way: cast(array_repeat(array(),size(percentiles)) as array<array<string>>)*/
            cast(array_repeat(array(),2) as array<array<string>>), 
            (acc, y) -> 
                transform(acc, (x,i) -> IF(y.val1 > percentiles[i], concat(x, array(y.val2)), x)),
            acc -> transform(acc, x -> size(array_distinct(x))) 
        ) as distinct_count_1 
      """
    ).show(10,0)                                                                                                       
    +---+-----+--------------+--------------+----------------+
    |grp|count|percentiles   |distinct_count|distinct_count_1|
    +---+-----+--------------+--------------+----------------+
    |a  |6    |[61.0, 861.0] |[2, 1]        |[2, 1]          |
    |b  |6    |[12.0, 1672.0]|[3, 2]        |[3, 2]          |
    +---+-----+--------------+--------------+----------------+

  Method-3: just do the pre-processing before aggregation which does not require Spark 2.4+
    
    from pyspark.sql import Window, functions as F
    
    w1 = Window.partitionBy('grp')
    
    df1 = df.withColumn('percentiles', F.expr('percentile(val1, array(0.5, 0.75))').over(w1)) \
        .withColumn('c1', F.expr('IF(val1>percentiles[0],val2,NULL)')) \
        .withColumn('c2', F.expr('IF(val1>percentiles[1],val2,NULL)'))
    
    grouped = df1.groupby('grp').agg(
        F.count('*').alias('count'), 
        F.first('percentiles').alias('percentiles'), 
        F.array(F.countDistinct('c1'), F.countDistinct('c2')).alias('distinct_count')
    )
    grouped.show()
    +---+-----+--------------+--------------+                                       
    |grp|count|   percentiles|distinct_count|
    +---+-----+--------------+--------------+
    |  b|    6|[12.0, 1672.0]|        [3, 2]|
    |  a|    6| [61.0, 861.0]|        [2, 1]|
    +---+-----+--------------+--------------+



(31) efficient way using aggregate to create and update a Map:
  REF: https://stackoverflow.com/questions/61947677/pyspark-higher-order-sql-functions-to-create-histograms-from-arrays

  Notes: to update a Map:
   (1) Insert a new key: use `map_concat(acc, map(y,1))`
   (2) Update a existing key, increment the value by 1 for matched key only
    for Spark 2.4+:  map_from_entries(transform(map_keys(acc), k -> (k as key, acc[k] + int(k=y) as val)))
    for Spark 3.0+:  transform_values(acc, (k,v) -> v + int(k=y))
    
    df = spark.createDataFrame([(["val1", "val2", "val1", "val1", "val3", "val2", "val1"],)],["vals"])

    # Spark 2.4+
    df.selectExpr("*", """ 
        aggregate( 
            vals,  
            cast(map() as map<string,int>), 
            (acc,y) -> CASE
                    /* insert a new key */
                    WHEN NOT array_contains(map_keys(acc), y) THEN map_concat(acc, map(y,1)) 
                    /* update an existing key */
                    ELSE map_from_entries(transform(map_keys(acc), k -> (k as key, acc[k] + IF(k==y,1,0) as val)))
                END
        ) AS new_data 
    """).show(1,0)      
    +------------------------------------------+---------------------------------+
    |vals                                      |new_data                         |
    +------------------------------------------+---------------------------------+
    |[val1, val2, val1, val1, val3, val2, val1]|[val1 -> 4, val2 -> 2, val3 -> 1]|
    +------------------------------------------+---------------------------------+
    
    # Spark 3.0+, using aggregate + map_zip_with functions:
    df.selectExpr("*", """
        aggregate(
            vals,
            cast(map() as map<string,int>),
            (acc,y) -> map_zip_with(acc, map(y,1), (k,v1,v2) -> coalesce(v1,0) + coalesce(v2,0))
        ) as new_data
    """).show(1,0)
    +------------------------------------------+---------------------------------+
    |vals                                      |new_data                         |
    +------------------------------------------+---------------------------------+
    |[val1, val2, val1, val1, val3, val2, val1]|{val1 -> 4, val2 -> 2, val3 -> 1}|
    +------------------------------------------+---------------------------------+

  Note: a similar example https://stackoverflow.com/questions/62422501/how-to-aggregate-values-within-array-in-pyspark  



(32) Use aggregate function to do stateful accumulation, no need to pre-sort data by using a MapType column.
  REF: https://stackoverflow.com/questions/61965306/how-can-i-calculate-a-moving-average-with-certain-days-missing

  Target: calculate a moving average: today's score + (yesterdya's score * .50) 
  Notes: days could be missing, also post-processing to filter days with score < 0.1

    from pyspark.sql.functions import expr
  
    df = spark.read.csv("/home/xicheng/test/window-25.txt", header=True, inferSchema=True)
    +---+----+-----+
    |day|user|score|
    +---+----+-----+
    |  0|   a|  1.1|
    |  0|   b|  0.6|
    |  1|   a|  0.2|
    |  1|   b|  0.2|
    |  1|   c|  0.6|
    |  2|   c|  1.1|
    |  3|   a|  0.2|
    |  3|   b|  0.7|
    +---+----+-----+
  
    # below create a MapType column: `data` with key=`day` and value=`score`
    df2 = df.groupby('user').agg(
        expr("map_from_entries(collect_list(struct(day,score))) as data"),
        fmin('day').alias('first_day'),
        fmax('day').alias('last_day')
    )
  
    df2.selectExpr("user", """ 
        inline_outer( 
            aggregate( 
                /* for each user, iterate from first_day to the next 7 days or last_day if it's 
                 * late than 7 days, interval by 1 day */
                sequence(first_day,greatest(first_day+7,last_day)), 
                /* zero_value as an array of structs: array<struct<day:int,score:double>> */
                CAST(array() as array<struct<day:int,score:double>>), 
                /* merge: iterate by `i` */
                (acc, i) -> array_append(acc, named_struct(
                        "day", i,
                        "score", 0.5*ifnull(element_at(acc,-1).score, 0) + ifnull(data[i], 0)
                    )
                ),
                /* finish: remove array items with score < 0.1 */
                acc -> filter(acc, x -> x.score >= 0.1) 
            ) 
        ) 
    """).show(22)
    +----+---+-------------------+
    |user|day|              score|
    +----+---+-------------------+
    |   c|  1|                0.6|
    |   c|  2| 1.4000000000000001|
    |   c|  3| 0.7000000000000001|
    |   c|  4|0.35000000000000003|
    |   c|  5|0.17500000000000002|
    |   b|  0|                0.6|
    |   b|  1|                0.5|
    |   b|  2|               0.25|
    |   b|  3|              0.825|
    |   b|  4|             0.4125|
    |   b|  5|            0.20625|
    |   b|  6|           0.103125|
    |   a|  0|                1.1|
    |   a|  1|               0.75|
    |   a|  2|              0.375|
    |   a|  3|             0.3875|
    |   a|  4|            0.19375|
    +----+---+-------------------+


(33) use sequence + filter to find the index of a specific value in an array of structs

  REF: https://stackoverflow.com/questions/63290611/pyspark-how-to-code-complicated-dataframe-calculation#63290611

  Target: find idx of an item and do some complex calculation based on this index.

  (1) groupby and create a collect_list of all related rows(`vals` in below code), sort the list by date in desencending 
    order. Note: change groupby(lit(1)) to whatever column you can use to divide you data into independant subset.
  (2) find the array index `idx` which has `col1 == 1`
  (3) if col2==-1 at `idx`, find the offset from idx to the beginning of the list with the first row having col2 != -1
    Note: in the current code, offset might be NULL if all col2 before `idx` are -1, you will have to decide what you want
  (4) after we have offset and idx, the `want` can be calculated by:

      IF(i<idx, 0, vals[idx-offset].col2 + offset + i - idx)

  See below:

    from pyspark.sql.functions import sort_array, collect_list, struct, expr, lit

    TEST_df = spark.createDataFrame([
      ('2020-08-01', -1, -1), ('2020-08-02', -1, -1), ('2020-08-03', -1, 3), 
      ('2020-08-04', -1, 2), ('2020-08-05', 1, -1), ('2020-08-06', 2, -1), 
      ('2020-08-07', 3, -1), ('2020-08-08', 4, 4), ('2020-08-09', 5, -1) 
    ], ['date', 'col1', 'col2'])

    cols = ["date", "col1", "col2"]

    df_new = TEST_df \
        .groupby(lit(1)) \
        .agg(sort_array(collect_list(struct(*cols)),False).alias('vals')) \
        .withColumn('idx', expr("filter(sequence(0,size(vals)-1), i -> vals[i].col1=1)[0]")) \
        .withColumn('offset', expr("IF(vals[idx].col2=-1, filter(sequence(1,idx), i -> vals[idx-i].col2 != -1)[0],0)")) \
        .selectExpr("""
           inline(
             transform(vals, (x,i) -> named_struct(
                 'dta', x, 
                 'want', IF(i<idx, 0, vals[idx-offset].col2 + offset + i - idx)
               )
             )
        )""").select("dta.*", "want")
    df_new.orderBy('date').show()
    +----------+----+----+----+                                                     
    |      date|col1|col2|want|
    +----------+----+----+----+
    |2020-08-01|  -1|  -1|  11|
    |2020-08-02|  -1|  -1|  10|
    |2020-08-03|  -1|   3|   9|
    |2020-08-04|  -1|   2|   8|
    |2020-08-05|   1|  -1|   7|
    |2020-08-06|   2|  -1|   0|
    |2020-08-07|   3|  -1|   0|
    |2020-08-08|   4|   4|   0|
    |2020-08-09|   5|  -1|   0|
    +----------+----+----+----+


  Method-2: if there are too many columns, and only three cols are of concern, then use Window function:

    from pyspark.sql import Window

    w1 = Window.partitionBy().orderBy('date').rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)

    df_new = TEST_df.withColumn('vals', sort_array(collect_list(struct(*cols)).over(w1),False)) \
        .withColumn('idx', expr("filter(sequence(0,size(vals)-1), i -> vals[i].col1=1)[0]")) \
        .withColumn('offset', expr("IF(vals[idx].col2=-1, filter(sequence(1,idx), i -> vals[idx-i].col2 != -1)[0],0)")) \
        .withColumn("cur_idx", expr("array_position(vals, struct(date,col1,col2))-1")) \
        .selectExpr(*TEST_df.columns, "IF(cur_idx<idx, 0, vals[idx-offset].col2 + offset + cur_idx - idx) as want") 

  Notes:
   (1) using *TEST_df.columns will yield error if any column name contains special characters like SPACE, dot etc. 
     use one of the followings to enclose column name with backticks instead:
     
         *[f"`{}`" for c in TEST_df.colmns]
         *map(lambda c: f"`{c}`", TEST_df.columns)



(34) Use aggregate function to increment timestamp containing microsecond. 
  REF: https://stackoverflow.com/questions/63600934/pyspark-insert-rows-with-specific-timestamp-into-dataframe
  Target: add 3 extra rows for each existing row and increment a timestamp column with 
      microsecond by 5 seconds and set value to NULL
  Method: cast using timestamp() can keep the microsecond, 
          interval N second can also keep microsecond, but N must be a constant (column not allowed)

    df = spark.createDataFrame([
        ('id1', '2020-02-22 04:57:36.843', 1.4), 
        ('id2', '2020-02-22 04:57:50.850', 1.7), 
        ('id3', '2020-02-22 04:57:22.133', 1.2) 
    ], schema='id string, time string, value double') 

    df = df.withColumn('time', df['time'].astype('timestamp'))
    #DataFrame[id: string, time: timestamp, Value: double]

    df.selectExpr(
        "id", 
        """
            inline_outer(
                aggregate(
                    /* */
                    sequence(1,3), 
                    /* zero_value: array of structs with first item having time/value on the current row */
                    array((time, value)),
                    /* merge: for sequence 1 to 3, append an array of struct having time 5 second from the 
                       time of the last item in existing acc array and set the corresponding value to NULL
                     */
                    (acc, x) -> array_append(acc, named_struct(
                            "time", element_at(acc,-1).time + INTERVAL 5 SECONDS, 
                            "value", double(NULL)
                        )
                    )
                )
            )
        """
    ).show(truncate=False)
    +---+-----------------------+-----+
    |id |time                   |value|
    +---+-----------------------+-----+
    |id1|2020-02-22 04:57:36.843|1.4  |
    |id1|2020-02-22 04:57:41.843|null |
    |id1|2020-02-22 04:57:46.843|null |
    |id1|2020-02-22 04:57:51.843|null |
    |id2|2020-02-22 04:57:50.85 |1.7  |
    |id2|2020-02-22 04:57:55.85 |null |
    |id2|2020-02-22 04:58:00.85 |null |
    |id2|2020-02-22 04:58:05.85 |null |
    |id3|2020-02-22 04:57:22.133|1.2  |
    |id3|2020-02-22 04:57:27.133|null |
    |id3|2020-02-22 04:57:32.133|null |
    |id3|2020-02-22 04:57:37.133|null |
    +---+-----------------------+-----+


(35) aggregate set zero_value using the first array item and then iterate from the 2nd to the last:
  REF: https://stackoverflow.com/questions/63739214
  Benefit: no need to do type-casting when setting zero_value

    df = spark.createDataFrame([
        ('12345', 'India', 'China', '2020-10-10', '20') 
      , ('12345', 'India', 'China', '2020-10-10', '30') 
      , ('12345', 'India', 'China', '2020-10-12', '10') 
      , ('12345', 'India', 'China', '2020-10-25', '20') 
      , ('12345', 'India', 'China', '2020-10-26', '10') 
      , ('12346', 'India', 'China', '2020-10-15', '20') 
      , ('12346', 'Nepal', 'China', '2020-10-16', '20') 
      , ('12346', 'India', 'China', '2020-10-17', '50') 
      , ('12347', 'India', 'China', '2020-10-18', '20') 
      , ('12347', 'Nepal', 'China', '2020-10-19', '21') 
    ], ['container_no', 'origin', 'destination', 'shipment_dt', 'volume'])

    df1 = spark.sql("""

          SELECT container_no,
                 array_sort(collect_list(
                     struct(date(shipment_dt) AS shipment_dt, volume, origin, destination)
                 )) AS dta
          FROM {tbl}
          GROUP BY container_no

     """, tbl=df)

     spark.sql("""

     SELECT container_no,
        inline_outer(
            aggregate(
                /* iterate through array dta(through below `x`) from the 2nd item to the last item */
                slice(dta,2,size(dta)-1),
                /* set zero_value as array of structs array<struct<dta:struct<...>,start_dt:date>>
                 * and initialize it with dta[0] and its shipment_dt as start_dt
                 */
                array((dta[0] as dta, dta[0].shipment_dt as start_dt)),
                /* merge: if x.shipment_at is within 10 days of the start_at from the last item of acc,
                 * then update dta of the last item of acc and keep its original start_at
                 * otherwise, added a new item `(x as dta, x.shipment_dt as start_dt)` to acc
                 */
                (acc, x) -> CASE 
                        WHEN x.shipment_dt <= acc[0].start_dt + INTERVAL 10 days
                            THEN array_insert(
                                   slice(acc, 2, size(acc)-1),
                                   1,
                                   named_struct(
                                       "dta", IF(acc[0].dta.volume > x.volume, acc[0].dta , x),
                                       "start_dt", acc[0].start_dt
                                   )
                                )
                        ELSE
                            array_insert(acc, 1, (x as dta, x.shipment_dt as start_dt))
                        END,      
                /* finish: retrieve only the dta field of acc */
                acc -> transform(acc, x -> x.dta)
            )
        )
     FROM {tbl}
     ORDER BY container_no, shipment_dt

    """, tbl=df1).show()

 Notes:
  (1) array_insert SparkSQL function is added since Spark 3.4, below are some equivalent for different Spark versions:
      
        array_prepend(acc, x)   <-- Spark 3.5+
        array_insert(acc, 1, x) <-- Spark 3.4+
        concat(array(x), acc)   <-- before Spark 3.4

        Alternative:   transform(acc, (y,i) -> IF(i = 0, x, y))   <- Spark 2.4+

  (2) spark.sql() method with **kwargs support requires Spark 3.3+

