Continue..

Spark SQL higher-order functions:

(30) aggregate: use cast in zero_value:
  REF: https://stackoverflow.com/questions/61854735/how-to-aggregate-on-percentiles-in-pyspark
  Notes: instead of define `array(string(NULL))` in the zero_value, which yield a NULL as the first array item of `acc`
    use `cast(array_repeat(array(),2) as array<array<string>>)` which not only skip the NULL item, but also force the 
    data types of `acc` which is essential in aggregate function.

  Method-1,2: use higher-order function, need spark 2.4+. 
    * this is a very good example setting zero_value with aggregate function

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        ['a', 1, 'w'], ['a', 1, 'y'], ['a', 11, 'x'], ['a', 111, 'zzz'], ['a', 1111, 'zz'], ['a', 1111, 'zz'],
        ['b', 2, 'w'], ['b', 2, 'w'], ['b', 2, 'w'], ['b', 22, 'y'], ['b', 2222, 'x'], ['b', 2222, 'z'],
    ], ['grp', 'val1', 'val2'])

    grouped = df.groupby('grp').agg(
        F.count('*').alias('count'),
        F.expr('percentile(val1, array(0.5, 0.75)) as percentiles'),
        F.collect_list(F.struct('val1','val2')).alias('vals')
    ) 

    grouped.selectExpr(
      "grp",
      "count",
      "percentiles",
      """
        /* less efficient version */
        transform(percentiles, x -> 
          size(
            array_distinct(
              transform(filter(vals, y -> y.val1 > x), z -> z.val2)
            )
          )
        ) as distinct_count
      """,
      """
        /* more efficient version */
        aggregate( 
          vals, 
          /* more robust way: cast(array_repeat(array(),size(percentiles)) as array<array<string>>)*/
          cast(array_repeat(array(),2) as array<array<string>>), 
          (acc, y) -> 
            transform(acc, (x,i) -> IF(y.val1 > percentiles[i], concat(x, array(y.val2)), x)),
          acc -> transform(acc, x -> size(array_distinct(x))) 
        ) as distinct_count_1 
      """
    ).show(10,0)                                                                                                       
    +---+-----+--------------+--------------+----------------+
    |grp|count|percentiles   |distinct_count|distinct_count_1|
    +---+-----+--------------+--------------+----------------+
    |a  |6    |[61.0, 861.0] |[2, 1]        |[2, 1]          |
    |b  |6    |[12.0, 1672.0]|[3, 2]        |[3, 2]          |
    +---+-----+--------------+--------------+----------------+

  Method-3: just do the pre-processing before aggregation which does not require Spark 2.4+
    
    from pyspark.sql import Window, functions as F
    
    w1 = Window.partitionBy('grp')
    
    df1 = df.withColumn('percentiles', F.expr('percentile(val1, array(0.5, 0.75))').over(w1)) \
        .withColumn('c1', F.expr('IF(val1>percentiles[0],val2,NULL)')) \
        .withColumn('c2', F.expr('IF(val1>percentiles[1],val2,NULL)'))
    
    grouped = df1.groupby('grp').agg(
        F.count('*').alias('count'), 
        F.first('percentiles').alias('percentiles'), 
        F.array(F.countDistinct('c1'), F.countDistinct('c2')).alias('distinct_count')
    )
    grouped.show()
    +---+-----+--------------+--------------+                                       
    |grp|count|   percentiles|distinct_count|
    +---+-----+--------------+--------------+
    |  b|    6|[12.0, 1672.0]|        [3, 2]|
    |  a|    6| [61.0, 861.0]|        [2, 1]|
    +---+-----+--------------+--------------+



(31) efficient way using aggregate to create and update a Map:
  REF: https://stackoverflow.com/questions/61947677/pyspark-higher-order-sql-functions-to-create-histograms-from-arrays

  Notes: to update a Map:
   (1) Insert a new key: use `map_concat(acc, map(y,1))`
   (2) Update a existing key, increment the value by 1 for matched key only
    for Spark 2.4+:  map_from_entries(transform(map_keys(acc), k -> (k as key, acc[k] + int(k=y) as val)))
    for Spark 3.0+:  transform_values(acc, (k,v) -> v + int(k=y))
    
    df = spark.createDataFrame([(["val1", "val2", "val1", "val1", "val3", "val2", "val1"],)],["vals"])

    # Spark 2.4+
    df.selectExpr("*", """ 
      aggregate( 
        vals,  
        cast(map() as map<string,int>), 
        (acc,y) -> 
          IF(acc[y] is NULL 
              /* insert a new key */
            , map_concat(acc, map(y,1))
              /* update an existing key */
            , map_from_entries(transform(map_keys(acc), k -> (k as key, acc[k] + int(k=y) as val))) 
          ) 
      ) as new_data 
    """).show(1,0)      
    +------------------------------------------+---------------------------------+
    |vals                                      |new_data                         |
    +------------------------------------------+---------------------------------+
    |[val1, val2, val1, val1, val3, val2, val1]|[val1 -> 4, val2 -> 2, val3 -> 1]|
    +------------------------------------------+---------------------------------+
    
    # Spark 3.0+, using transform_values:
    df.selectExpr("*", """                                                     
      aggregate(vals, 
        cast(map() as map<string,int>),
        (acc,y) -> IF(acc[y] is NULL, map_concat(acc, map(y,1)), transform_values(acc, (k,v) -> v + int(k=y)))
      ) as new_data
    """).show(1,0)
    +------------------------------------------+---------------------------------+
    |vals                                      |new_data                         |
    +------------------------------------------+---------------------------------+
    |[val1, val2, val1, val1, val3, val2, val1]|[val1 -> 4, val2 -> 2, val3 -> 1]|
    +------------------------------------------+---------------------------------+
    
  Note: a similar example https://stackoverflow.com/questions/62422501/how-to-aggregate-values-within-array-in-pyspark  



(32) Use aggregate function to do stateful aggregation, no need to pre-sort data by using a MapType column.
  REF: https://stackoverflow.com/questions/61965306/how-can-i-calculate-a-moving-average-with-certain-days-missing

  Target: calculate a moving average: today's score + (yesterdya's score * .50) 
  Notes: days could be missing, also post-processing to filter days with score < 0.1

    from pyspark.sql.functions import expr
  
    df = spark.read.csv("/home/xicheng/test/window-25.txt", header=True, inferSchema=True)
    +---+----+-----+
    |day|user|score|
    +---+----+-----+
    |  0|   a|  1.1|
    |  0|   b|  0.6|
    |  1|   a|  0.2|
    |  1|   b|  0.2|
    |  1|   c|  0.6|
    |  2|   c|  1.1|
    |  3|   a|  0.2|
    |  3|   b|  0.7|
    +---+----+-----+
  
    # below create a MapType column: `data` with key=`day` and value=`score`
    df2 = df.groupby('user').agg(expr("map_from_entries(collect_list(struct(day,score))) as data"))
  
    df2.selectExpr("user", """ 
       
        inline( 
          aggregate( 
            /* iterate through 1 -> 6 */
            sequence(0,6), 
            /* zero_value as an array of structs: array<struct<day:int,score:double>> */
            array((int(NULL) as day, double(NULL) as score)), 
            /* reduce, iterate by `i` */
            (acc, i) -> 
              CASE
                /* first element, setup acc based on i and data[i] */
                WHEN (element_at(acc,-1).day is NULL THEN array((i as day, ifnull(data[i],0) as score)) 
                /* else, append an new element using day=i and score calculated by 
                 * the previous score: element_at(acc,-1).score 
                 * and the current score: ifnull(data[i],0)
                 */
                ELSE concat(acc, array((i as day, 0.5*element_at(acc,-1).score + ifnull(data[i],0) as score)))
              END
            /* remove array items with score < 0.1 */
            acc -> filter(acc, x -> x.score >= 0.1) 
          ) 
        ) 
  
    """).show(22)
    +----+---+-------------------+                                                  
    |user|day|              score|
    +----+---+-------------------+
    |   c|  1|                0.6|
    |   c|  2| 1.4000000000000001|
    |   c|  3| 0.7000000000000001|
    |   c|  4|0.35000000000000003|
    |   c|  5|0.17500000000000002|
    |   b|  0|                0.6|
    |   b|  1|                0.5|
    |   b|  2|               0.25|
    |   b|  3|              0.825|
    |   b|  4|             0.4125|
    |   b|  5|            0.20625|
    |   b|  6|           0.103125|
    |   a|  0|                1.1|
    |   a|  1|               0.75|
    |   a|  2|              0.375|
    |   a|  3|             0.3875|
    |   a|  4|            0.19375|
    +----+---+-------------------+


  

(33) use sequence + filter to find the index of a specific value in an array of structs

  REF: https://stackoverflow.com/questions/63290611/pyspark-how-to-code-complicated-dataframe-calculation#63290611

  Target: find idx of an item and do some complex calculation based on this index.

  (1) groupby and create a collect_list of all related rows(`vals` in below code), sort the list by date in desencending 
    order. Note: change groupby(lit(1)) to whatever column you can use to divide you data into independant subset.
  (2) find the array index `idx` which has `col1 == 1`
  (3) if col2==-1 at `idx`, find the offset from idx to the beginning of the list with the first row having col2 != -1
    Note: in the current code, offset might be NULL if all col2 before `idx` are -1, you will have to decide what you want
  (4) after we have offset and idx, the `want` can be calculated by:

      IF(i<idx, 0, vals[idx-offset].col2 + offset + i - idx)

  See below:

    from pyspark.sql.functions import sort_array, collect_list, struct, expr, lit

    TEST_df = spark.createDataFrame([
      ('2020-08-01', -1, -1), ('2020-08-02', -1, -1), ('2020-08-03', -1, 3), 
      ('2020-08-04', -1, 2), ('2020-08-05', 1, -1), ('2020-08-06', 2, -1), 
      ('2020-08-07', 3, -1), ('2020-08-08', 4, 4), ('2020-08-09', 5, -1) 
    ], ['date', 'col1', 'col2'])

    cols = ["date", "col1", "col2"]

    df_new = TEST_df \
        .groupby(lit(1)) \
        .agg(sort_array(collect_list(struct(*cols)),False).alias('vals')) \
        .withColumn('idx', expr("filter(sequence(0,size(vals)-1), i -> vals[i].col1=1)[0]")) \
        .withColumn('offset', expr("IF(vals[idx].col2=-1, filter(sequence(1,idx), i -> vals[idx-i].col2 != -1)[0],0)")) \
        .selectExpr("""
           inline(
             transform(vals, (x,i) -> named_struct(
                 'date', x.date, 
                 'col1', x.col1, 
                 'col2', x.col2, 
                 'want', IF(i<idx, 0, vals[idx-offset].col2 + offset + i - idx)
               )
             )
        )""")
    df_new.orderBy('date').show()
    +----------+----+----+----+                                                     
    |      date|col1|col2|want|
    +----------+----+----+----+
    |2020-08-01|  -1|  -1|  11|
    |2020-08-02|  -1|  -1|  10|
    |2020-08-03|  -1|   3|   9|
    |2020-08-04|  -1|   2|   8|
    |2020-08-05|   1|  -1|   7|
    |2020-08-06|   2|  -1|   0|
    |2020-08-07|   3|  -1|   0|
    |2020-08-08|   4|   4|   0|
    |2020-08-09|   5|  -1|   0|
    +----------+----+----+----+


  Method-2: if there are too many columns, and only three cols are of concern, then use Window function:

    from pyspark.sql import Window

    w1 = Window.partitionBy().orderBy('date').rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)

    df_new = TEST_df.withColumn('vals', sort_array(collect_list(struct(*cols)).over(w1),False)) \
        .withColumn('idx', expr("filter(sequence(0,size(vals)-1), i -> vals[i].col1=1)[0]")) \
        .withColumn('offset', expr("IF(vals[idx].col2=-1, filter(sequence(1,idx), i -> vals[idx-i].col2 != -1)[0],0)")) \
        .withColumn("cur_idx", expr("array_position(vals, struct(date,col1,col2))-1")) \
        .selectExpr(*TEST_df.columns, "IF(cur_idx<idx, 0, vals[idx-offset].col2 + offset + cur_idx - idx) as want") 

  Notes:
   (1) using *TEST_df.columns will yield error if any column name contains special characters like SPACE, dot etc. 
     use one of the followings to enclose column name with backticks instead:
     
         *[f"`{}`" for c in TEST_df.colmns]
         *map(lambda c: f"`{c}`", TEST_df.columns)



(34) Use aggregate function to increment timestamp containing microsecond. 
  REF: https://stackoverflow.com/questions/63600934/pyspark-insert-rows-with-specific-timestamp-into-dataframe
  Target: add 3 extra rows for each existing row and increment a timestamp column with 
      microsecond by 5 seconds and set value to NULL
  Method: cast using timestamp() can keep the microsecond, 
          interval N second can also keep microsecond, but N must be a constant (column not allowed)

    df = spark.createDataFrame([
        ('id1', '2020-02-22 04:57:36.843', 1.4), 
        ('id2', '2020-02-22 04:57:50.850', 1.7), 
        ('id3', '2020-02-22 04:57:22.133', 1.2) 
    ], schema='id string, time string, value double') 

    df = df.withColumn('time', df['time'].astype('timestamp'))
    #DataFrame[id: string, time: timestamp, Value: double]

    df.selectExpr(
        "id", 
        """
          inline_outer(
            aggregate(
              /* */
              sequence(1,3), 
              /* zero_value: array of structs with first item having time/value on the current row */
              array((time, value)),
              /* merge: for sequence 1 to 3, append an array of struct having time 5 second from the 
                 time of the last item in existing acc array and set the corresponding value to NULL */
              (acc, x) -> concat(acc, array((element_at(acc,-1).time + interval 5 seconds as time, NULL as value)))
            )
          )
        """
    ).show(truncate=False)
    +---+-----------------------+-----+
    |id |time                   |value|
    +---+-----------------------+-----+
    |id1|2020-02-22 04:57:36.843|1.4  |
    |id1|2020-02-22 04:57:41.843|null |
    |id1|2020-02-22 04:57:46.843|null |
    |id1|2020-02-22 04:57:51.843|null |
    |id2|2020-02-22 04:57:50.85 |1.7  |
    |id2|2020-02-22 04:57:55.85 |null |
    |id2|2020-02-22 04:58:00.85 |null |
    |id2|2020-02-22 04:58:05.85 |null |
    |id3|2020-02-22 04:57:22.133|1.2  |
    |id3|2020-02-22 04:57:27.133|null |
    |id3|2020-02-22 04:57:32.133|null |
    |id3|2020-02-22 04:57:37.133|null |
    +---+-----------------------+-----+


