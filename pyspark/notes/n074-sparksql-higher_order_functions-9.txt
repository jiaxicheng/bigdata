Continue..

Spark SQL higher-order functions:

(30) aggregate: use cast in zero_value:
  REF: https://stackoverflow.com/questions/61854735/how-to-aggregate-on-percentiles-in-pyspark
  Notes: instead of define `array(string(NULL))` in the zero_value, which yield a NULL as the first array item of `acc`
    use `cast(array_repeat(array(),2) as array<array<string>>)` which not only skip the NULL item, but also force the 
    data types of `acc` which is essential in aggregate function.

  Method-1,2: use higher-order function, need spark 2.4+. 
    * this is a very good example setting zero_value with aggregate function

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        ['a', 1, 'w'], ['a', 1, 'y'], ['a', 11, 'x'], ['a', 111, 'zzz'], ['a', 1111, 'zz'], ['a', 1111, 'zz'],
        ['b', 2, 'w'], ['b', 2, 'w'], ['b', 2, 'w'], ['b', 22, 'y'], ['b', 2222, 'x'], ['b', 2222, 'z'],
    ], ['grp', 'val1', 'val2'])

    grouped = df.groupby('grp').agg(
        F.count('*').alias('count'),
        F.expr('percentile(val1, array(0.5, 0.75)) as percentiles'),
        F.collect_list(F.struct('val1','val2')).alias('vals')
    ) 

    grouped.selectExpr(
      "grp",
      "count",
      "percentiles",
      """
        /* less efficient version */
        transform(percentiles, x -> 
          size(
            array_distinct(
              transform(filter(vals, y -> y.val1 > x), z -> z.val2)
            )
          )
        ) as distinct_count
      """,
      """
        /* more efficient version */
        aggregate( 
          vals, 
          /* more robust way: cast(array_repeat(array(),size(percentiles)) as array<array<string>>)*/
          cast(array_repeat(array(),2) as array<array<string>>), 
          (acc, y) -> 
            transform(acc, (x,i) -> IF(y.val1 > percentiles[i], concat(x, array(y.val2)), x)),
          acc -> transform(acc, x -> size(array_distinct(x))) 
        ) as distinct_count_1 
      """
    ).show(10,0)                                                                                                       
    +---+-----+--------------+--------------+----------------+
    |grp|count|percentiles   |distinct_count|distinct_count_1|
    +---+-----+--------------+--------------+----------------+
    |a  |6    |[61.0, 861.0] |[2, 1]        |[2, 1]          |
    |b  |6    |[12.0, 1672.0]|[3, 2]        |[3, 2]          |
    +---+-----+--------------+--------------+----------------+

  Method-3: just do the pre-processing before aggregation which does not require Spark 2.4+
    
    from pyspark.sql import Window, functions as F
    
    w1 = Window.partitionBy('grp')
    
    df1 = df.withColumn('percentiles', F.expr('percentile(val1, array(0.5, 0.75))').over(w1)) \
        .withColumn('c1', F.expr('IF(val1>percentiles[0],val2,NULL)')) \
        .withColumn('c2', F.expr('IF(val1>percentiles[1],val2,NULL)'))
    
    grouped = df1.groupby('grp').agg(
        F.count('*').alias('count'), 
        F.first('percentiles').alias('percentiles'), 
        F.array(F.countDistinct('c1'), F.countDistinct('c2')).alias('distinct_count')
    )
    grouped.show()
    +---+-----+--------------+--------------+                                       
    |grp|count|   percentiles|distinct_count|
    +---+-----+--------------+--------------+
    |  b|    6|[12.0, 1672.0]|        [3, 2]|
    |  a|    6| [61.0, 861.0]|        [2, 1]|
    +---+-----+--------------+--------------+



(31) efficient way using aggregate to create and update a Map:
  REF: https://stackoverflow.com/questions/61947677/pyspark-higher-order-sql-functions-to-create-histograms-from-arrays

  Notes: to update a Map:
   (1) Insert a new key: use `map_concat(acc, map(y,1))`
   (2) Update a existing key, increment the value by 1 for matched key only
    for Spark 2.4+:  map_from_entries(transform(map_keys(acc), k -> (k as key, acc[k] + int(k=y) as val)))
    for Spark 3.0+:  transform_values(acc, (k,v) -> v + int(k=y))
    
    df = spark.createDataFrame([(["val1", "val2", "val1", "val1", "val3", "val2", "val1"],)],["vals"])

    # Spark 2.4+
    df.selectExpr("*", """ 
      aggregate( 
        vals,  
        cast(map() as map<string,int>), 
        (acc,y) -> 
          IF(acc[y] is NULL 
              /* insert a new key */
            , map_concat(acc, map(y,1))
              /* update an existing key */
            , map_from_entries(transform(map_keys(acc), k -> (k as key, acc[k] + int(k=y) as val))) 
          ) 
      ) as new_data 
    """).show(1,0)      
    +------------------------------------------+---------------------------------+
    |vals                                      |new_data                         |
    +------------------------------------------+---------------------------------+
    |[val1, val2, val1, val1, val3, val2, val1]|[val1 -> 4, val2 -> 2, val3 -> 1]|
    +------------------------------------------+---------------------------------+
    
    # Spark 3.0+, using transform_values:
    df.selectExpr("*", """                                                     
      aggregate(vals, 
        cast(map() as map<string,int>),
        (acc,y) -> IF(acc[y] is NULL, map_concat(acc, map(y,1)), transform_values(acc, (k,v) -> v + int(k=y)))
      ) as new_data
    """).show(1,0)
    +------------------------------------------+---------------------------------+
    |vals                                      |new_data                         |
    +------------------------------------------+---------------------------------+
    |[val1, val2, val1, val1, val3, val2, val1]|[val1 -> 4, val2 -> 2, val3 -> 1]|
    +------------------------------------------+---------------------------------+
    
    
    
(32) Use aggregate function to do state-ful aggregation, no need to pre-sort data by using a MapType column.
  REF: https://stackoverflow.com/questions/61965306/how-can-i-calculate-a-moving-average-with-certain-days-missing

  Target: calculate a moving average: today's score + (yesterdya's score * .50) 
  Notes: days could be missing, also post-processing to filter days with score < 0.1

    from pyspark.sql.functions import expr
  
    df = spark.read.csv("/home/xicheng/test/window-25.txt", header=True, inferSchema=True)
    +---+----+-----+
    |day|user|score|
    +---+----+-----+
    |  0|   a|  1.1|
    |  0|   b|  0.6|
    |  1|   a|  0.2|
    |  1|   b|  0.2|
    |  1|   c|  0.6|
    |  2|   c|  1.1|
    |  3|   a|  0.2|
    |  3|   b|  0.7|
    +---+----+-----+
  
    # below create a MapType column: `data` with key=`day` and value=`score`
    df2 = df.groupby('user').agg(F.expr("map_from_entries(collect_list(struct(day,score))) as data"))
  
    df2.selectExpr("user", """ 
       
        inline( 
          aggregate( 
            /* iterate through 1 -> 6 */
            sequence(0,6), 
            /* zero_value as an array of structs: array<struct<day:int,score:double>> */
            array((int(NULL) as day, double(NULL) as score)), 
            /* reduce, iterate by `i` */
            (acc, i) -> 
              CASE
                /* first element, setup acc based on i and data[i] */
                WHEN (element_at(acc,-1).day is NULL THEN array((i as day, ifnull(data[i],0) as score)) 
                /* else, append an new element using day=i and score calculated by 
                 * the previous score: element_at(acc,-1).score 
                 * and the current score: ifnull(data[i],0)
                 */
                ELSE concat(acc, array((i as day, 0.5*element_at(acc,-1).score + ifnull(data[i],0) as score)))
              END
            /* remove array items with score < 0.1 */
            acc -> filter(acc, x -> x.score >= 0.1) 
          ) 
        ) 
  
    """).show(22)
    +----+---+-------------------+                                                  
    |user|day|              score|
    +----+---+-------------------+
    |   c|  1|                0.6|
    |   c|  2| 1.4000000000000001|
    |   c|  3| 0.7000000000000001|
    |   c|  4|0.35000000000000003|
    |   c|  5|0.17500000000000002|
    |   b|  0|                0.6|
    |   b|  1|                0.5|
    |   b|  2|               0.25|
    |   b|  3|              0.825|
    |   b|  4|             0.4125|
    |   b|  5|            0.20625|
    |   b|  6|           0.103125|
    |   a|  0|                1.1|
    |   a|  1|               0.75|
    |   a|  2|              0.375|
    |   a|  3|             0.3875|
    |   a|  4|            0.19375|
    +----+---+-------------------+

  

