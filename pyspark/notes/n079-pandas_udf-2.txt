Continue..

pandas_udf and related applications:
---

Example-3: cumprod + GROUPED_MAP

https://stackoverflow.com/questions/59212758/pandas-to-pyspark-cumprod-function

Using PandasUDFType.GROUPED_MAP when PandasUDFType.GROUPED_AGG is not applicable:
Notes: this is an example when PandasUDFType.GROUPED_AGG is NOT working, for Window 
       aggregation, pandas_udf requires a unbounded Window.


Method-1: using pandas_udf / PandasUDFType.GROUPED_MAP

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType
    schema = StructType.fromJson(df.schema.jsonValue()).add('col4', 'double')    

    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)
    def udf_cumprod(key, pdf):
        # notice that sorting is important here
        pdf = pdf.sort_values('col2')
        pdf['col4'] = (1.0 - pdf['col3']).cumprod().round(4)
        return pdf

    df.groupby('col1').apply(udf_cumprod).show()
    +----+----+----+------+                                                         
    |col1|col2|col3|  col4|
    +----+----+----+------+
    |   1|   1| 0.9|   0.1|
    |   1|   2|0.13| 0.087|
    |   1|   3| 0.5|0.0435|
    |   1|   4| 1.0|   0.0|
    |   1|   5| 0.6|   0.0|
    +----+----+----+------+


Method-2: for Spark 2.4+, using Window + collect_list + aggregate

    from pyspark.sql.functions import collect_list, col

    w1 = Window.partitionBy('col1').orderBy('col2')
    df.withColumn('col4_arr', collect_list(1.0-col('col3')).over(w1)) \
      .selectExpr(
          '*'
        , 'aggregate(col4_arr, double(1.0), (x,y) -> x*y, z -> round(z,4)) as col4'
      ).show(truncate=False)
    +----+----+----+------------------------------------------+------+              
    |col1|col2|col3|col4_arr                                  |col4  |
    +----+----+----+------------------------------------------+------+
    |1   |1   |0.9 |[0.09999999999999998]                     |0.1   |
    |1   |2   |0.13|[0.09999999999999998, 0.87]               |0.087 |
    |1   |3   |0.5 |[0.09999999999999998, 0.87, 0.5]          |0.0435|
    |1   |4   |1.0 |[0.09999999999999998, 0.87, 0.5, 0.0]     |0.0   |
    |1   |5   |0.6 |[0.09999999999999998, 0.87, 0.5, 0.0, 0.4]|0.0   |
    +----+----+----+------------------------------------------+------+




Example-4: jaro distance + list comprehension + PandasUDFType.SCALAR
  Notes: input Series, output Series in the same size
         used in withColumn or select list.

   REF: 
    [1] question post: https://stackoverflow.com/questions/59212255
    [2] discussion df.apply: https://stackoverflow.com/questions/52673285

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyjarowinkler import distance
    from pandas import Series

    # preferred method
    @pandas_udf("float", PandasUDFType.SCALAR)
    def get_distance(col1, col2):
       return Series([ distance.get_jaro_distance(x, y, winkler=True, scaling=0.1) for x,y in zip(col1, col2)])

    df.withColumn('jaro_distance', get_distance('x', 'y')).show()
    +---+---+---+-------------+
    |  x|  y|  z|jaro_distance|
    +---+---+---+-------------+
    | AB| 1B|  2|         0.67|
    | BB| BB|  4|          1.0|
    | CB| 5D|  6|          0.0|
    | DB|B7F|  8|         0.61|
    +---+---+---+-------------+




Example-5: Timestamp functions + replace + PandasUDFType.SCALAR
---
    REF: https://stackoverflow.com/questions/59063138/run-pyspark-date-column-thru-datetime-pandas-function

    df = spark.range(10) \
        .withColumn('pd_date', expr('timestamp(from_unixtime(unix_timestamp(now())+monotonically_increasing_id()*2500))')) 

    from pyspark.sql.functions import expr, pandas_udf, PandasUDFType 
    from pandas import Series

    @pandas_udf('timestamp', PandasUDFType.SCALAR)
    def add_one(pd_date):
        return Series([ t.replace(hour=7,minute=0) if t.minute > 30 else t for t in pd_date.tolist() ])

    df.withColumn('ao', add_one('pd_date')).show()                                                                     
    +---+-------------------+-------------------+
    | id|            pd_date|                 ao|
    +---+-------------------+-------------------+
    |  0|2019-12-06 21:42:09|2019-12-06 07:00:09|
    |  1|2019-12-06 22:23:49|2019-12-06 22:23:49|
    |  2|2019-12-06 23:05:29|2019-12-06 23:05:29|
    |  3|2019-12-06 23:47:09|2019-12-06 07:00:09|
    |  4|2019-12-07 00:28:49|2019-12-07 00:28:49|
    |  5|2019-12-07 01:10:29|2019-12-07 01:10:29|
    |  6|2019-12-07 01:52:09|2019-12-07 07:00:09|
    |  7|2019-12-07 02:33:49|2019-12-07 07:00:49|
    |  8|2019-12-07 03:15:29|2019-12-07 03:15:29|
    |  9|2019-12-07 03:57:09|2019-12-07 07:00:09|
    +---+-------------------+-------------------+



Example-6: GROUPED_MAP: run sklearn machine-learning code per group/id independantly:
---
  REF: https://stackoverflow.com/questions/60112968/combining-pyspark-and-dbscan-with-pandas-udf
  Below example to run sklearn.cluster.DBSCAN on each id: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html

Use pandas_udf:
    
    from sklearn.cluster import DBSCAN
    from pyspark.sql.functions import pandas_udf
    
    data = [(1, 11.6133, 48.1075),
             (1, 11.6142, 48.1066),
             (1, 11.6108, 48.1061),
             (1, 11.6207, 48.1192),
             (1, 11.6221, 48.1223),
             (1, 11.5969, 48.1276),
             (2, 11.5995, 48.1258),
             (2, 11.6127, 48.1066),
             (2, 11.6430, 48.1275),
             (2, 11.6368, 48.1278),
             (2, 11.5930, 48.1156)]
    
    df = spark.createDataFrame(data, ["id", "X", "Y"])
    
    @pandas_udf("id long, X double, Y double, cluster double", PandasUDFType.GROUPED_MAP)
    def dbscan_udf(pdf):
        pdf['cluster'] = DBSCAN(eps=0.005, min_samples=3).fit_predict(pdf[['X', 'Y']])
        return pdf
    
In another form:
    
    dbscan_udf = pandas_udf(
        lambda pdf: pdf.assign(cluster=DBSCAN(eps=0.005, min_samples=3).fit_predict(pdf[['X', 'Y']])),
        "id long, X double, Y double, cluster double", 
        PandasUDFType.GROUPED_MAP
    )
    
    df.groupby("id").apply(dbscan_udf).show()

