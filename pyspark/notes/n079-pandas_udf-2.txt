Continue..

pandas_udf and related applications:
---

Example-3: cumprod + GROUPED_MAP | Spark 3.0+ Window aggregate function Series -> Scalar

https://stackoverflow.com/questions/59212758/pandas-to-pyspark-cumprod-function

Using PandasUDFType.GROUPED_MAP when PandasUDFType.GROUPED_AGG is not applicable:
Notes: this is an example when PandasUDFType.GROUPED_AGG is NOT working, for Window 
       aggregation, pandas_udf requires a unbounded Window.


Method-1: using pandas_udf / PandasUDFType.GROUPED_MAP / Spark 3.0: Series -> Scalar

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType

    df = spark.createDataFrame([(1, 1,0.9), (1, 2,0.13), (1, 3,0.5), (1, 4,1.0), (1, 5,0.6)], ['col1', 'col2','col3'])

    schema = StructType.fromJson(df.schema.jsonValue()).add('col4', 'double')    

    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)
    def udf_cumprod(key, pdf):
        # notice that sorting is important here
        pdf = pdf.sort_values('col2')
        pdf['col4'] = (1.0 - pdf['col3']).cumprod().round(4)
        return pdf

    df.groupby('col1').apply(udf_cumprod).show()
    +----+----+----+------+                                                         
    |col1|col2|col3|  col4|
    +----+----+----+------+
    |   1|   1| 0.9|   0.1|
    |   1|   2|0.13| 0.087|
    |   1|   3| 0.5|0.0435|
    |   1|   4| 1.0|   0.0|
    |   1|   5| 0.6|   0.0|
    +----+----+----+------+


  For Spark 3.0+ (from pd.Series -> Any scalar) which supports not just unbounded Window frame

    from pyspark.sql import Window
    import pandas as pd
    
    w1 = Window.partitionBy('col1').orderBy('col2')

    @pandas_udf("double")
    def udf_cumprod(x:pd.Series) -> float:
      return (1-x).prod().round(4)
    df.withColumn("col4", udf_cumprod('col3').over(w1)).show()
    +----+----+----+------+                                                         
    |col1|col2|col3|  col4|
    +----+----+----+------+
    |   1|   1| 0.9|   0.1|
    |   1|   2|0.13| 0.087|
    |   1|   3| 0.5|0.0435|
    |   1|   4| 1.0|   0.0|
    |   1|   5| 0.6|   0.0|
    +----+----+----+------+


Method-2: for Spark 2.4+, using Window + collect_list + aggregate

    from pyspark.sql.functions import collect_list, col

    w1 = Window.partitionBy('col1').orderBy('col2')
    df.withColumn('col4_arr', collect_list(1.0-col('col3')).over(w1)) \
      .selectExpr(
          '*'
        , 'aggregate(col4_arr, double(1.0), (x,y) -> x*y, z -> round(z,4)) as col4'
      ).show(truncate=False)
    +----+----+----+------------------------------------------+------+              
    |col1|col2|col3|col4_arr                                  |col4  |
    +----+----+----+------------------------------------------+------+
    |1   |1   |0.9 |[0.09999999999999998]                     |0.1   |
    |1   |2   |0.13|[0.09999999999999998, 0.87]               |0.087 |
    |1   |3   |0.5 |[0.09999999999999998, 0.87, 0.5]          |0.0435|
    |1   |4   |1.0 |[0.09999999999999998, 0.87, 0.5, 0.0]     |0.0   |
    |1   |5   |0.6 |[0.09999999999999998, 0.87, 0.5, 0.0, 0.4]|0.0   |
    +----+----+----+------------------------------------------+------+




Example-4: jaro distance + Iterator of Multiple Series to Iterator of Series
  Notes: input Series, output Series in the same size
         used in withColumn or select list.

   REF: 
    [1] question post: https://stackoverflow.com/questions/59212255
    [2] discussion df.apply: https://stackoverflow.com/questions/52673285

    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyjarowinkler import distance
    from pandas import Series
    from typing import Iterator, Tuple

    df = spark.createDataFrame([('AB','1B',2),('BB','BB',4),('CB','5D',6),('DB','B7F',8)], ['x','y','z']) 

    # method using Spark 3.0+
    @pandas_udf("float")
    def get_distance(iterator: Iterator[Tuple[Series,Series]]) -> Iterator[Series]:
      for col1, col2 in iterator:
        yield Series([ distance.get_jaro_distance(x, y, winkler=True, scaling=0.1) for x,y in zip(col1, col2)])

    df.withColumn('jaro_distance', get_distance('x', 'y')).show()
    +---+---+---+-------------+
    |  x|  y|  z|jaro_distance|
    +---+---+---+-------------+
    | AB| 1B|  2|         0.67|
    | BB| BB|  4|          1.0|
    | CB| 5D|  6|          0.0|
    | DB|B7F|  8|         0.61|
    +---+---+---+-------------+



Example-5: Timestamp functions + replace + PandasUDFType.SCALAR
---
    REF: https://stackoverflow.com/questions/59063138/run-pyspark-date-column-thru-datetime-pandas-function

    df = spark.range(10) \
        .withColumn('pd_date', expr('timestamp(from_unixtime(unix_timestamp(now())+monotonically_increasing_id()*2500))')) 

    from pyspark.sql.functions import expr, pandas_udf, PandasUDFType 
    from pandas import Series

    @pandas_udf('timestamp', PandasUDFType.SCALAR)
    def add_one(pd_date):
        return Series([ t.replace(hour=7,minute=0) if t.minute > 30 else t for t in pd_date.tolist() ])

    df.withColumn('ao', add_one('pd_date')).show()                                                                     
    +---+-------------------+-------------------+
    | id|            pd_date|                 ao|
    +---+-------------------+-------------------+
    |  0|2019-12-06 21:42:09|2019-12-06 07:00:09|
    |  1|2019-12-06 22:23:49|2019-12-06 22:23:49|
    |  2|2019-12-06 23:05:29|2019-12-06 23:05:29|
    |  3|2019-12-06 23:47:09|2019-12-06 07:00:09|
    |  4|2019-12-07 00:28:49|2019-12-07 00:28:49|
    |  5|2019-12-07 01:10:29|2019-12-07 01:10:29|
    |  6|2019-12-07 01:52:09|2019-12-07 07:00:09|
    |  7|2019-12-07 02:33:49|2019-12-07 07:00:49|
    |  8|2019-12-07 03:15:29|2019-12-07 03:15:29|
    |  9|2019-12-07 03:57:09|2019-12-07 07:00:09|
    +---+-------------------+-------------------+

  for Spark 3.0 use Iterator[Series] -> Iterator[Series]
  Note: Arrow supports timestamp, but not array of timestamps

    from typing import Iterator

    @pandas_udf('timestamp')
    def add_one(iterator: Iterator[Series]) -> Iterator[Series]:
      for pd_date in iterator:
        yield Series([ t.replace(hour=7,minute=0) if t.minute > 30 else t for t in pd_date.tolist() ])

    df.withColumn('ao', add_one('pd_date')).show()




Example-6: ApplyInPandas to run sklearn machine-learning code per group/id independantly:
---
  REF: https://stackoverflow.com/questions/60112968/combining-pyspark-and-dbscan-with-pandas-udf
  Below example to run sklearn.cluster.DBSCAN on each id: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html

Use pandas_udf:
    
    from sklearn.cluster import DBSCAN
    from pyspark.sql.functions import pandas_udf
    
    data = [(1, 11.6133, 48.1075),
             (1, 11.6142, 48.1066),
             (1, 11.6108, 48.1061),
             (1, 11.6207, 48.1192),
             (1, 11.6221, 48.1223),
             (1, 11.5969, 48.1276),
             (2, 11.5995, 48.1258),
             (2, 11.6127, 48.1066),
             (2, 11.6430, 48.1275),
             (2, 11.6368, 48.1278),
             (2, 11.5930, 48.1156)]
    
    df = spark.createDataFrame(data, ["id", "X", "Y"])
    
    schema = StructType.fromJson(df.schema.jsonValue()).add('cluster', 'double')    

    # type hint is not required
    def dbscan_udf(pdf):
        pdf['cluster'] = DBSCAN(eps=0.005, min_samples=3).fit_predict(pdf[['X', 'Y']])
        return pdf
    
    df.groupby('id').applyInPandas(dbscan_udf,schema).show()
    +---+-------+-------+-------+                                                   
    | id|      X|      Y|cluster|
    +---+-------+-------+-------+
    |  1|11.6133|48.1075|    0.0|
    |  1|11.6142|48.1066|    0.0|
    |  1|11.6108|48.1061|    0.0|
    |  1|11.6207|48.1192|   -1.0|
    |  1|11.6221|48.1223|   -1.0|
    |  1|11.5969|48.1276|   -1.0|
    |  2|11.5995|48.1258|   -1.0|
    |  2|11.6127|48.1066|   -1.0|
    |  2| 11.643|48.1275|   -1.0|
    |  2|11.6368|48.1278|   -1.0|
    |  2| 11.593|48.1156|   -1.0|
    +---+-------+-------+-------+


