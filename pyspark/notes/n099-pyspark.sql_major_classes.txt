pyspark.sql notes:

group all related method/functions under the same class to have a full understanding about the 
current pyspark.sql library:


pyspark.sql.SparkSession:
---
  + properties:
    + conf
    + catalog
    + SparkContext
    + streams
    + read
    + readStream
    + udf
    + version
  + builder
    + appName()
    + config(key=None, value=Nonee, conf=None)
      ** use either key/value pairs or conf= and existing SparkConf()
    + master()
    + getOrCreate()
    + enableHiveSupport()
  + createDataFrame(data, schema=None, samplingRatio=Nove, verifySchema=True)
  + range(start, end=None, step=1, numPArtitions=None)
    * if only one argument is specified, it will be used as the `end` value
  + sql(sqlQuery)
  + table(tableName): returns the table name as a dataframe
  + newSession()
    stop()

  Notes:
   (1) below are very useful in testing codes:
     + testing DF-API functions: spark.range(1).select('...').show()
     + testing Spark SQL       : spark.sql('select ...').show()
   (2) data in spark.createDataFrame() can be:
     + an RDD of any kind of SQL data representation
     + list
     + pandas.DataFrame
   (3) newSession() and stop() are two methods which you need to start a new SparkSession
       spark1 = SparkSession.builder.getOrCreate() will get the same sparksession as existing one
   (4) set up run-time configurations 

         spark = SparkSession.builder \
                        .master('local[*]') \
                        .appName('test') \
                        .config("spark.sql.shuffle.partitions", 1000) \
                        .getOrCreate()
 


pyspark.sql.Catalog:
---
  + currentDatabase()
    setCurrentDatabase(dbname)
  + createTable(tbl_name, path=None, source=None, schema=None, **options)
    createExternalTable(tbl_name, path=None, source=None, schema=None, **options)
  + listDatabases()
    listColumns(tbl_name, dbname=None)
    listFunctions(dbname=None)
    listTable(dbname=None)
  + dropGlobalTempView(view_name)
    dropTempView(view_name)
  + cacheTable(tbl_name)
    isCached()
    uncacheTable(tbl_name)
    clearCache()
    + remove all cached tables from the in-memory cache. 
  + refreshByPath(path)
    refreshTable(tbl_name)
  + recoverPartitions(tbl_name)
  + registerFunction: alias to spark.udf.register()

  Notes: 
   (1) list metadata information about database, table, columns, cache etc.
   (2) Use spark.catalog.listColumns() return a list of collections.Columns, this command
       only work with HiveSupported tables, not TempView since they do not have database information



pyspark.sql.Row: 
---
  + asDict(recursive=False)

  Note:  
   (1) Row object is immutable and can NOT change key/value pairs
       use dict comprehension to create new Row object instead

        key_list= ['k1', 'k2', 'k3']
        row = Row(k1=1, k2=2)
        Row(**dict([ (k, row.asDict().get(k, None)) for k in key_list ]))
        Row(**dict({ k:row.asDict().get(k, None) for k in key_list }))

        add a new field into the existing row object:
        Row(k3=4, **row.asDict())
   (2) The columns is lexical-ordered using the above method, if you
       want to customize the Column order, use the following method:
        NRow = Row('k2', 'k1', 'k3')
        row = NRow(k2_value, k1_value, k3_value)   <-- must be ordered




pyspark.sql.Column:
---
  + alias(*alias):                   *** can be more than one columns
  + asc()
    asc_null_first()  spark v2.4+
    asc_null_last()
    desc()
    desc_null_first()
    desc_null_last()
  + between(lowerBound, upperBound): *** Both boundaries are inclusive and handle null values
  + bitwiseAND(other)
    bitwiseOR(other)
    bitwiseXOR(other)
  + cast(datatype), astype(datatype)
  + contains(other)
    startswith(other)
    endswith(other):                 *** All three are based on a string match
  + eqNullSafe(other)
  + isNotNull()
    isNull()
    isin()
  + like(sql_like): follows `SQL-like` expression
    rlike(regex): follows `Regex-like` expression
  + substr(startPos, length)
  + when(cond, value)
    otherwise(value)
    over(win_spec)
  + getField(name): get a field by name in a StructField
    getItem(key)

  Notes: 
   (1) isNull() and isNotNull() do NOT work on numeric columns, to check numeric Nan values
       use pyspark.sql.functions.isnan() function. 
   (2) col.astype('double'), will trim the leading/trailing whitespces before type casting
       however `1$2`, `$23.5` containing `[^-+.0-9]` will be casted to null
       notice that `[+-]` must be the first char if exists and `.`(dot) can have only one.
   (3) dot `.` in the column name must be enclosed with backticks when selecting column
       , no need for alias, for example:

         c = 'SDV.1'
         df.select(F.col('`{}`'.format(c)).alias(c))    -->  F.col(c)
         df.withColumn(c, df['`{}`'.format(c)])         -->  df[c]

         This fixed the following ERROR:
             pyspark.sql.utils.AnalysisException: "Can't extract value from SDV#27: need struct type but got string;"



spark.sql.DataFrame:
---
  + properties:
    + dtypes
    + columns
    + schema
    + rdd
    + stat
    + na
    + storageLevel 
    + write
    + writeStream
    + isStreaming
  + explain(extended=False)
    printSchema()
    hint(name, *param)  <-- version 2.2+
    isLocal()
  + first():   return a Row object
    collect(): return a list of Row objects
    head(N):   return a list of Row objects
    take(N):   return a list of Row objects
    show(N=20, truncate=False, vertical=False)
  + colRegex(regex)
    Notes: regex is Java-based and must be enclosed by backticks and preferred using Raw string: r'`...`', 
           example (all columns except col_name == 'param'): 

              df.colRegex(r'`^(?!param$).+`')
              df.colRegex(r'`(?:param)?+.+`')       `?+`: match 0 or 1 times and never backtrack

           this returns a Column object not a list of columns: Column<b'unresolvedregex()'>
  + crossJoin(df2)
    join(df2, on=None, how='inner')
    exceptAll(df2)
    union(df2)
    unionAll(df2)
    unionByName(df2)
    intersect(df2)
    intersectAll(df2)
    subtract(df2)
  + select(*cols)
    selectExpr(*expr)
    withColumn()
    withColumnRenamed()
    alias(alias)
    filter(cond), where(cond)
    distinct()
    greoupBy(*col), groupby(*col)
    limit(N)
  + orderBy(*col, ascending=True)   <-- alias of `sort`
      df.orderBy(['col1', 'col2'], ascending=[False,True])
      df.orderBy('col1', 'col2', ascending=[0,1])
    sort(*col) <-- same as above 
    sortWithinPartitions
  + foreach(f)
    foreachPartition(f)
  + drop(*cols)
    dropDuplicates(subset=None)  same to drop_duplicates
     ** Notes: dropduplicates will trigger a shuffling (default partitions to 200)
               spark.sql.shuffle.partitions = 200 by default
    dropna(how='any', thresh=None, subset=None)
  + fillna(value, subset=None)
  + replace(to_replace, value=<>, subset=None): 
    * Note: value and to_replace must have the same type, only available to numerics, booleans or strings
            to_replace and value can be list, to_replace can be a dict when value is missing.
  + describe(): 
      compute basic stat for numeric and string columns
    summary(): 
      similar to the above, but can take specific status, i.e. df.summary('count')
    agg(*expr): 
      shorthand for df.groupby().agg()
    approxQuantile(col, probablities, relativeError)
    count()
    cov(col1, col2)
    corr(col1, col2, method=None)
    freqItems(cols, 0.01) 
    crosstab(col1, col2)
    cube(*col)
    rollup(*col)
  + coalesce(N)
    repartition(N, *cols)
    repartitonByRange(N, *cols)
  + cache()
    persist(storageLevel=StorageLEvel(True,True,False,False,1))
    unpersist()
  + checkpoint()
    localCheckpoint()
  + createGlobalTempView(name)
    createOrReplaceGlobalTempView(name)
    createOrReplaceTempView(name)
    createTempView(name)
      Notes: 
       (1) GlobalTempView's lifespan is till the application terminated, TempView is the current SparkSession
       (2) GlobalTempView must be accessed through `global_temp`, TempView does not need a namespace:

              df.createTempView('dummy1')
              spark.sql('select * from dummy1').show()
              spark.newSession().sql('select * from dummy1').show()   <-- NOT working
              df.createOrReplaceGlobalTempView('dummy')
              spark.sql('select * from global_temp.dummy').show()
              spark.newSession().sql('select * from global_temp.dummy').show()

  + randomSplit(weights, seed=None)
    sample(withreplacement=False, fraction=[0,1], seed=None)
    sampleBy(col, fractions, seed=None)    <-- withreplacement=False
  + toDF()
    toJSON(): return RDD of JSONs
    toLocalIterator()
     * this brings all Rows to driver as iterator by partitions (need RAM to hold max-sized partition)
       useful in debugging when collect() is often used.
    toPandas()

  Notes:
   (1) coalesce will not trigger shuffling if go from 100 partitions to 10 partitions.
       if a larger partition is requested, it will stay at the current # of partitions
       use repartition() if you want to increase # of partitions
   (2) for df.replace(), value must match the **WHOLE** string, not a regex match or substring match.
       the following can be used to explore number of `STR` in the StringType() columns of the dataframe:

           df.count() - df.fillna('').replace('STR', None).summary('count').toPandas().T.iloc[1:,0].astype(int)
       
       + df.replace(), df.fillna(), the replacement part can be a bool, int, long, float, string or None. complex
         data types (array, map, struct) are not supported.
       + df.fillna(0) changes only numeric columns, df.fillna('nn') only changes string columns
   (3) create an empty RDD and empty DataFrame

          rdd_empty = spark.sparkContext.emptyRDD()
          df_empty  = spark.createDataFrame(rdd_empty, schema='')

   (4) df.freqItems(['col1'], 0.5) return an array of the freqItems from 'col1', the items in the array
       are unsorted, dont expect the first item to be the most counted item
       the 2nd argument set percentage of rows shown as frequentItem, default is 1%
   (5) df.rdd covert the whole DF to RDD and will slow down the process, be carefulwhen you do this.
   (6) Check if an instance is DataFrame or RDD

          from pyspark.rdd import RDD
          from pyspark.sql import DataFrame

          isinstance(df, DataFrame)     True
          isinstance(df, RDD)           False

   (7) Interestingly, in df.sort(sort_cols).write.parquet(..), the sort() mathod can help divide the
       data into k evenly-sized partitions (k = spark.sql.shuffle.partitions, default 200). sorted
       data can also help reduce data size(when compression is used), and improve query performace
       (for parquet, metadata max/min will help speed up query). However, sort is also expensive
       than simplr repartition. 
       ref: https://stackoverflow.com/questions/50775870
   (8) persist/cache and broadcast: 
       + use persist/cache when a transformation is reused in multiple calculations
       + use broadcast in join when the dataframe is small and network traffic is not a bottleneck
   (9) checkpoint breaks the lineage while persist/cache keeps the lineage. It is especially useful in 
       iterative algorithms where the plan may grow exponentially. Notice that the checkpoint
       is lazy evaluated, thus there must be an action to actual break the lineage. 

       check also: sc.setCheckpointDir(), persist(StorageLevel.DISK_ONLY)
       REF: https://stackoverflow.com/questions/35127720
   (10) unpersist, releasing resources etc:

        1. delete the object and then gc.collect()

           import gc
           del mydf
           gc.collect()

         2. unpersist

           mydf.unpersist()    <-- mark for removal

         3. Java gc: REF: https://stackoverflow.com/questions/58759929

           sc._gateway.detach(my_model._java_object)
          
         4. use spark.catalog to unpersist all dataframe

           spark.catalog.clearCache()
           spark.catalog.uncacheTable('df_table')

       

pyspark.sql.GroupedData
---
  + agg(*exprs)
  + apply(pandas_udf)
  + avg(*col)
    count()
    max(*col)
    min(*col)
    mean(*col)
    sum(*col)
  + pivot(pivot_col, values=None)
 Notes:
  (1) agg() function can take a dict, weakness is one column can has only one aggregate function
      and you can not directly set up alias:

        df.groupby(grped_cols).agg({'col1': 'max', 'col2': 'min'})
        # column-name will be `col1(max)` and `col2(min)`



pyspark.sql.Window:
---
  + partitionBy(*cols)
  + orderBy(*cols)
  + rangeBetween(start, end)
  + rowsBetween(start, end)
    
  Notes: 
   (1) By default, when orderBy() is missing, an unbounded Window frame is used
	 otherwise, a growing Window frame (Window.unboundedPreceding, currentRow)
   (2) boundary start/end are both inclusive and are relative to the current row
   (3) functions used with WindowSpec:
       + Window functions:
         + lag(col, count=1, default=None)
           lead(col, count=1, default=None) 
         + ranking based on: cols in the orderBy()
           + individual row: 
             * row_number()
             * rank()
             * dense_rank()
           + individual row but in percent:
             * percent_rank(): relative rank of rows within a window partition
             * cume_dist(): cumulative distribution of a window partition (fraction of rows that is below the current row)
           + a group of rows:
             * ntile(n): split rows into buckets of size-N, based on the orderBy() column
           REF:https://docs.microsoft.com/en-us/sql/t-sql/functions/percent-rank-transact-sql?view=sql-server-2017
       + Aggregate functions: 
         * count(c), max(c), min(c), avg(c), mean(c), sum(c), sumDistinct(c), approx_count_distinct(c, rsd)
         * kurtosis(c), skewness(c)
         * first(c, ignorenulls=False)
           last(c, ignorenulls=False)
         * stddev(c), stddev_pop(c), stddev_samp(c), var_pop(c), var_samp(c), variance(c)
         * collect_list(c), collect_set(c)
   (4) By default, sorting is ascending, use the following to adjust to desc:
       + use API functions: 
         * F.desc(c)
         * F.desc_null_first(c)
       + use column method: desc()
         * F.col('col1').desc()
         * df.col1.desc_null_last()
   (5) For Spark < version 2.3, rangeBetween() only applied to numeric data types(Long, Double etc), newer version 
       support DateType and TimestampType.



pyspark.sql.DataFrameReader:
---
spark.read
  + option(key, value)
  + options(**options)
  + schema()
  + format()
    + csv():
      + path: string, list of paths or RDD of string saving CSV rows
      + schema
 *    + sep: default ,
      + encoding
 *    + quote: default '"', if empty string, it uses 'u0000' (null character)
 *    + escape: default \
      + comment
 *    + header: default false
 *    + inferSchema
      + enforceSchema:
      + samplingRatio: to inferSchema, default 1.0
 *    + multiLine: default false
 *    + ignoreLeadingWhiteSpace: default true   <-- strip() to both column names and field values
 *    + ignoreTrailingWhiteSpace: default true  <-- strip() to both column names and field values
 *    + nullValue: default EMPTY string
 *    + nanValue:
      + positiveInf
      + negativeInf
      + emptyValue: default ""
 *    + dateFormat: default yyyy-MM-dd
 *    + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
      + maxColumns: default 20480
      + maxCharsPerColumn: default unlimited
      + mode: PERMISSIVE, DROPMALFORMATED, FAILFAST
      + columnNameOfCorruptRecord: 
      + charToEscapeQuoteEscaping: 
    + json():
      + path
      + schema
 *    + multiLine: default false
 *    + lineSep: default `\r?\n`
      + primitivesAsString: default false
      + prefersDecimal: default false
      + allowComments: default false
 *    + allowUnquotedFieldNames: default false
 *    + allowSingleQuotes: default true
      + allowNumericLeadingZero: default false
      + allowBackslashEscapingAnyCharacter: default false
      + allowUnquotedControlChars:
 *    + dateFormat: default yyyy-MM-dd
 *    + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
      + mode: PERMISSIVE, DROPMALFORMATED, FAILFAST
      + columnNameOfCorruptRecord:
      + encoding:
      + samplingRatio: default 1.0
      + dropFieldIfAllNull: default false
    + jdbc():
      + url: example jdbc:mysql://127.0.1:3305/warehouse
      + table
      + properties: JDBC connection arguments: user, password etc
      + column: an integer column for partitioning
      + numPartitions, lowerBound, upperBound: used for partitions
      + predicates: for partitions
    + orc():
    + parquet():
      + mergeSchema: default is set spark.sql.parquet.mergeSchema
    + text(): 
      note: default is line mode.
      + wholetext: if true, read each file as a single Row
      + lineSep: default '\r?\n'
  + format with plugins:
    + avro: since spark 2.4+, SparkSQL provides builtin support for reading/writing Apache Avro data
      + pyspark --packages org.apache.spark:spark-avro_2.11:2.4.0
    + xml: 
      + spark-xml: https://github.com/databricks/spark-xml#python-api
    + bigquery: (untested)
      + https://github.com/GoogleCloudPlatform/spark-bigquery-connector
    + mongodb: (untested)
      + https://docs.mongodb.com/spark-connector/master/
  + load():
    * parameters: path, format, schema, options
  + table(tbl_name)
    * return `tbl_name` as a dataframe

  Notes:
  (1) JSON file can use single quotes(by default), no quotes(allowUnquotedFieldNames)
  (2) csv file sep must be single-char delimiter, for multi-char or regex, might have to go through RDD methods
      if there are multiLine for the header, you will need to customize schema (header=False, inferSchema=False 
      are set by default)
  (3) mode=FAILFAST might not work as expected
  (4) the option `mergeSchema` is currently only applied to parquet format, for orc, avro, you can 
      explicitly specify the schema, spark will merge the result accororingly.
  (5) hadoop support glob the same as in bash glob fo FileInputFormat:
    https://books.google.com.vn/books?id=Wu_xeGdU4G8C&lpg=PA238&ots=i8wUUFRcZw&pg=PA65#v=onepage&q&f=false
     + *: 0 or more chars
     + ?: match single char
     + [ab]: char set
     + [^ab]: nagated char set
     + [a-b]: char range
     + {a,b}: alternation
     + \c: escape char
  (6) most path for DataFrameReader take list of strings for input path(s) or RDD; 
      for RDD (textFile, wholeTextFile) takes only one String as path (not list)
      but you can use shell glob to add more than one files, example:

          rdd = sc.wholeTextFiles('/home/xicheng/test/{time-1.txt,merge-1.txt}')
          rdd = sc.wholeTextFiles(','.join(list_of_files))

      Notice that list of files must be joined by comma ',', semi-comma does NOT work.



pyspark.sql.DataFrameWriter:
---
spark.write
  + mode():
    * append, overwrite, error/errorifexists, ignore
  + option(key, value)
  + options(**options)
  + partitionBy(*col)
  + format():
    + csv()
      + path
 *    + mode: append, overwrite, ignore, error(default)
      + compression: none, bzip2, gzip, lz4, snappy and deflate
 *    + sep: default ,
 *    + quote: default '"', if empty string, it uses 'u0000' (null character)
 *    + escape: default \
      + escapeQuotes: default true 
      + quoteAll:
 *    + header: default false
 *    + nullValue: default EMPTY string
 *    + dateFormat: default yyyy-MM-dd
 *    + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
 *    + ignoreLeadingWhiteSpace: default true   <-- strip() to both column names and field values
 *    + ignoreTrailingWhiteSpace: default true  <-- strip() to both column names and field values
      + charToEscapeQuoteEscaping: 
      + encoding:
      + emptyValue: default ""
    + jdbc()
      + url: example jdbc:mysql://127.0.1:3305/warehouse
      + table
      + mode
      + properties: JDBC connection arguments: user, password etc
    + json()
      + path
      + mode
      + compression
      + dateFormat
      + timestampFormat
      + encoding
      + lineSep
    + orc(path, mode, partitonBy, compression)
    + parquet(path, mode, partitonBy, compression)
    + text(path, compression, lineSep) 
  + format with plugins: 
    + avro: 
    + xml: 
  + save():
    * parameters: path, format, mode, partitionBy and options
  + insertInto(tableName, overwrite=False)
  + bucketBy(numBuckets, col, *cols)
    * Note: applicable for file-based data sources using saveAsTable()
  + sortBy(*col)
    * sort the output in each bucket by the given cols
  + saveAsTable(tableName, format, mode, partitonBy, options)

  Notes:
  (1) `csv` format can not handle ArrayType(), StructType(), MapType() etc compound data types.
      to save such data types, either convert them to JSON, or use another format, 
      for example, parquet, orc, avro etc.
      Note: by default, csv file writer does not export header(column names)
  (2) `text` format must have the DF in one single StringType() column, can use concat_ws:

          df.select(concat_ws('\t', *df.columns)).write.mode('overwrite').text('/path/file')

       However, text file will not save headers(column names)

  (3) do not want to escape the quotes iside the fields, then set `escapeQuotes` = `false`
      example: https://stackoverflow.com/questions/58348617
  (4) To use saveAsTable(), you must add enableHiveSupport(), see below. you dont have to 
      run ApacheHive services, you can use local metastore_db and customize 
      the spark.sql.warehouse.dir

        spark = SparkSession \
            .builder \
            .appName('test') \
            .config('spark.sql.warehouse.dir', '/path/to/my/warehouse') \
            .enableHiveSupport() \
            .getOrCreate()
        df1.write.format('parquet').mode('overwrite').saveAsTable('tbl_df1')



pyspark.sql.UDFRegistration
---
  + spark.udf.register('func_name', <func_def>, <return_type>)
    * Python-based user-define function
  + spark.udf.registerJavaFunction('func_name', javaClassName, <return_type>)   <-- from spark v2.3
    * Java/Scala-based user-defined function as SQL function
  + spark.udf.registerJavaUDAF(name, javaClassName)      <-- from sparn v2.3
    * java-based aggregation function

  Notes: 
  (1) accessed by spark.udf or sqlContext.udf
  (2) functions only available in SQL context, i.e. selectExpr() 
  (3) Another old way to register Java/Scala-based udf function:

        sqlContext = SQLContext(spark.sparkContext)
        spark._jvm.com.jxc.spark.Functions.registerFunc(sqlContext._jsqlContext, "set_group_label_2")

      All mentioned above are applicable only to the Spark SQL context.
  (4) The following two only applied to DataFrame API:
      + pyspark.sql.functions.udf(<func_def>, <return_type>)
      + pyspark.sql.functions.pandas_udf()
  (5) All udf functions involve data serialization, deserialization and data movement between JVM 
      and Python interpreter. [ref: https://stackoverflow.com/questions/38296609]



pyspark.sql.types:
---
  + NullType()                          <-- Not a Numeric type
  + StringType()                        <-- Not a Numeric type
  + BinaryType()                        <-- Not a Numeric type
  + BooleanType()                       <-- Not a Numeric type
  + DateType()                          <-- Not a Numeric type 
  + TimeStampType()                     <-- Not a Numeric type
  + DecimalType(precision=10, scale=0)
    DoubleType()
    FloatType()
    ByteType()
    IntegerType()
    LongType()
    ShortType()
  + ArrayType(elementType, containsNull=True)
  + MapType(keyType, valueType, valueContainsNull=True)
  + StructField(name,dataType,nullable=True,metadata=None)
  + StructType(fields=None)
    + add(field, data_type=None, nullable=True, metadata=None)
      new_schema = schema.add('new_field', LongType())
    + fieldNames(): returns all field names in a list

  Notes: 
  (1) common methods to all DataType()
      + json() <-- JSON string
      + jsonValue()  <-- JSON value: Python object
      + simpleString()  
      + fromInternal() convert between internal SQL object to native Python object.
        toInternal()
        needConversion()  <-- check if From and To Internal have any differences

  (2) common to MapType(), ArrayType(), StructField() and StructType()
      + fromJson()    <-- classmethod

  (3) Mapping Python types from/to Spark SQL DataType
      Reference: https://spark.apache.org/docs/2.4.4/api/python/_modules/pyspark/sql/types.html

      _type_mappings = {
          type(None): NullType,
          bool: BooleanType,
          int: LongType,
          float: DoubleType,
          str: StringType,
          bytearray: BinaryType,
          decimal.Decimal: DecimalType,
          datetime.date: DateType,
          datetime.datetime: TimestampType,
          datetime.time: TimestampType,
      }

      _acceptable_types = {
          BooleanType: (bool,),
          ByteType: (int, long),
          ShortType: (int, long),
          IntegerType: (int, long),
          LongType: (int, long),
          FloatType: (float,),
          DoubleType: (float,),
          DecimalType: (decimal.Decimal,),
          StringType: (str, unicode),
          BinaryType: (bytearray,),
          DateType: (datetime.date, datetime.datetime),
          TimestampType: (datetime.datetime,),
          ArrayType: (list, tuple, array),
          MapType: (dict,),
          StructType: (tuple, list, dict),
      }

      int_size_to_type: {
          <=8 : (ByteType, tinyint),
          <=16: (ShortType, smallint)
          <=32: (IntegerType, int)
          <=64: (LongType, long)
      }

   (4) BinaryType() is byteArray, you can access it through substr(barr,1,1), not `barr[0]`
       , the result of `substr(barr,1,1)` is still a BinaryType(), not ByteType()

        spark.sql("select binary('test')").show()   --> [74 65 73 74]


   (5) DataTypes for columns can be retrieved from df.schema.jsonValues() into a dictionary:

       Example-1: mapping between column name and corresponding data-type

           field_dtypes = { f['name']:f['type'] for f in df.schema.jsonValue()['fields'] }

       Example-2: mapping between StructType() columns with corresponding fields(in a list)
       https://stackoverflow.com/questions/58219205

           struct_fields_mapping = { 
                f['name']:[i['name'] for i in f['type']['fields']] 
                    for f in df.schema.jsonValue()['fields'] 
                    if type(f['type']) is dict and f['type']['type'] == 'struct' 
           }



