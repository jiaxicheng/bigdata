pyspark.sql notes:

group all related method/functions under the same class to have a full understanding about the 
current pyspark.sql library:


pyspark.sql.SparkSession:
---
  + properties:
    + conf
    + catalog
    + SparkContext
    + streams
    + read
    + readStream
    + udf
    + version
  + builder
    + appName()
    + config(key=None, value=Nonee, conf=None)
    + master()
    + getOrCreate()
    + enableHiveSupport()
  + createDataFrame(data, schema=None, samplingRatio=Nove, verifySchema=True)
  + range(start, end=None, step=1, numPArtitions=None)
    * if only one argument is specified, it will be used as the `end` value
  + sql(sqlQuery)
  + table(tableName): returns the table name as a dataframe
  + newSession()
    stop()

  Notes:
   (1) below are very useful in testing codes:
     + testing DF-API functions: spark.range(1).select('...').show()
     + testing Spark SQL       : spark.sql('select ...').show()
   (2) data in spark.createDataFrame() can be:
     + an RDD of any kind of SQL data representation
     + list
     + pandas.DataFrame
    


pyspark.sql.Catalog:
---
  + currentDatabase()
    setCurrentDatabase(dbname)
  + createTable(tbl_name, path=None, source=None, schema=None, **options)
    createExternalTable(tbl_name, path=None, source=None, schema=None, **options)
  + listDatabases()
    listColumns(tbl_name, dbname=None)
    listFunctions(dbname=None)
    listTable(dbname=None)
  + dropGlobalTempView(view_name)
    dropTempView(view_name)
  + cacheTable(tbl_name)
    isCached()
    uncacheTable(tbl_name)
    clearCache()
  + refreshByPath(path)
    refreshTable(tbl_name)
  + recoverPartitions(tbl_name)
  + registerFunction: alias to spark.udf.register()

  Notes: list metadata information about database, table, columns, cache etc.



pyspark.sql.Row: 
---
  + asDict(recursive=False)

    Note:  Row object is immutable and can NOT change key/value pairs
           use dict comprehension to create new Row object instead

        key_list= ['k1', 'k2', 'k3']
        row = Row(k1=1, k2=2)
        Row(**dict([ (k, row.asDict().get(k, None)) for k in key_list ]))
        Row(**dict({ k:row.asDict().get(k, None) for k in key_list }))



pyspark.sql.Column:
---
  + alias(*alias):                   *** can be more than one columns
  + asc()
    asc_null_first()  spark v2.4+
    asc_null_last()
    desc()
    desc_null_first()
    desc_null_last()
  + between(lowerBound, upperBound): *** Both boundaries are inclusive and handle null values
  + bitwiseAND(other)
    bitwiseOR(other)
    bitwiseXOR(other)
  + cast(datatype), astype(datatype)
  + contains(other)
    startswith(other)
    endswith(other):                 *** All three are based on a string match
  + eqNullSafe(other)
  + isNotNull()
    isNull()
    isin()
  + like(sql_like): follows `SQL-like` expression
    rlike(regex): follows `Regex-like` expression
  + substr(startPos, length)
  + when(cond, value)
    otherwise(value)
    over(win_spec)
  + getField(name): get a field by name in a StructField
    getItem(key)

  Notes: 
   (1) isNull() and isNotNull() do NOT work on numeric columns, to check numeric Nan values
       use pyspark.sql.functions.isnan() function. 



spark.sql.DataFrame:
---
  + properties:
    + dtypes
    + columns
    + schema
    + rdd
    + stat
    + na
    + storageLevel 
    + write
    + writeStream
    + isStreaming
  + explain(extended=False)
    printSchema()
    hint(name, *param)  <-- version 2.2+
    isLocal()
  + first():   return a Row object
    collect(): return a list of Row objects
    head(N):   return a list of Row objects
    take(N):   return a list of Row objects
    show(N=20, truncate=False, vertical=False)
  + colRegex(regex)
  + crossJoin(df2)
    join(df2, on=None, how='inner')
    exceptAll(df2)
    union(df2)
    unionAll(df2)
    unionByName(df2)
    intersect(df2)
    intersectAll(df2)
    subtract(df2)
  + select(*cols)
    selectExpr(*expr)
    withColumn()
    withColumnRenamed()
    alias(alias)
    filter(cond), where(cond)
    distinct()
    greoupBy(*col), groupby(*col)
    limit(N)
  + orderBy(*col)
    sort(*col)
    sortWithinPartitions
  + foreach(f)
    foreachPartition(f)
  + drop(*cols)
    dropDuplicates(subset=None)  same to drop_duplicates
    dropna(how='any', thresh=None, subset=None)
  + fillna(value, subset=None)
  + replace(to_replace, value=<>, subset=None): 
    * Note: value and to_replace must have the same type, only available to numerics, booleans or strings
            to_replace and value can be list, to_replace can be a dict when value is missing.
  + describe(): compute basic stat for numeric and string columns
    summary()
    agg(*expr)
    approxQuantile(col, probablities, relativeError)
    count()
    cov(col1, col2)
    corr(col1, col2, method=None)
    freqItems(cols, support=None)
    crosstab(col1, col2)
    cube(*col)
    rollup(*col)
  + coalesce(N)
    repartition(N, *cols)
    repartitonByRange(N, *cols)
  + cache()
    persist(storageLevel=StorageLEvel(True,True,False,False,1))
    unpersist()
  + checkpoint()
    localCheckpoint()
  + createGlobalTempView(name)
    createOrReplaceGlobalTempView(name)
    createOrReplaceTempView(name)
    createTempView(name)
  + randomSplit(weights, seed=None)
    sample(withreplacement=False, fraction=[0,1], seed=None)
    sampleBy(col, fractions, seed=None)    <-- withreplacement=False
  + toDF()
    toJSON(): return RDD of JSONs
    toLocalIterator()
    toPandas()

  Notes:
   (1) coalesce will not trigger shuffling if go from 100 partitions to 10 partitions.
       if a larger partition is requested, it will stay at the current # of partitions
       use repartition() if you want to increase # of partitions
   (2) for df.replace(), value must match the **WHOLE** string, not a regex match or substring match.
       the following can be used to explore number of `STR` in the StringType() columns of the dataframe:

           df.count() - df.fillna('').replace('STR', None).summary('count').toPandas().T.iloc[1:,0].astype(int)
       
   (3) create an empty RDD and empty DataFrame

          rdd_empty = spark.sparkContext.emptyRDD()
          df_empty  = spark.createDataFrame(rdd_empty, schema='')



pyspark.sql.GroupedData
---
  + agg(*exprs)
  + apply(pandas_udf)
  + avg(*col)
    count()
    max(*col)
    min(*col)
    mean(*col)
    sum(*col)
  + pivot(pivot_col, values=None)



pyspark.sql.Window:
---
  + partitionBy(*cols)
  + orderBy(*cols)
  + rangeBetween(start, end)
  + rowsBetween(start, end)
    
  Notes: 
   (1) By default, when orderBy() is missing, an unbounded Window frame is used
	 otherwise, a growing Window frame (Window.unboundedPreceding, currentRow)
   (2) boundary start/end are both inclusive and are relative to the current row
   (3) functions used with WindowSpec:
       + Window functions:
         + lag(col, count=1, default=None)
           lead(col, count=1, default=None) 
         + ranking based on 
           + individual row: 
             * row_number()
             * rank()
             * dense_rank()
           + individual row but in percent:
             * percent_rank(): relative rank of rows within a window partition
             * cume_dist(): cumulative distribution of a window partition (fraction of rows that is below the current row)
           + a group of rows:
             * ntile(n): split rows into buckets of size-N, based on the orderBy() column
           REF:https://docs.microsoft.com/en-us/sql/t-sql/functions/percent-rank-transact-sql?view=sql-server-2017
       + Aggregate functions: 
         * count(c), max(c), min(c), avg(c), mean(c), sum(c), sumDistinct(c), approxCountDistinct(c, rsd)
         * kurtosis(c), skewness(c)
         * first(c, ignorenulls=False)
           last(c, ignorenulls=False)
         * stddev(c), stddev_pop(c), stddev_samp(c), var_pop(c), var_samp(c), variance(c)
         * collect_list(c), collect_set(c)
   (4) By default, sorting is ascending, use the following to adjust to desc:
       + use API functions: 
         * F.desc(c)
         * F.desc_null_first(c)
       + use column method: desc()
         * F.col('col1').desc()
         * df.col1.desc_null_last()
   (5) For Spark < version 2.3, rangeBetween() only applied to numeric data types(Long, Double etc), newer version 
       support DateType and TimestampType.



pyspark.sql.DataFrameReader:
---
spark.read
  + option(key, value)
  + options(**options)
  + schema()
  + format()
    + csv():
      + path: string, list of paths or RDD of string saving CSV rows
      + schema
 *    + sep: default ,
      + encoding
 *    + quote: default '"', if empty string, it uses 'u0000' (null character)
 *    + escape: default \
      + comment
 *    + header: default false
 *    + inferSchema
      + enforceSchema:
      + samplingRatio: to inferSchema, default 1.0
 *    + multiLine: default false
 *    + ignoreLeadingWhiteSpace: default true   <-- strip() to both column names and field values
 *    + ignoreTrailingWhiteSpace: default true  <-- strip() to both column names and field values
 *    + nullValue: default EMPTY string
 *    + nanValue:
      + positiveInf
      + negativeInf
      + emptyValue: default ""
 *    + dateFormat: default yyyy-MM-dd
 *    + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
      + maxColumns: default 20480
      + maxCharsPerColumn: default unlimited
      + mode: PERMISSIVE, DROPMALFORMATED, FAILFAST
      + columnNameOfCorruptRecord: 
      + charToEscapeQuoteEscaping: 
    + json():
      + path
      + schema
 *    + multiLine: default false
 *    + lineSep: default `\r?\n`
      + primitivesAsString: default false
      + prefersDecimal: default false
      + allowComments: default false
 *    + allowUnquotedFieldNames: default false
 *    + allowSingleQuotes: default true
      + allowNumericLeadingZero: default false
      + allowBackslashEscapingAnyCharacter: default false
      + allowUnquotedControlChars:
 *    + dateFormat: default yyyy-MM-dd
 *    + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
      + mode: PERMISSIVE, DROPMALFORMATED, FAILFAST
      + columnNameOfCorruptRecord:
      + encoding:
      + samplingRatio: default 1.0
      + dropFieldIfAllNull: default false
    + jdbc():
      + url: example jdbc:mysql://127.0.1:3305/warehouse
      + table
      + properties: JDBC connection arguments: user, password etc
      + column: an integer column for partitioning
      + numPartitions, lowerBound, upperBound: used for partitions
      + predicates: for partitions
    + orc():
    + parquet():
      + mergeSchema: default is set spark.sql.parquet.mergeSchema
    + text(): 
      note: default is line mode.
      + wholetext: if true, read each file as a single Row
      + lineSep: default '\r?\n'
  + format with plugins:
    + avro: since spark 2.4+, SparkSQL provides builtin support for reading/writing Apache Avro data
      + pyspark --packages org.apache.spark:spark-avro_2.11:2.4.0
    + xml: 
      + spark-xml: https://github.com/databricks/spark-xml#python-api
    + bigquery: (untested)
      + https://github.com/GoogleCloudPlatform/spark-bigquery-connector
  + load():
    * parameters: path, format, schema, options
  + table(tbl_name)
    * return `tbl_name` as a dataframe

  Notes:
  (1) JSON file can use single quotes(by default), no quotes(allowUnquotedFieldNames)
  (2) csv file sep must be single-char delimiter, for multi-char or regex, might have to go through RDD methods
  (3) mode=FAILFAST might not work as expected
  (4) the option `mergeSchema` is currently only applied to parquet format, for orc, avro, you can 
      explicitly specify the schema, spark will merge the result accororingly.



pyspark.sql.DataFrameWriter:
---
spark.write
  + mode():
    * append, overwrite, error/errorifexists, ignore
  + option(key, value)
  + options(**options)
  + partitionBy(*col)
  + format():
    + csv()
      + path
 *    + mode: append, overwrite, ignore, error(default)
      + compression: none, bzip2, gzip, lz4, snappy and deflate
 *    + sep: default ,
 *    + quote: default '"', if empty string, it uses 'u0000' (null character)
 *    + escape: default \
      + escapeQuotes:
      + quoteAll:
 *    + header: default false
 *    + nullValue: default EMPTY string
 *    + dateFormat: default yyyy-MM-dd
 *    + timestampFormat: default yyyy-MM-dd'T'HH:mm:ss.SSSXXX
 *    + ignoreLeadingWhiteSpace: default true   <-- strip() to both column names and field values
 *    + ignoreTrailingWhiteSpace: default true  <-- strip() to both column names and field values
      + charToEscapeQuoteEscaping: 
      + encoding:
      + emptyValue: default ""
    + jdbc()
      + url: example jdbc:mysql://127.0.1:3305/warehouse
      + table
      + mode
      + properties: JDBC connection arguments: user, password etc
    + json()
      + path
      + mode
      + compression
      + dateFormat
      + timestampFormat
      + encoding
      + lineSep
    + orc(path, mode, partitonBy, compression)
    + parquet(path, mode, partitonBy, compression)
    + text(path, compression, lineSep): 
      *** the DF must have only one StringType() column
  + format with plugins: 
    + avro: 
    + xml: 
  + save():
    * parameters: path, format, mode, partitionBy and options
  + insertInto(tableName, overwrite=False)
  + bucketBy(numBuckets, col, *cols)
    * Note: applicable for file-based data sources using saveAsTable()
  + sortBy(*col)
    * sort the output in each bucket by the given cols
  + saveAsTable(tableName, format, mode, partitonBy, options)

  Notes:
  (1) `csv` format can not handle ArrayType(), StructType(), MapType() etc compound data types.
      to save such data types, either convert them to JSON, or use another format, 
      for example, parquet, orc, avro etc.



pyspark.sql.UDFRegistration
---
  + spark.udf.register('func_name', <func_def>, <return_type>)
    * Python-based user-define function
  + spark.udf.registerJavaFunction('func_name', javaClassName, <return_type>)   <-- from spark v2.3
    * Java/Scala-based user-defined function as SQL function
  + spark.udf.registerJavaUDAF(name, javaClassName)      <-- from sparn v2.3
    * java-based aggregation function

  Notes: 
  (1) accessed by spark.udf or sqlContext.udf
  (2) functions only available in SQL context, i.e. selectExpr() 
  (3) Another old way to register Java/Scala-based udf function:

        sqlContext = SQLContext(spark.sparkContext)
        spark._jvm.com.jxc.spark.Functions.registerFunc(sqlContext._jsqlContext, "set_group_label_2")

      All mentioned above are applicable only to the Spark SQL context.
  (4) The following two only applied to DataFrame API:
      + pyspark.sql.functions.udf(<func_def>, <return_type>)
      + pyspark.sql.functions.pandas_udf()



pyspark.sql.types:
---
  + NullType()                          <-- Not a Numeric type
  + StringType()                        <-- Not a Numeric type
  + BinaryType()                        <-- Not a Numeric type
  + BooleanType()                       <-- Not a Numeric type
  + DateType()                          <-- Not a Numeric type 
  + TimeStampType()                     <-- Not a Numeric type
  + DecimalType(precision=10, scale=0)
    DoubleType()
    FloatType()
    ByteType()
    IntegerType()
    LongType()
    ShortType()
  + ArrayType(elementType, containsNull=True)
  + MapType(keyType, valueType, valueContainsNull=True)
  + StructField(name,dataType,nullable=True,metadata=None)
  + StructType(fields=None)
    + add(field, data_type=None, nullable=True, metadata=None)
      new_schema = schema.add('new_field', LongType())
    + fieldNames(): returns all field names in a list

  Notes: 
  (1) common methods to all DataType()
      + json() <-- JSON string
      + jsonValue()  <-- JSON value: Python object
      + simpleString()  
      + fromInternal() convert between internal SQL object to native Python object.
        toInternal()
        needConversion()  <-- check if From and To Internal have any differences

  (2) common to MapType(), ArrayType(), StructField() and StructType()
      + fromJson()    <-- classmethod

  (3) Mapping Python types from/to Spark SQL DataType
      Reference: https://spark.apache.org/docs/2.4.4/api/python/_modules/pyspark/sql/types.html

      _type_mappings = {
          type(None): NullType,
          bool: BooleanType,
          int: LongType,
          float: DoubleType,
          str: StringType,
          bytearray: BinaryType,
          decimal.Decimal: DecimalType,
          datetime.date: DateType,
          datetime.datetime: TimestampType,
          datetime.time: TimestampType,
      }

      _acceptable_types = {
          BooleanType: (bool,),
          ByteType: (int, long),
          ShortType: (int, long),
          IntegerType: (int, long),
          LongType: (int, long),
          FloatType: (float,),
          DoubleType: (float,),
          DecimalType: (decimal.Decimal,),
          StringType: (str, unicode),
          BinaryType: (bytearray,),
          DateType: (datetime.date, datetime.datetime),
          TimestampType: (datetime.datetime,),
          ArrayType: (list, tuple, array),
          MapType: (dict,),
          StructType: (tuple, list, dict),
      }

      int_size_to_type: {
          <=8 : (ByteType, tinyint),
          <=16: (ShortType, smallint)
          <=32: (IntegerType, int)
          <=64: (LongType, long)
      }

   (4) BinaryType() is byteArray, you can access it through substr(barr,1,1), not `barr[0]`
       , the result of `substr(barr,1,1)` is still a BinaryType(), not ByteType()

        spark.sql("select binary('test')").show()   --> [74 65 73 74]


   (5) DataTypes for columns can be retrieved from df.schema.jsonValues() into a dictionary:

       Example-1: mapping between column name and corresponding data-type

           field_dtypes = { f['name']:f['type'] for f in df.schema.jsonValue()['fields'] }

       Example-2: mapping between StructType() columns with corresponding fields(in a list)
       https://stackoverflow.com/questions/58219205

           struct_fields_mapping = { 
                f['name']:[i['name'] for i in f['type']['fields']] 
                    for f in df.schema.jsonValue()['fields'] 
                    if type(f['type']) is dict and f['type']['type'] == 'struct' 
           }



