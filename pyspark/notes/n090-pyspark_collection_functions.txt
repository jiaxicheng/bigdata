Collection functions: 

Note: 
  (1) the index for array-related functions are all 1-based,
      col.getItem(N) or col[N] is 0-based
  (2) except list version for example from v1.5, the functions are from v2.4
  (3) functions having drop-duplicates as side-effects: array_except, array_intersect and array_union

REF: https://issues.apache.org/jira/browse/SPARK-23899

Initialization:
---
  + create_map(*cols)L from v2.0
    + SQL: map(k0, v0, k1, v1, ...)
    + map_concat(m1, m2, ..): concat multiple maps
  + array(*cols): from v1.4
    + SQL: array(expr,...)
    + concat(a1, a2,...): concat multiple arrays
  + struct(*cols): from v1.4
    + SQL: struct(col1, col2,...)
           named_struct(name1, val1, name2, val2,...)
           (f1, f2)    <-- number of fields must be more than 1

  Some Examples: 
    + named_struct:  (c1,c2)  <-- default field names are col1, col2 etc

        named_struct('f1', col1)
        named_struct('f1', col1, 'f2', col2)
        ('f1', col2)   --> struct<col1:string,col2:double>
        (col2)         --> same as col2 returning `double`
        (col1 as f1)   --> not working with spark 3.0+
        (col1 as f1, col2 as f2)  <-- fine with 1+ fields `struct<f1:double,f2:double>`
        (f1*f2, f2)    <-- named_struct with defult names struct<col1:string,f2:string>
      
      Notes:
       (1) when using `(...)` to create struct, fields from calculated columns will by default have named `colN`
          otherwise it will be the same as colName as colName, that's why when using struct fields, it can not stand
          on its own due to the dot in name, see below:
           
            (f1*f2, f2)              -->   (f1*f2 as col1, f2 as f2)
            (f1.s1, f2.s2)           -->   failed, use (f1.s1 as col1, f2.s2 as col2) instead
            (col1*col2 as c1, col2)  -->  find with mixed 
        

    + struct:

       struct('f1', 'f2')
       struct(*cols)
       struct('f1', col('f2').alias('f2_name'))

      Add a new field into an StructType column:
        df = spark.createDataFrame([({"c1":"F6","c2":"mo"},)], schema="tt struct<c1:string,c2:string>")
        df.selectExpr("struct(tt.*,'123' as tmp) as tt")
       Note: '*' can ONLY be used after a column, not a field inside a StructType column. 
          thus for array of structs, you can not use transform function iterate through and use `x.*`

    + array:

      to create an ArrayType Column: split, array_repeat, from_json 

    + map:

      To create a Map, use map_from_entries, map_from_arrays, str_to_map, create_map, from_json


ArrayType: The index for all array functions are 1-based, Column.getItem() method are 0-based 
---
  + basic operations:
    + element_at(col, N)
    + size(col): number of items
    + slice(arr, start, length): 
    + array_position(col, value):
      return the index of the first occurance of 'value' in array (1-based)
  + relationships:
    + array_union + concat
      + array_union(arr_1, arr_2): 
        union two arrays and remove duplicates                              <-- note: drop duplicates is a side-effect
      + concat(arr_1, arr_2, ...): 
        union 2 or more arrays, can contain duplicates
      Note: all array elements must have the same DataType()
            for StructType(), all field names and types must match
    + array_except, array_remove:
      + array_except(arr_1, arr_2): 
        remove a list of values from the array (drop duplicated elements)   <-- note: drop duplicates is a side-effect
      + array_remove(arr, v): from v2.4
        remove 1 value from the array
    + array_intersect(arr_1, arr_2): 
      return elements both in arr_1 and arr_2 withoud duplicates            <-- note: drop duplicates is a side-effect
  + manipulations:
    + array_sort and sort_array:
      + array_sort(arr): 
        elements of the array must be orderable, Null will be put to the end of the array
      + sort_array(arr, asc=True): from v1.5
        Null first for asc=True, otherwise Null to the last
      + reserse(col): reverse the order of array items
      Note: array elements can be string, all numeric dtypes, struct, array (MapType has no order)
    + array_distinct(arr): 
      Note: array elements can be string, all numeric dtypes, struct, array
    + array_repeat(col, N): 
      create a new array with the same element of N times
    + array_join(col, delimiter, null_replacement=None): 
      convert array into a string:
    + array_max
      array_min:
    + arrays_zip(*cols): 
      + functionality for two cols can be replaced by zip_with for more flexibilities
        `zip_with` is similar to zip_longest, NULL will be filled for the element from short list
    + flatten(arr):
      * flatten only 1-level(no recursive), array of arrays into an array
    + shuffle(col): 
      * generate a random permutation of the given array: on-deterministic
  + conditions:
    + array_contains(arr, value): from v1.5
      * return true/false if value is in/not in the array, return null if array is EMPTY
    + arrays_overlap(arr_1, arr_2):
      * return boolean
    + exists(arr, x -> cond(x))
      * higher-order function
  + sequence(start, stop, step=None): 
    Notes:
      + Dataframe API must be integers, Spark SQL can generate Date Sequences
      + monthly sequence is based on 30-day which might not be useful
      + supported types with SparkSQL: byte, short, integer, long, date, timestamp
        only integers for DF-API
      + step can be negative, default 1 or -1 based on start/stop 
  + SparkSQL higher-order functions:
    + filter: from v2.4

        filter(arr, x -> func(x))           
        filter(arr, (x,i) -> func(x,i))         <-- i added Spark 3.0+
        
    + transform: from v2.4

        transform(arr, x -> func(x))      
        transform(arr, (x,i) -> func(x,i))  

     Notes:
      (1) the array created by transform function by default has `containsNull = False`, this will raise
          datatype mis-match error when caocatenating with other arrays with the same schema

    + zip_with: only works with two ArrayType columns

        transform(arr1, arr2, (x,y) -> func(x,y))  

    + aggregate: from v2.4

        aggregate(
          arr,                      /* expr: an array to iterate */
          start,                    /* start: zero value */ 
          (acc, y) -> func(acc, y), /* merge: logic to do reduce */ 
          acc -> func(acc)          /* finish: post-processing */
        )

    + exists: from v2.4

        exists(arr, x -> func(x))

   Notes:
    (1) above x,y are item values, i is item index, arr[,1,2] are ArrayType columns


MapType:
---
  + map_concat(map1, map2, ...): from v2.4
    * concatenate maps, might contain duplicate keys.
  + map_from_arrays(keys_arr, values_arr): from v2.4
  + map_from_entries(arr_of_structs):   from v2.4
  + map_keys(col): from v2.3
    * notes return unsorted array containing keys of the map, can be used in transform()
  + map_values(col): from v2.3
    * return unsorted array containing values of the map
  + map_entries(col): from v3.0
    * return an unsorted array of all entries in the given map
  + str_to_map(text[,pairDelim[,keyValueDelim])
    * both pairDelim and keyValueDelim are treated as regexp
    * by default, pairDelim=',' and keyValueDelim=':'
  + SparkSQL higher-order functions: from v3.0
    + transform_keys: 

        transform_keys(map1, (k,v) -> func(k,v))   <-- adjust Keys only

    + transform_values:

        transform_values(map1, (k,v) -> func(k,v))  <-- adjust values only

    + map_filter:

        map_filter(map1, (k,v) -> func(k,v))

    + map_zip_with:

        map_zip_with(map1, map2, (k,v1,v2) -> func(k,v1,v2))

    + forall: from 3.0   test if a predicate holds for all elements in the array
      
        forall(arr, x -> func(x))



ArrayType + MapType:
---
  + size(col): from v1.5
    length of the array or map stored in the column
    for StringType(), use length()
  + element_at(arr, idx): 
    Note: idx is 1-based, can use `-1` to refer index from the end
          arr[idx] is 0-based, `-1` is not working


ArrayType + StringType:
---
  + reverse(col): from v1.5
    reverse can work on both StringType() and ArrayType() columns
  + concat(*cols): support array concat from v2.4
  + concat_ws(sep, *cols): convert a list of StringType, ArrayType into StringType, can be mixed of two data types

