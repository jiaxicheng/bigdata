Using from_json and to_json to convert an Spark ML Vector column into an ArrayType column without using UDF

---
Code example see below:
    
    from pyspark.sql.functions import from_json, to_json, array
    from pyspark.ml.linalg import Vectors 
    
    df = spark.createDataFrame([
        (Vectors.dense([0.01,0.98,0.0]),), 
        (Vectors.dense([0.12,0.82,0.06]),),
        (Vectors.sparse(3, [0,2], [0.88,0.12]),),
        (Vectors.sparse(3, [2], [1.02]),)
    ], ["vec"]) 
    
    # schema for ArrayType(VectorUDT())
    schema = 'array<struct<type:int,size:int,indices:array<int>,values:array<double>>>'
    
    df1 = df.withColumn('data', from_json(to_json(array('vec')), schema)[0])
    df1.show(truncate=False)                                                                                           
    +---------------------+--------------------------------+
    |vec                  |data                            |
    +---------------------+--------------------------------+
    |[0.01,0.98,0.0]      |[1,,, [0.01, 0.98, 0.0]]        | <-- dense
    |[0.12,0.82,0.06]     |[1,,, [0.12, 0.82, 0.06]]       | <-- dense 
    |(3,[0,2],[0.88,0.12])|[0, 3, [0.0, 2.0], [0.88, 0.12]]| <-- sparse
    |(3,[2],[1.02])       |[0, 3, [2.0], [1.02]]           | <-- sparse
    +---------------------+--------------------------------+
    
    df1.printSchema()                                                                                                  
    root
     |-- vec: vector (nullable = true)
     |-- data: struct (nullable = true)
     |    |-- type: integer (nullable = true)
     |    |-- size: integer (nullable = true)
     |    |-- indices: array (nullable = true)
     |    |    |-- element: float (containsNull = true)
     |    |-- values: array (nullable = true)
     |    |    |-- element: float (containsNull = true)
    
    df_new = df1.selectExpr("vec", """
       /* data.type == 1, dense vector, take data.values */
       IF(data.type = 1
         , data.values
         /* otherwise, take sequence from 0 to data.size-1 and convert it into
           array with i from data.indices and corresponding value in data.values
           using array_position(1-based)
         */
         , transform(sequence(0,data.size-1), i -> IFNULL(data.values[int(array_position(data.indices, i))-1],0)) 
       ) as arr
    """)
    df_new.show(truncate=False)                                                                                        
    +---------------------+------------------+
    |vec                  |arr               |
    +---------------------+------------------+
    |[0.01,0.98,0.0]      |[0.01, 0.98, 0.0] |
    |[0.12,0.82,0.06]     |[0.12, 0.82, 0.06]|
    |(3,[0,2],[0.88,0.12])|[0.88, 0.0, 0.12] |
    |(3,[2],[1.02])       |[0.0, 0.0, 1.02]  |
    +---------------------+------------------+

    df_new.printSchema()                                                                                               
    root
     |-- vec: vector (nullable = true)
     |-- arr: array (nullable = true)
     |    |-- element: double (containsNull = true)


**WHERE:**

(1) use `to_json` to convert array('vec') into StringType column, note, to_json can not be directly apply to
`pyspark.ml.linalg.VectorUDT`, argument must be one of the Complex datatype: ArrayType, StructType or MapType.

(2) The VectorUDT takes a StructType with 4 fields: 
  + type: 0 for Sparse Vector and 1 for Dense Vector
  + size: for SparseVector only
  + indices: for SparseVector only, array of indices for items with non-zero values
  + values: all values for Dense Vector, non-zero values for Sparse Vector

(3) Use from_json and the following schema to retrieve the 4 fields mentioned above:

    array<struct<type:int,size:int,indices:array<int>,values:array<double>>>

(4) For Dense Vector where type == 1, take the values array directly

(5) For Sparse Vector, create a sequence from `0` to `size-1`, iterate through this list by `i`
    use `array_position(indices, i)` to find the index of `i` in `indices` array (return 0 if not exist)
    and retrieve the corresponding value at the same index in `values` array.
    (notice array_position is 1-based index, while Array reference is 0-based). 

        IFNULL(values[array_position(indices,i) - 1], 0)

    for i not in indices, `values[array_position(indices,i) - 1]` becomes values[-1] which is always NULL


**Notes:**

(1) if the column contains only Dense Vector, then it can be simplified using from_json

    df = spark.createDataFrame([
        (Vectors.dense([0.01,0.98,0.0]),), 
        (Vectors.dense([0.12,0.82,0.06]),)
    ], ["vec"]) 

    df_new = df.selectExpr("vec", "from_json(string(vec), 'array<double>') as arr")
    #DataFrame[vec: vector, arr: array<double>]

    df_new.show()
    +----------------+------------------+
    |vec             |arr               |
    +----------------+------------------+
    |[0.01,0.98,0.0] |[0.01, 0.98, 0.0] |
    |[0.12,0.82,0.06]|[0.12, 0.82, 0.06]|
    +----------------+------------------+

(2) With Spark 3.0+, use pyspark.ml.functions.vector_to_array

   see ref: https://stackoverflow.com/questions/38384347/how-to-split-vector-into-columns-using-pyspark

