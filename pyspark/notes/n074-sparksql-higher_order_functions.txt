Some applications with Spark SQL higher-order functions:

from pyspark.sql.functions import expr

(1) transform | zip_with:
https://stackoverflow.com/questions/57811151/how-to-concat-corresponding-elements-of-two-arrays-in-pypspark-dataframe-with-sp

join and merge cols1 and col2 into col3, see below:

    >>> df.withColumn('col3', expr("""
            array_join(
                transform(sequence(0, size(col1)-1), i -> CONCAT(col1[i], ':', col2[i]))
              , '_'
            )
      """)).show()
    +------+----------+-----------+
    |  col1|      col2|       col3|
    +------+----------+-----------+
    |[1, 2]|[ABC, DEF]|1:ABC_2:DEF|
    +------+----------+-----------+

Another way: using `zip_with()` to replace `transform() + sequence()`

    zip_with(col1, col2, (x,y) -> concat(x,':',y))

Another way: using `transform()` only:

    transform(col1, (x,i) -> CONCAT(x, ':', col2[i])) 


(2) filter:
https://stackoverflow.com/questions/58099344/filter-json-array-data-in-spark-dataframe

keep only recipients which has code == user1 (recipients field is an array of MapType()/StructType())

    >>> df = spark.createDataFrame([
        ("4f19-9deb-0ef861c1a6a1",[
             {"account":"45678765457876545678","code":"user1","status":"pending"}
           , {"account":"12354567897545678","code":"error2","status":"pending"}]
        ),  
        ("09ad-451e-8fb1-50bc185ef02f",[
             {"account":"4567654567876545678","code":"user3","status":"pending"}
           , {"account":"12354567876545678","code":"user2","status":"pending"}]
        )], ["lot_number", "recipients"] 
    )

    >>> df.withColumn('recipients', expr("filter(recipients, x -> x.code = 'user1')")) \
          .where('size(recipients) > 0') \
          .show(truncate=False)
    +----------------------+---------------------------------------------------------------------+
    |lot_number            |recipients                                                           |
    +----------------------+---------------------------------------------------------------------+
    |4f19-9deb-0ef861c1a6a1|[[code -> user1, account -> 45678765457876545678, status -> pending]]|
    +----------------------+---------------------------------------------------------------------+


(3) transform with MapType() + using exists()
https://stackoverflow.com/questions/58156078/pyspark-compare-empty-map-literal/58156434?noredirect=1#comment102702298_58156434

    concatenate map_keys and their corresponding values:
 
    >>> df.withColumn('m2', F.expr("transform(array_sort(map_keys(m1)), k -> concat_ws('|', k, m1[k]))")) \
          .show(truncate=False)
    +---+------------------------+---------------+
    |key|m1                      |m2             |
    +---+------------------------+---------------+
    |2  |[1 -> A, 2 -> B, 3 -> C]|[1|A, 2|B, 3|C]|
    |3  |[1 -> A, 2 -> B]        |[1|A, 2|B]     |
    +---+------------------------+---------------+


    >>> df.withColumn('m2', F.expr("map(bigint(1),'A' , bigint(2),'B')")) \
          .withColumn('f1', F.expr("exists(map_keys(m2), k -> (m2[k] != m1[k]) or (m1[k] is null))")) \
          .show()
    +---+--------------------+----------------+-----+
    |key|                  m1|              m2|   f1|
    +---+--------------------+----------------+-----+
    |  1|                  []|[1 -> A, 2 -> B]| true|
    |  2|[1 -> A, 2 -> B, ...|[1 -> A, 2 -> B]|false|
    |  3|    [1 -> A, 2 -> B]|[1 -> A, 2 -> B]|false|
    +---+--------------------+----------------+-----+

    After we make sure all key/value in m2 are exists and same in m1, we will need to compare the size of two maps
    only if it's the same, we can make sure two maps are the same. asume we saved the above transformation pipeline 
    into df1, then

        df1.filter('!f1 and size(m1) == size(m2)').show()

    Note: Spark MapType() can contains duplicates keys, this could make things complex, but since the OP's dict 
          mapping is in Python which does not allow duplicate keys, thus we can skip this issue here.


(4) differences between array_except and filter

    Notes: array_except(a1, a2) will create a new array with element in a1 but not in a2, the resulting array
           will drop duplicates as a side-effect. this might not be what you are looking for. if you want to
           keep duplicates in the original array a1, then use filter:

    >>> from pyspark.sql.functions import array_except, expr

    >>> df = spark.createDataFrame([(["10","20","30","30"],["10","20"])],["age","id"])

    >>> df.withColumn('c1', F.array_except('age', 'id')) \
          .withColumn('c2', F.expr("filter(age, x -> !array_contains(id, x))")) \
          .show()
    +----------------+--------+----+--------+
    |             age|      id|  c1|      c2|
    +----------------+--------+----+--------+
    |[10, 20, 30, 30]|[10, 20]|[30]|[30, 30]|
    +----------------+--------+----+--------+


(5) find non-null values which has key == 'searchkey' - use filter

    Notes: this is a question asked several times on stackoverflow.
    REF: https://stackoverflow.com/questions/58260374

    the field schema is the following:
    root
     |-- event_params: array (nullable = true)
     |    |-- element: struct (containsNull = true)
     |    |    |-- key: string (nullable = true)
     |    |    |-- value: struct (nullable = true)
     |    |    |    |-- int_value: long (nullable = true)
     |    |    |    |-- string_value: string (nullable = true)

   
    # use filter to find the first array element which key satisfies key == searchkey
    stmt = '''filter(event_params, x -> x.key == "{}")[0]'''.format(searchkey)

    # Run the above `stmt` with expr() function and asign the value to a temporary column `f1`
    # then use `coalesce()` function to retrieve the non-null value.
    df.withColumn('f1', expr(stmt)) \
        .selectExpr("coalesce(f1.value.string_value, string(f1.value.int_value),'---') AS event_category") \
        .show()


(6) sequence/transform etc with many date manipulations:
    REF: https://stackoverflow.com/questions/58270388    

    data = [("2000-01-01 15:20:37", "2000-01-01 19:12:22"), ("2000-01-01 15:00:00", "2000-01-01 19:00:00")]
    df = spark.createDataFrame(data, ["minDate", "maxDate"]) 
    df = df.withColumn('minDate', F.to_timestamp('minDate')).withColumn('maxDate', F.to_timestamp('maxDate'))
    >>> df.show()                                                                                                          
    +-------------------+-------------------+
    |            minDate|            maxDate|
    +-------------------+-------------------+
    |2000-01-01 15:20:37|2000-01-01 19:12:22|
    |2000-01-01 15:00:00|2000-01-01 19:00:00|
    +-------------------+-------------------+

   Below two selectExpr methods:
   * the first selectExpr, create a list `dt_list`:
     (1) use sequence to generate a list of dates from minDate to maxDate interval by 1 hour
     (2) use transform to truncate the date to `HOUR`
     (3) union/concat the original two date: minDate, maxDate
     (4) sort and then remove duplicates (when minData/maxDate on exact hour)
     (5) filter the date when it's > maxDate 
   * the 2nd selectExpr:
     (1) use the sequence to iterate from 0 to size(dt_list)-2
     (2) transform the array into a named_struct using the adjacents two dates
     (3) explode the resultset into table using inline function

    >>> df.selectExpr("""
        filter(
          array_distinct(
            array_sort(
              concat(
                array(minDate, maxDate),
                transform(
                  sequence(minDate, maxDate, interval 1 hour),
                  x -> date_trunc('HOUR', x + interval 1 hour)
                )
              )
            )
          ), x -> x <= maxDate
        ) as dt_list
      """).selectExpr("""

        inline(
          transform(sequence(0, size(dt_list)-2), i ->
            named_struct('minDate', IF(i=0, dt_list[i], dt_list[i] + interval 1 second), 'maxDate', dt_list[i+1])
          )
        )

      """).show(truncate=False)
    +-------------------+-------------------+
    |minDate            |maxDate            |
    +-------------------+-------------------+
    |2000-01-01 15:20:37|2000-01-01 16:00:00|
    |2000-01-01 16:00:01|2000-01-01 17:00:00|
    |2000-01-01 17:00:01|2000-01-01 18:00:00|
    |2000-01-01 18:00:01|2000-01-01 19:00:00|
    |2000-01-01 19:00:01|2000-01-01 19:12:22|
    |2000-01-01 15:00:00|2000-01-01 16:00:00|
    |2000-01-01 16:00:01|2000-01-01 17:00:00|
    |2000-01-01 17:00:01|2000-01-01 18:00:00|
    |2000-01-01 18:00:01|2000-01-01 19:00:00|
    +-------------------+-------------------+


(7) Use slice, transform, filter etc to find top-N (based on count) element in an array columns:
    REF: https://stackoverflow.com/questions/58281219

    N = 2
    df1.withColumn('topN', F.expr("""
        transform(
          slice(
              sort_array(
                  transform(
                      array_distinct(value_list)
                    , x -> array(size(filter(value_list, y -> y == x)), x)
                  )
                , False
              )       
            , 1     
            , {0} 
          ), z -> z[1]     
        )
      """.format(N))).show(truncate=False)
    +---+-----------------------+-------+                                           
    |id |value_list             |topN   |
    +---+-----------------------+-------+
    |5  |[12, 12]               |[12]   |
    |1  |[9, 6, 6, 9, 53]       |[9, 6] |
    |3  |[1, 1, 1, 1]           |[1]    |
    |2  |[1, 2, 85, 2, 85, 1, 1]|[1, 85]|
    |4  |[2, 2]                 |[2]    |
    +---+-----------------------+-------+



