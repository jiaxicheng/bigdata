{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Merge RDD rows by Start/End Pattern</h1>\n",
    "\n",
    "<p>Concatenate block of multiple lines based on Start/End patterns</p>\n",
    "<p>Example: https://stackoverflow.com/questions/50865236/convert-multiple-rdd-rows-into-one-row-in-pyspark</p>\n",
    "\n",
    "<p>In this case merging lines between <b>STARTING</b> and <b>STOP</b> marks. Since the same block (from STARTING to STOP) can be processed in multiple partitions, it's not easy to handle the Start and End marks at the same time. </p>\n",
    "\n",
    "<p>We can handle this task in two steps:</p>\n",
    "\n",
    "<ol>\n",
    " <li>Split the block with the '<b>STOP</b>' block by functions like <b>fold()</b>, <b>aggregate()</b>, <b>reduce()</b>\n",
    "    Mark STARTING with a preceeding NUL character: '<b>\\0</b>' </li>\n",
    " <li>Reload data into RDD and clean the unrelated text before the <b>NUL</b> + '<b>STARTING</b>' using <b>map()</b> function </li>\n",
    "</ol>\n",
    "\n",
    "<p>Below code was tested under ipython:</p>\n",
    "\n",
    "<p>Sample text:</p>\n",
    "<pre>\n",
    "skip0 STARTING\n",
    "skip0\n",
    "STARTING |1|TH|TGG|132|8|T|Fall|\n",
    "EVENT 1|56|HT|JUP||||||||\n",
    "EVENT 2|BHT|987|231|||||||||||||||||\n",
    "STOP|HFR|0.5|90|\n",
    "skip1\n",
    "skip1\n",
    "skip1\n",
    "STARTING |8|TH|TGG|12|8|T|Fall|\n",
    "EVENT 1|6|HT|UP||||||||\n",
    "EVENT 2|BT|987|31|||||||||||||||||\n",
    "STOP|FR|0.5|90|\n",
    "skip2\n",
    "skip2\n",
    "</pre>\n",
    "\n",
    "<h4>Note:</h4>\n",
    "<ol>\n",
    "    <li>For large amount of data, merging data into one might raise memory or disk space issue. The following method by using foldByKey() transformation replace the fold() action may or may not resulve the issue. since the order of lines matters for this problem, the first function using a constant key is probably required. My conclusion is that using map/reduce and reduce's cousions like fold(), aggregate(), combiner etc is probably not appropriate for string concatenation since it does not reduce the size of data chucnks.\n",
    "<pre>\n",
    "    rdd.map(lambda x: (1,x))   \\\n",
    "   .foldByKey([''], merge) \\\n",
    "   .map(lambda x: x[1])    \\\n",
    "   .saveAsTextFile(\"/path/to/file\")\n",
    "</pre>\n",
    "    </li>\n",
    "    <li>The result can be easier by running the following awk command line\n",
    "<pre>\n",
    "    awk '/^STARTING/{f=1; ORS=\" \"}/^STOP/{ORS=\"\\n\"; f=0}f==1||/^STOP/' merge-orig.txt > merge-new.txt\n",
    "</pre>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder                   \\\n",
    "                    .master(\"local[2]\")        \\\n",
    "                    .appName(\"pyspark-test1\")  \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file and check the partitions\n",
    "rdd = sc.textFile(\"/test/pyspark/merge-1.txt\", minPartitions=20)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[[u'skip0', u'skip0STARTING'],\n",
    " [u'STARTING |1|TH|TGG|132|8|T|Fall|'],\n",
    " [],\n",
    " [],\n",
    " [u'EVENT 1|56|HT|JUP||||||||'],\n",
    " [],\n",
    " [u'EVENT 2|BHT|987|231|||||||||||||||||'],\n",
    " [],\n",
    " [u'STOP|HFR|0.5|90|'],\n",
    " [],\n",
    " [u'skip1', u'skip1'],\n",
    " [u'skip1', u'STARTING |8|TH|TGG|12|8|T|Fall|'],\n",
    " [],\n",
    " [],\n",
    " [u'EVENT 1|6|HT|UP||||||||'],\n",
    " [u'EVENT 2|BT|987|31|||||||||||||||||'],\n",
    " [],\n",
    " [],\n",
    " [u'STOP|FR|0.5|90|'],\n",
    " [u'skip2'],\n",
    " [u'skip2']]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold function to merge data in and between partitions\n",
    "# Note: we preceeded a NUL char before the 'STARTING' to split the unwanted text later\n",
    "def merge(x, y):\n",
    "    if type(y) is list:\n",
    "        x[-1] += y[0]\n",
    "        x = x + y[1:]\n",
    "    else:\n",
    "        if y.startswith('STARTING'):\n",
    "            x[-1] += '\\0' + y\n",
    "        else:\n",
    "            x[-1] += y\n",
    "        if y.startswith('STOP'):\n",
    "            x.append('')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result from the first step Terminate the block at 'STOP' line while keeping messy before the 'Start' line\n",
    "rdd.fold([''], merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[u'skip0skip0STARTING\\x00STARTING |1|TH|TGG|132|8|T|Fall|EVENT 1|56|HT|JUP||||||||EVENT 2|BHT|987|231|||||||||||||||||STOP|HFR|0.5|90|',\n",
    " u'skip1skip1skip1\\x00STARTING |8|TH|TGG|12|8|T|Fall|EVENT 1|6|HT|UP||||||||EVENT 2|BT|987|31|||||||||||||||||STOP|FR|0.5|90|',\n",
    " u'skip2skip2']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into RDD again and remove the precedding text with map() function\n",
    "rdd1 = sc.parallelize(rdd.fold([''], merge)[:-1])\n",
    "rdd1.map(lambda x: x[x.find('\\0STARTING')+1:]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[u'STARTING |1|TH|TGG|132|8|T|Fall|EVENT 1|56|HT|JUP||||||||EVENT 2|BHT|987|231|||||||||||||||||STOP|HFR|0.5|90|',\n",
    " u'STARTING |8|TH|TGG|12|8|T|Fall|EVENT 1|6|HT|UP||||||||EVENT 2|BT|987|31|||||||||||||||||STOP|FR|0.5|90|']\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
