{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Merge RDD rows by Start/End Pattern</h1>\n",
    "\n",
    "<p>Concatenate block of multiple lines based on Start/End patterns</p>\n",
    "<p>Example: https://stackoverflow.com/questions/50865236/convert-multiple-rdd-rows-into-one-row-in-pyspark</p>\n",
    "\n",
    "<p>In this case merging lines between <b>STARTING</b> and <b>STOP</b> marks. Since the same block (from STARTING to STOP) can be processed in multiple partitions, it's not easy to handle the Start and End marks at the same time. </p>\n",
    "\n",
    "<p>We can handle this task in two steps:</p>\n",
    "\n",
    "<ol>\n",
    " <li>Split the block with the '<b>STOP</b>' block by functions like <b>fold()</b>, <b>aggregate()</b>, <b>reduce()</b>\n",
    "    Mark STARTING with a preceeding NUL character: '<b>\\0</b>' </li>\n",
    " <li>Reload data into RDD and clean the unrelated text before the <b>NUL</b> + '<b>STARTING</b>' using <b>map()</b> function </li>\n",
    "</ol>\n",
    "\n",
    "<p>Below code was tested under ipython:</p>\n",
    "\n",
    "<p>Sample text:</p>\n",
    "<pre>\n",
    "skip0 STARTING\n",
    "skip0\n",
    "STARTING |1|TH|TGG|132|8|T|Fall|\n",
    "EVENT 1|56|HT|JUP||||||||\n",
    "EVENT 2|BHT|987|231|||||||||||||||||\n",
    "STOP|HFR|0.5|90|\n",
    "skip1\n",
    "skip1\n",
    "skip1\n",
    "STARTING |8|TH|TGG|12|8|T|Fall|\n",
    "EVENT 1|6|HT|UP||||||||\n",
    "EVENT 2|BT|987|31|||||||||||||||||\n",
    "STOP|FR|0.5|90|\n",
    "skip2\n",
    "skip2\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder                   \\\n",
    "                    .master(\"local[2]\")        \\\n",
    "                    .appName(\"pyspark-test1\")  \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file and check the partitions\n",
    "rdd = sc.textFile(\"/test/pyspark/merge-1.txt\", minPartitions=20)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[u'skip0', u'skip0STARTING'],\n",
    " [u'STARTING |1|TH|TGG|132|8|T|Fall|'],\n",
    " [],\n",
    " [],\n",
    " [u'EVENT 1|56|HT|JUP||||||||'],\n",
    " [],\n",
    " [u'EVENT 2|BHT|987|231|||||||||||||||||'],\n",
    " [],\n",
    " [u'STOP|HFR|0.5|90|'],\n",
    " [],\n",
    " [u'skip1', u'skip1'],\n",
    " [u'skip1', u'STARTING |8|TH|TGG|12|8|T|Fall|'],\n",
    " [],\n",
    " [],\n",
    " [u'EVENT 1|6|HT|UP||||||||'],\n",
    " [u'EVENT 2|BT|987|31|||||||||||||||||'],\n",
    " [],\n",
    " [],\n",
    " [u'STOP|FR|0.5|90|'],\n",
    " [u'skip2'],\n",
    " [u'skip2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold function to merge data in and between partitions\n",
    "# Note: we preceeded a NUL char before the 'STARTING' to split the unwanted text later\n",
    "def merge(x, y):\n",
    "    if type(y) is list:\n",
    "        x[-1] += y[0]\n",
    "        x = x + y[1:]\n",
    "    else:\n",
    "        if y.startswith('STARTING'):\n",
    "            x[-1] += '\\0' + y\n",
    "        else:\n",
    "            x[-1] += y\n",
    "        if y.startswith('STOP'):\n",
    "            x.append('')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result from the first step Terminate the block at 'STOP' line while keeping messy before the 'Start' line\n",
    "rdd.fold([''], merge)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[u'skip0skip0STARTING\\x00STARTING |1|TH|TGG|132|8|T|Fall|EVENT 1|56|HT|JUP||||||||EVENT 2|BHT|987|231|||||||||||||||||STOP|HFR|0.5|90|',\n",
    " u'skip1skip1skip1\\x00STARTING |8|TH|TGG|12|8|T|Fall|EVENT 1|6|HT|UP||||||||EVENT 2|BT|987|31|||||||||||||||||STOP|FR|0.5|90|',\n",
    " u'skip2skip2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into RDD again and remove the precedding text with map() function\n",
    "rdd1 = sc.parallelize(rdd.fold([''], merge)[:-1])\n",
    "rdd1.map(lambda x: x[x.find('\\0STARTING')+1:]).collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[u'STARTING |1|TH|TGG|132|8|T|Fall|EVENT 1|56|HT|JUP||||||||EVENT 2|BHT|987|231|||||||||||||||||STOP|HFR|0.5|90|',\n",
    " u'STARTING |8|TH|TGG|12|8|T|Fall|EVENT 1|6|HT|UP||||||||EVENT 2|BT|987|31|||||||||||||||||STOP|FR|0.5|90|']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
